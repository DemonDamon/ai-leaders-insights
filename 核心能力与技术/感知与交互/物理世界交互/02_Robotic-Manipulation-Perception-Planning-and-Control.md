# Robotic Manipulation: Perception, Planning, and Control

**来源**: [http://manipulation.csail.mit.edu/index.html](http://manipulation.csail.mit.edu/index.html)

---

# Robotic Manipulation

原文链接: http://manipulation.csail.mit.edu/index.html

# [Robotic Manipulation](index.html)

Perception, Planning, and Control

[Russ Tedrake](http://people.csail.mit.edu/russt/)

© Russ Tedrake, 2020-2025  
Last modified .
[How to cite these notes, use annotations, and give feedback.](misc.html)

**Note:** These are working notes used for [a course being taught
at MIT](http://manipulation.csail.mit.edu/Fall2025/). They will be updated throughout the Fall 2025 semester.

# Search these notes

# PDF version of the notes

You can also download a PDF version of these notes (updated much less frequently) from [here](https://github.com/RussTedrake/manipulation/releases).

The PDF version of these notes are autogenerated from the HTML version.
There are a few conversion/formatting artifacts that are easy to fix (please
feel free to point them out). But there are also interactive elements in the
HTML version are not easy to put into the PDF. When possible, I try to
provide a link. But I consider the [online HTML version](http://manipulation.csail.mit.edu) to be the
main version.

# Table of Contents

* [Preface](#preface)
* [Chapter 1: Introduction](intro.html)

+ [Manipulation is more than pick-and-place](intro.html#section1)
+ [Open-world manipulation](intro.html#section2)
+ [Simulation](intro.html#section3)
+ [These notes are interactive](intro.html#section4)
+ [Model-based design and analysis](intro.html#section5)
+ [Organization of these notes](intro.html#section6)
+ [Exercises](intro.html#section7)

* [Chapter 2: Let's get you a robot](robot.html)

+ [Robot description files](robot.html#section1)
+ [Arms](robot.html#section2)

- Position-controlled robots

* Position Control.
* An aside: link dynamics with a transmission.

- Torque-controlled robots
- A proliferation of hardware
- Simulating the Kuka iiwa

+ [Hands](robot.html#section3)

- Dexterous hands
- Simple grippers
- Soft/underactuated hands
- Other end effectors
- If you haven't seen it...

+ [Sensors](robot.html#section4)
+ [Putting it all together](robot.html#hardware_station)

- HardwareStation
- HardwareStationInterface
- HardwareStation stand-alone simulation

+ [More HardwareStation examples](robot.html#section6)
+ [Exercises](robot.html#section7)

* [Chapter 3: Basic Pick and Place](pick.html)

+ [Monogram Notation](pick.html#monogram)
+ [Pick and place via spatial transforms](pick.html#section2)
+ [Spatial Algebra](pick.html#spatial_algebra)

- Representations for 3D rotation

+ [Forward kinematics](pick.html#kinematics)

- The kinematic tree
- Forward kinematics for pick and place

+ [Differential kinematics (Jacobians)](pick.html#jacobian)
+ [Differential inverse kinematics](pick.html#section6)

- The Jacobian pseudo-inverse
- Invertibility of the Jacobian

+ [Defining the grasp and pre-grasp poses](pick.html#section7)
+ [A pick and place trajectory](pick.html#pick_and_place_trajectory)
+ [Putting it all together](pick.html#section9)
+ [Differential inverse kinematics with
  constraints](pick.html#diff_ik_w_constraints)

- Pseudo-inverse as an optimization
- Adding velocity constraints
- Adding position and acceleration constraints
- Joint centering
- Tracking a desired pose
- Alternative formulations

+ [Exercises](pick.html#section11)

* [Chapter 4: Geometric Pose Estimation](pose.html)

+ [Cameras and depth sensors](pose.html#section1)

- Depth sensors
- Simulation

+ [Representations for geometry](pose.html#section2)
+ [Point cloud registration with known
  correspondences](pose.html#registration)
+ [Iterative Closest Point (ICP)](pose.html#icp)
+ [Dealing with partial views and outliers](pose.html#section5)

- Detecting outliers
- Point cloud segmentation
- Generalizing correspondence
- Soft correspondences
- Nonlinear optimization

* Precomputing distance functions

- Global optimization

+ [Non-penetration and "free-space" constraints](pose.html#section6)

- Free space constraints as non-penetration constraints

+ [Tracking](pose.html#section7)
+ [Putting it all together](pose.html#section8)
+ [Looking ahead](pose.html#section9)
+ [Exercises](pose.html#exercises)

* [Chapter 5: Bin Picking](clutter.html)

+ [Generating random cluttered scenes](clutter.html#section1)

- Falling things

+ [Static equilibrium with frictional contact](clutter.html#section2)

- Spatial force
- Collision geometry
- Contact forces between bodies in collision
- The Contact Frame
- The (Coulomb) Friction Cone
- Static equilibrium as an optimization problem

+ [Contact simulation](clutter.html#section3)
+ [Model-based grasp selection](clutter.html#section4)

- The contact wrench cone
- Colinear antipodal grasps

+ [Grasp selection from point clouds](clutter.html#section5)

- Point cloud pre-processing
- Estimating normals and local curvature
- Evaluating a candidate grasp
- Generating grasp candidates

+ [The corner cases](clutter.html#section6)
+ [Programming the Task Level](clutter.html#task)

- State Machines and Behavior Trees
- Task planning
- Large Language Models
- A simple state machine for "clutter clearing"

+ [Putting it all together](clutter.html#section8)
+ [Exercises](clutter.html#exercises)

* [Chapter 6: Motion Planning](trajectories.html)

+ [Inverse Kinematics](trajectories.html#section1)

- From end-effector pose to joint angles
- IK as constrained optimization
- Global inverse kinematics
- Inverse kinematics vs differential inverse kinematics
- Grasp planning using inverse kinematics

+ [Kinematic trajectory optimization](trajectories.html#section2)

- Trajectory parameterizations
- Optimization algorithms

+ [Sampling-based motion planning](trajectories.html#section3)

- Rapidly-exploring random trees (RRT)
- The Probabilistic Roadmap (PRM)
- Post-processing
- Sampling-based planning in practice

+ [Motion Planning w/ Graphs of Convex Sets (GCS)](trajectories.html#gcs)

- Graphs of Convex Sets
- GCS (Kinematic) Trajectory Optimization
- Convex decomposition of (collision-free) configuration
  space
- GcsTrajOpt Examples
- Variations and Extensions

+ [Time-optimal path parameterizations](trajectories.html#topp)
+ [Exercises](trajectories.html#section6)

* [Chapter 7: Mobile Manipulation](mobile.html)

+ [A New Cast of Characters](mobile.html#section1)
+ [What's different about perception?](mobile.html#section2)

- Partial views / active perception
- Unknown (potentially dynamic) environments
- Robot state estimation

+ [What's different about motion planning?](mobile.html#section3)

- Wheeled robots

* Holonomic drives
* Nonholonomic drives

- Legged robots

+ [What's different about simulation?](mobile.html#section4)
+ [Navigation](mobile.html#section5)

- Mapping (in addition to localization)
- Identifying traversable terrain

+ [Exercises](mobile.html#section6)

* [Chapter 8: Manipulator Control](force.html)

+ [The Manipulator-Control Toolbox](force.html#toolbox)
+ [Assume your robot is a point mass](force.html#section2)

- Trajectory tracking
- (Direct) force control
- Indirect force control
- Hybrid position/force control

+ [The general case (using the manipulator equations)](force.html#section3)

- Trajectory tracking
- Joint stiffness control
- Cartesian stiffness and operational space control
- Some implementation details on the iiwa

+ [Putting it all together](force.html#section4)
+ [Peg in hole](force.html#section5)
+ [Exercises](force.html#section6)

* [Chapter 9: Object Detection and
  Segmentation](segmentation.html)

+ [Getting to big data](segmentation.html#section1)

- Crowd-sourced annotation datasets
- Segmenting new classes via fine tuning
- Annotation tools for manipulation
- Synthetic datasets
- Self-supervised learning
- Even bigger datasets

+ [Object detection and segmentation](segmentation.html#section2)
+ [Putting it all together](segmentation.html#section3)
+ [Variations and Extensions](segmentation.html#section4)

- Pretraining wth self-supervised learning
- Leveraging large-scale models

+ [Exercises](segmentation.html#section5)

* [Chapter 10: Deep Perception for
  Manipulation](deep_perception.html)

+ [Pose estimation](deep_perception.html#section1)

- Pose representation
- Loss functions
- Pose estimation benchmarks
- Limitations

+ [Grasp selection](deep_perception.html#section2)
+ [(Semantic) Keypoints](deep_perception.html#section3)
+ [Dense Correspondences](deep_perception.html#section4)
+ [Scene Flow](deep_perception.html#section5)
+ [Task-level state](deep_perception.html#section6)
+ [Other perceptual tasks / representations](deep_perception.html#section7)
+ [Exercises](deep_perception.html#exercises)

* [Chapter 11: Reinforcement Learning](rl.html)

+ [RL Software](rl.html#section1)
+ [Policy-gradient methods](rl.html#section2)

- Black-box optimization
- Stochastic optimal control
- Using gradients of the policy, but not the environment
- REINFORCE, PPO, TRPO
- Control for manipulation should be easy

+ [Value-based methods](rl.html#section3)
+ [Model-based RL](rl.html#section4)
+ [Exercises](rl.html#section5)

* [Chapter 12: Soft Robots and Tactile Sensing](tactile.html)

+ [Why soft?](tactile.html#section1)
+ [Soft robot hardware](tactile.html#section2)
+ [Soft-body simulation](tactile.html#section3)
+ [Tactile sensing](tactile.html#section4)

- What information do we want/need?
- Visuotactile sensing
- Whole-body sensing
- Simulating tactile sensors

+ [Perception with tactile sensors](tactile.html#section5)
+ [Control with tactile sensors](tactile.html#section6)

**Appendix**

* [Appendix A: Spatial Algebra](spatial.html)

+ [Position, Rotation, and Pose](spatial.html#section1)
+ [Spatial velocity](spatial.html#section2)
+ [Spatial force](spatial.html#section3)

* [Appendix B: Drake](drake.html)

+ [Pydrake](drake.html#section1)
+ [Online Jupyter Notebooks](drake.html#notebooks)

- Running on Deepnote
- Enabling licensed solvers

+ [Running on your own machine](drake.html#section3)
+ [Getting help](drake.html#section4)

* [Appendix C: DrakeGym Environments](environments.html)

+ [Box Flipup](environments.html#section1)

* [Appendix D: Setting up your own "Manipulation Station"](station.html)

+ [Message Passing](station.html#section1)
+ [Kuka LBR iiwa + Schunk WSG Gripper](station.html#section2)
+ [Franka Panda](station.html#section3)
+ [Intel Realsense D415 Depth Cameras](station.html#section4)

* [Appendix E: Miscellaneous](misc.html)

+ [How to cite these notes](misc.html#cite)
+ [Annotation tool etiquette](misc.html#annotation)
+ [Some great final projects](misc.html#projects)
+ [Please give me feedback!](misc.html#feedback)

You can find documentation for the source code supporting these notes [here](python/index.html).

# Preface

I've always loved robots, but it's only relatively recently that I've
turned my attention to robotic manipulation. I particularly like the
challenge of building robots that can master physics to achieve
human/animal-like dexterity and agility. It was [passive
dynamic walkers](http://underactuated.mit.edu/intro.html) and the beautiful analysis that accompanies them that
first helped cement this centrality of dynamics in my view of the world and my
approach to robotics. From there I became fascinated with (experimental)
fluid dynamics, and the idea that birds with articulated wings actually
"manipulate" the air to achieve incredible efficiency and agility. Humanoid
robots and fast-flying aerial vehicles in clutter forced me to start thinking
more deeply about the role of perception in dynamics and control. Now I
believe that this interplay between perception and dynamics is truly
fundamental, and I am passionate about the observation that relatively
"simple" problems in manipulation (how do I button up my dress shirt?) expose
the problem beautifully.

My approach to programming robots has always been very
computational/algorithmic. I started out using tools primarily from machine
learning (especially reinforcement learning) to develop the control systems
for simple walking machines; but as the robots and tasks got more complex I
turned to more sophisticated tools from model-based planning and
optimization-based control. In my view, no other discipline has thought so
deeply about dynamics as has control theory, and the algorithmic efficiency
and guaranteed performance/robustness that can be obtained by the best
model-based control algorithms far surpasses what we can do today with
learning control. Unfortunately, the mathematical maturity of
controls-related research has also led the field to be relatively conservative
in their assumptions and problem formulations; the requirements for robotic
manipulation break these assumptions. For example, robust control typically
assumes dynamics that are (nearly) smooth and uncertainty that can be
represented by simple distributions or simple sets; but in robotic
manipulation, we must deal with the non-smooth mechanics of contact and
uncertainty that comes from varied lighting conditions, and different numbers
of objects with unknown geometry and dynamics. In practice, no
state-of-the-art robotic manipulation system to date (that I know of) uses
rigorous control theory to design even the low-level feedback that determines
when a robot makes and breaks contact with the objects it is manipulating. An
explicit goal of these notes is to try to change that.

In the past few years, deep learning has had an unquestionable impact on
robotic perception, unblocking some of the most daunting challenges in
performing manipulation outside of a laboratory or factory environment. We
will discuss relevant tools from deep learning for object recognition,
segmentation, pose/keypoint estimation, shape completion, etc. Now relatively
old approaches to learning control are also enjoying an incredible surge in
popularity, fueled in part by massive computing power and increasingly
available robot hardware and simulators. Unlike learning for perception,
learning control algorithms are still far from a technology, with some of the
most impressive looking results still being hard to understand and to
reproduce. But the recent work in this area has unquestionably highlighted
the pitfalls of the conservatism taken by the controls community. Learning
researchers are boldly formulating much more aggressive and exciting problems
for robotic manipulation than we have seen before -- in many cases we are
realizing that some manipulation tasks are actually quite easy, but in other
cases we are finding problems that are still fundamentally hard.

Finally, it feels that the time is ripe for robotic manipulation to have a
real and dramatic impact in the world, in fields from logistics to home
robots. Over the last few years, we've seen UAVs/drones transition from
academic curiosities into consumer products. Even more recently, autonomous
driving has transitioned from academic research to industry, at least in
terms of dollars invested. Manipulation feels like the next big thing that
will make the leap from robotic research to practice. It's still a bit risky
for a venture capitalist to invest in, but nobody doubts the size of the
market once we have the technology. How lucky are we to potentially be able
to play a role in that transition?

So this is where the notes begin... we are at an incredible crossroads
between learning and control and robotics with an opportunity to have
immediate impact in industrial and consumer applications and potentially even
to forge entirely new eras for systems theory and controls. I'm just trying
to hold on and to enjoy the ride.

# A manipulation toolbox

Another explicit goal of these lecture notes is to provide high-quality
implementations of the most useful tools in a manipulation scientist's
toolbox. When I am forced to choose between mathematical clarity and
runtime performance, the clear formulation is always my first priority; I
will try to include a performant formulation, too, if possible or try to
give pointers to alternatives. Manipulation research is moving quickly, and
I aim to evolve these notes to keep pace. I hope that the software
components provided in
 and in these notes can be directly useful to you in your own
work.

If you would like to replicate any or all of the hardware that we use for
these notes, you can find information and instructions in the [appendix](station.html).

As you use the code, please consider [contributing back](https://drake.mit.edu/getting_help.html)
(especially to the mature code in ). Even questions/bug
reports can be important contributions. If you have questions/find issues
with these notes, please submit them [here](https://github.com/RussTedrake/manipulation/issues).

[First chapter](intro.html)

---

|  |  |
| --- | --- |
| [Accessibility](https://accessibility.mit.edu/) | © Russ Tedrake, 2020-2025 |

---

*爬取时间: 2025-11-28 21:51:10*
