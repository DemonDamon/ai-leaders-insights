






<!DOCTYPE html>
<html
  lang="en"
  
  data-color-mode="auto" data-light-theme="light" data-dark-theme="dark"
  data-a11y-animated-images="system" data-a11y-link-underlines="true"
  
  >




  <head>
    <meta charset="utf-8">
  <link rel="dns-prefetch" href="https://github.githubassets.com">
  <link rel="dns-prefetch" href="https://avatars.githubusercontent.com">
  <link rel="dns-prefetch" href="https://github-cloud.s3.amazonaws.com">
  <link rel="dns-prefetch" href="https://user-images.githubusercontent.com/">
  <link rel="preconnect" href="https://github.githubassets.com" crossorigin>
  <link rel="preconnect" href="https://avatars.githubusercontent.com">

  

  <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/light-8e973f836952.css" /><link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/light_high_contrast-34b642d57214.css" /><link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/dark-4bce7af39e21.css" /><link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/dark_high_contrast-ad512d3e2f3b.css" /><link data-color-theme="light" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light-8e973f836952.css" /><link data-color-theme="light_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_high_contrast-34b642d57214.css" /><link data-color-theme="light_colorblind" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_colorblind-54be93e666a7.css" /><link data-color-theme="light_colorblind_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_colorblind_high_contrast-8ae7edf5489c.css" /><link data-color-theme="light_tritanopia" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_tritanopia-84d50df427c0.css" /><link data-color-theme="light_tritanopia_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_tritanopia_high_contrast-a80873375146.css" /><link data-color-theme="dark" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark-4bce7af39e21.css" /><link data-color-theme="dark_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_high_contrast-ad512d3e2f3b.css" /><link data-color-theme="dark_colorblind" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_colorblind-d152d6cd6879.css" /><link data-color-theme="dark_colorblind_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_colorblind_high_contrast-fa4060c1a9da.css" /><link data-color-theme="dark_tritanopia" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_tritanopia-d7bad0fb00bb.css" /><link data-color-theme="dark_tritanopia_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_tritanopia_high_contrast-4a0107c0f60c.css" /><link data-color-theme="dark_dimmed" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_dimmed-045e6b6ac094.css" /><link data-color-theme="dark_dimmed_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_dimmed_high_contrast-5de537db5e79.css" />

  <style type="text/css">
    :root {
      --tab-size-preference: 4;
    }

    pre, code {
      tab-size: var(--tab-size-preference);
    }
  </style>

    <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-primitives-c37d781e2da5.css" />
    <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-efa08b71f947.css" />
    <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/global-6dcb16809e76.css" />
    <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/github-f86c648606b5.css" />
  <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/repository-5d735668c600.css" />
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/code-9c9b8dc61e74.css" />

  

  <script type="application/json" id="client-env">{"locale":"en","featureFlags":["a11y_status_checks_ruleset","actions_custom_images_public_preview_visibility","actions_custom_images_storage_billing_ui_visibility","actions_enable_snapshot_keyword","actions_image_version_event","alternate_user_config_repo","api_insights_show_missing_data_banner","client_version_header","codespaces_prebuild_region_target_update","contentful_lp_footnotes","copilot_agent_cli_public_preview","copilot_agent_task_list_v2","copilot_agent_tasks_btn_code_nav","copilot_agent_tasks_btn_code_view","copilot_agent_tasks_btn_code_view_lines","copilot_agent_tasks_btn_repo","copilot_api_agentic_issue_marshal_yaml","copilot_api_draft_issue_reference_with_project_id","copilot_api_draft_issues_with_dependencies","copilot_api_github_draft_update_issue_skill","copilot_chat_agents_empty_state","copilot_chat_attach_multiple_images","copilot_chat_file_redirect","copilot_chat_input_commands","copilot_chat_opening_thread_switch","copilot_chat_reduce_quota_checks","copilot_chat_search_bar_redirect","copilot_chat_selection_attachments","copilot_chat_vision_in_claude","copilot_chat_vision_preview_gate","copilot_coding_agent_task_response","copilot_custom_copilots","copilot_custom_copilots_feature_preview","copilot_duplicate_thread","copilot_extensions_hide_in_dotcom_chat","copilot_extensions_removal_on_marketplace","copilot_features_raycast_logo","copilot_file_block_ref_matching","copilot_ftp_hyperspace_upgrade_prompt","copilot_ftp_settings_upgrade","copilot_immersive_generate_thread_name_async","copilot_immersive_issues_readonly_viewer","copilot_immersive_issues_show_relationships","copilot_immersive_job_result_preview","copilot_immersive_structured_model_picker","copilot_immersive_task_hyperlinking","copilot_immersive_task_within_chat_thread","copilot_org_policy_page_focus_mode","copilot_security_alert_assignee_options","copilot_share_active_subthread","copilot_spaces_as_attachments","copilot_spaces_ga","copilot_spark_empty_state","copilot_spark_loading_webgl","copilot_spark_progressive_error_handling","copilot_spark_use_billing_headers","copilot_stable_conversation_view","copilot_swe_agent_progress_commands","copilot_swe_agent_use_subagents","copilot_workbench_agent_seed_tool","copilot_workbench_agent_user_edit_awareness","copilot_workbench_cache","copilot_workbench_preview_analytics","copilot_workbench_use_single_prompt","data_router_force_refetch_on_navigation","direct_to_salesforce","disable_dashboard_universe_2025_private_preview","dom_node_counts","dotcom_chat_client_side_skills","enterprise_ai_controls","failbot_report_error_react_apps_on_page","fetch_graphql_improved_error_serialization","ghost_pilot_confidence_truncation_25","ghost_pilot_confidence_truncation_40","global_search_multi_orgs","hyperspace_2025_logged_out_batch_1","hyperspace_nudges_universe25","hyperspace_nudges_universe25_post_event","initial_per_page_pagination_updates","issue_fields_report_usage","issues_expanded_file_types","issues_lazy_load_comment_box_suggestions","issues_react_bots_timeline_pagination","issues_react_prohibit_title_fallback","issues_report_sidebar_interactions","issues_sticky_sidebar","issues_title_flex_header_fix","item_picker_assignee_tsq_migration","item_picker_project_tsq_migration","kb_convert_to_space","kb_sunset","lifecycle_label_name_updates","link_contact_sales_swp_marketo","marketing_pages_search_explore_provider","mcp_registry_install","mcp_registry_oss_v0_1_api","memex_default_issue_create_repository","memex_grouped_by_edit_route","memex_mwl_filter_field_delimiter","mission_control_use_body_html","new_traffic_page_banner","open_agent_session_in_vscode_insiders","open_agent_session_in_vscode_stable","primer_react_segmented_control_tooltip","projects_assignee_max_limit","react_fetch_graphql_ignore_expected_errors","record_sso_banner_metrics","repos_insights_remove_new_url","ruleset_deletion_confirmation","sample_network_conn_type","scheduled_reminders_updated_limits","site_features_copilot_universe","site_homepage_collaborate_video","site_homepage_contentful","site_homepage_eyebrow_banner","site_homepage_universe_animations","site_msbuild_webgl_hero","spark_fix_rename","spark_force_push_after_checkout","spark_improve_image_upload","spark_kv_encocoded_keys","spark_show_data_access_on_publish","spark_sync_repository_after_iteration","viewscreen_sandbox","webp_support","workbench_store_readonly"],"copilotApiOverrideUrl":"https://api.githubcopilot.com"}</script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/high-contrast-cookie-1a011967750c.js"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/wp-runtime-996734cac1d1.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/913-ca2305638c53.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/6488-de87864e6818.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/environment-122ed792f7a3.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/11683-aa3d1ebe6648.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/43784-4652ae97a661.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/4712-809eac2badf7.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/81028-5b8c5e07a4fa.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/74911-6a311b93ee8e.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/91853-b5d2e5602241.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/78143-31968346cf4c.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/52430-c46e2de36eb2.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/github-elements-5b3e77949adb.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/element-registry-26394fb18607.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/28546-ee41c9313871.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/17688-a9e16fb5ed13.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/2869-a4ba8f17edb3.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/70191-5122bf27bf3e.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/7332-5ea4ccf72018.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/3561-d56ebea34f95.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/19037-69d630e73af8.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/89708-dcace8d1bd5c.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/51519-dc0d4e14166a.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/7534-e77ef16596b9.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/96384-750ef5263abe.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/19718-676a65610616.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/behaviors-f4043678d2f5.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/48011-1f20a5c80dd7.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/notifications-global-eb21f5b0029d.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/39713-8508e9483898.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/45688-b093405a7bf6.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/90787-a2980eb97100.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/codespaces-9f0a42ea762f.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/23387-1b12da426b92.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/72568-d9b14327a489.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/20065-de16f7379718.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/repositories-93cb558d0fd9.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/31615-236504c8966f.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/code-menu-67717e88b7e6.js" defer="defer"></script>
  
  <script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/primer-react-e4ebdadc89a4.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/react-lib-760965ba27bb.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/react-core-021efa96a7a2.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/octicons-react-a215e6ee021a.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/12979-6037a19d779b.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/48775-3cc79d2cd30e.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/42892-341e79a04903.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/23832-db66abd83e08.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/39560-54753a208c77.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/notifications-subscriptions-menu-887db8ef106b.js" defer="defer"></script>
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.fbf61c683a537e8f92d5.module.css" />
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/notifications-subscriptions-menu.933100a30c03fd4e8ae4.module.css" />

  <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.fbf61c683a537e8f92d5.module.css" />
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/notifications-subscriptions-menu.933100a30c03fd4e8ae4.module.css" />


  <title>GitHub - HCPLab-SYSU/Embodied_AI_Paper_List: [Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI</title>



  <meta name="route-pattern" content="/:user_id/:repository" data-turbo-transient>
  <meta name="route-controller" content="files" data-turbo-transient>
  <meta name="route-action" content="disambiguate" data-turbo-transient>
  <meta name="fetch-nonce" content="v2:b0864956-446a-ccd6-cec6-a695a0cbc5f5">

    
  <meta name="current-catalog-service-hash" content="f3abb0cc802f3d7b95fc8762b94bdcb13bf39634c40c357301c4aa1d67a256fb">


  <meta name="request-id" content="EA90:1CDBB:635D9AE:84BE128:692A5FE7" data-pjax-transient="true"/><meta name="html-safe-nonce" content="a24224c6452742c63947313b7b227e9d1ed301d9a8d50208b4c6304c9f3e36c9" data-pjax-transient="true"/><meta name="visitor-payload" content="eyJyZWZlcnJlciI6IiIsInJlcXVlc3RfaWQiOiJFQTkwOjFDREJCOjYzNUQ5QUU6ODRCRTEyODo2OTJBNUZFNyIsInZpc2l0b3JfaWQiOiI3MzA1MTcyNTQzOTUxNDk1MTQzIiwicmVnaW9uX2VkZ2UiOiJpYWQiLCJyZWdpb25fcmVuZGVyIjoiaWFkIn0=" data-pjax-transient="true"/><meta name="visitor-hmac" content="2fdaab4bc87910c7665847041ac9fa657723c4fcddfe584eb3dcbc5eba730c53" data-pjax-transient="true"/>


    <meta name="hovercard-subject-tag" content="repository:811638645" data-turbo-transient>


  <meta name="github-keyboard-shortcuts" content="repository,copilot" data-turbo-transient="true" />
  

  <meta name="selected-link" value="repo_source" data-turbo-transient>
  <link rel="assets" href="https://github.githubassets.com/">

    <meta name="google-site-verification" content="Apib7-x98H0j5cPqHWwSMm6dNU4GmODRoqxLiDzdx9I">

<meta name="octolytics-url" content="https://collector.github.com/github/collect" />

  <meta name="analytics-location" content="/&lt;user-name&gt;/&lt;repo-name&gt;" data-turbo-transient="true" />

  




    <meta name="user-login" content="">

  

    <meta name="viewport" content="width=device-width">

    

      <meta name="description" content="[Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI - HCPLab-SYSU/Embodied_AI_Paper_List">

      <link rel="search" type="application/opensearchdescription+xml" href="/opensearch.xml" title="GitHub">

    <link rel="fluid-icon" href="https://github.com/fluidicon.png" title="GitHub">
    <meta property="fb:app_id" content="1401488693436528">
    <meta name="apple-itunes-app" content="app-id=1477376905, app-argument=https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List" />

      <meta name="twitter:image" content="https://opengraph.githubassets.com/3d0188ddc36fda22cd0541b1150da2d009f00ee4244e3e19866e5fdf6f4291d4/HCPLab-SYSU/Embodied_AI_Paper_List" /><meta name="twitter:site" content="@github" /><meta name="twitter:card" content="summary_large_image" /><meta name="twitter:title" content="GitHub - HCPLab-SYSU/Embodied_AI_Paper_List: [Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI" /><meta name="twitter:description" content="[Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI - HCPLab-SYSU/Embodied_AI_Paper_List" />
  <meta property="og:image" content="https://opengraph.githubassets.com/3d0188ddc36fda22cd0541b1150da2d009f00ee4244e3e19866e5fdf6f4291d4/HCPLab-SYSU/Embodied_AI_Paper_List" /><meta property="og:image:alt" content="[Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI - HCPLab-SYSU/Embodied_AI_Paper_List" /><meta property="og:image:width" content="1200" /><meta property="og:image:height" content="600" /><meta property="og:site_name" content="GitHub" /><meta property="og:type" content="object" /><meta property="og:title" content="GitHub - HCPLab-SYSU/Embodied_AI_Paper_List: [Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI" /><meta property="og:url" content="https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List" /><meta property="og:description" content="[Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI - HCPLab-SYSU/Embodied_AI_Paper_List" />
  




      <meta name="hostname" content="github.com">



        <meta name="expected-hostname" content="github.com">


  <meta http-equiv="x-pjax-version" content="f1ef05d97019b1c5904087ccca23886d5604f88ae705d7634cf261e30c6a1d99" data-turbo-track="reload">
  <meta http-equiv="x-pjax-csp-version" content="21a43568025709b66240454fc92d4f09335a96863f8ab1c46b4a07f6a5b67102" data-turbo-track="reload">
  <meta http-equiv="x-pjax-css-version" content="8487fec39c8c06b815832ce80ebd89387f6b3bac9b5498b820bc97b9a51ed30a" data-turbo-track="reload">
  <meta http-equiv="x-pjax-js-version" content="45d2fec5ad6c61133758fb797a1644433467986e76e5ee183a4793856ceeb023" data-turbo-track="reload">

  <meta name="turbo-cache-control" content="no-preview" data-turbo-transient="">

      <meta data-hydrostats="publish">
  <meta name="go-import" content="github.com/HCPLab-SYSU/Embodied_AI_Paper_List git https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.git">

  <meta name="octolytics-dimension-user_id" content="38847349" /><meta name="octolytics-dimension-user_login" content="HCPLab-SYSU" /><meta name="octolytics-dimension-repository_id" content="811638645" /><meta name="octolytics-dimension-repository_nwo" content="HCPLab-SYSU/Embodied_AI_Paper_List" /><meta name="octolytics-dimension-repository_public" content="true" /><meta name="octolytics-dimension-repository_is_fork" content="false" /><meta name="octolytics-dimension-repository_network_root_id" content="811638645" /><meta name="octolytics-dimension-repository_network_root_nwo" content="HCPLab-SYSU/Embodied_AI_Paper_List" />



      <link rel="canonical" href="https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List" data-turbo-transient>


    <meta name="turbo-body-classes" content="logged-out env-production page-responsive">
  <meta name="disable-turbo" content="false">


  <meta name="browser-stats-url" content="https://api.github.com/_private/browser/stats">

  <meta name="browser-errors-url" content="https://api.github.com/_private/browser/errors">

  <meta name="release" content="053169ff9e7f6a2cc2a6e68417b1a5ae5352141c">
  <meta name="ui-target" content="full">

  <link rel="mask-icon" href="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" color="#000000">
  <link rel="alternate icon" class="js-site-favicon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png">
  <link rel="icon" class="js-site-favicon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" data-base-href="https://github.githubassets.com/favicons/favicon">

<meta name="theme-color" content="#1e2327">
<meta name="color-scheme" content="light dark" />


  <link rel="manifest" href="/manifest.json" crossOrigin="use-credentials">

  </head>

  <body class="logged-out env-production page-responsive" style="word-wrap: break-word;" >
    <div data-turbo-body class="logged-out env-production page-responsive" style="word-wrap: break-word;" >
      



    <div class="position-relative header-wrapper js-header-wrapper ">
      <a href="#start-of-content" data-skip-target-assigned="false" class="px-2 py-4 color-bg-accent-emphasis color-fg-on-emphasis show-on-focus js-skip-to-content">Skip to content</a>

      <span data-view-component="true" class="progress-pjax-loader Progress position-fixed width-full">
    <span style="width: 0%;" data-view-component="true" class="Progress-item progress-pjax-loader-bar left-0 top-0 color-bg-accent-emphasis"></span>
</span>      
      
      <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.fbf61c683a537e8f92d5.module.css" />
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/keyboard-shortcuts-dialog.29aaeaafa90f007c6f61.module.css" />

<react-partial
  partial-name="keyboard-shortcuts-dialog"
  data-ssr="false"
  data-attempted-ssr="false"
  data-react-profiling="false"
>
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{"docsUrl":"https://docs.github.com/get-started/accessibility/keyboard-shortcuts"}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>





      

          

              
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/21481-a66c6eab7bbf.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/85110-f7be2f54525a.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/sessions-8cc3729f6e8d.js" defer="defer"></script>

<header class="HeaderMktg header-logged-out js-details-container js-header Details f4 py-3" role="banner" data-is-top="true" data-color-mode=light data-light-theme=light data-dark-theme=dark>
  <h2 class="sr-only">Navigation Menu</h2>

  <button type="button" class="HeaderMktg-backdrop d-lg-none border-0 position-fixed top-0 left-0 width-full height-full js-details-target" aria-label="Toggle navigation">
    <span class="d-none">Toggle navigation</span>
  </button>

  <div class="d-flex flex-column flex-lg-row flex-items-center px-3 px-md-4 px-lg-5 height-full position-relative z-1">
    <div class="d-flex flex-justify-between flex-items-center width-full width-lg-auto">
      <div class="flex-1">
        <button aria-label="Toggle navigation" aria-expanded="false" type="button" data-view-component="true" class="js-details-target js-nav-padding-recalculate js-header-menu-toggle Button--link Button--medium Button d-lg-none color-fg-inherit p-1">  <span class="Button-content">
    <span class="Button-label"><div class="HeaderMenu-toggle-bar rounded my-1"></div>
            <div class="HeaderMenu-toggle-bar rounded my-1"></div>
            <div class="HeaderMenu-toggle-bar rounded my-1"></div></span>
  </span>
</button>
      </div>

      <a class="mr-lg-3 color-fg-inherit flex-order-2 js-prevent-focus-on-mobile-nav"
        href="/"
        aria-label="Homepage"
        data-analytics-event="{&quot;category&quot;:&quot;Marketing nav&quot;,&quot;action&quot;:&quot;click to go to homepage&quot;,&quot;label&quot;:&quot;ref_page:Marketing;ref_cta:Logomark;ref_loc:Header&quot;}">
        <svg height="32" aria-hidden="true" viewBox="0 0 24 24" version="1.1" width="32" data-view-component="true" class="octicon octicon-mark-github">
    <path d="M12 1C5.923 1 1 5.923 1 12c0 4.867 3.149 8.979 7.521 10.436.55.096.756-.233.756-.522 0-.262-.013-1.128-.013-2.049-2.764.509-3.479-.674-3.699-1.292-.124-.317-.66-1.293-1.127-1.554-.385-.207-.936-.715-.014-.729.866-.014 1.485.797 1.691 1.128.99 1.663 2.571 1.196 3.204.907.096-.715.385-1.196.701-1.471-2.448-.275-5.005-1.224-5.005-5.432 0-1.196.426-2.186 1.128-2.956-.111-.275-.496-1.402.11-2.915 0 0 .921-.288 3.024 1.128a10.193 10.193 0 0 1 2.75-.371c.936 0 1.871.123 2.75.371 2.104-1.43 3.025-1.128 3.025-1.128.605 1.513.221 2.64.111 2.915.701.77 1.127 1.747 1.127 2.956 0 4.222-2.571 5.157-5.019 5.432.399.344.743 1.004.743 2.035 0 1.471-.014 2.654-.014 3.025 0 .289.206.632.756.522C19.851 20.979 23 16.854 23 12c0-6.077-4.922-11-11-11Z"></path>
</svg>
      </a>

      <div class="d-flex flex-1 flex-order-2 text-right d-lg-none gap-2 flex-justify-end">
          <a
            href="/login?return_to=https%3A%2F%2Fgithub.com%2FHCPLab-SYSU%2FEmbodied_AI_Paper_List"
            class="HeaderMenu-link HeaderMenu-button d-inline-flex f5 no-underline border color-border-default rounded-2 px-2 py-1 color-fg-inherit js-prevent-focus-on-mobile-nav"
            data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="b3731461444c6fe5a28723de1b21edc08737ccf020676bb63d9dfef3ebc3d605"
            data-analytics-event="{&quot;category&quot;:&quot;Marketing nav&quot;,&quot;action&quot;:&quot;click to Sign in&quot;,&quot;label&quot;:&quot;ref_page:Marketing;ref_cta:Sign in;ref_loc:Header&quot;}"
          >
            Sign in
          </a>
              <div class="AppHeader-appearanceSettings">
    <react-partial-anchor>
      <button data-target="react-partial-anchor.anchor" id="icon-button-af7497f5-26ee-44bf-a7d8-87dc4eb2690a" aria-labelledby="tooltip-3aa036b2-2966-4e75-abfd-a6f4bf48be93" type="button" disabled="disabled" data-view-component="true" class="Button Button--iconOnly Button--invisible Button--medium AppHeader-button HeaderMenu-link border cursor-wait">  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-sliders Button-visual">
    <path d="M15 2.75a.75.75 0 0 1-.75.75h-4a.75.75 0 0 1 0-1.5h4a.75.75 0 0 1 .75.75Zm-8.5.75v1.25a.75.75 0 0 0 1.5 0v-4a.75.75 0 0 0-1.5 0V2H1.75a.75.75 0 0 0 0 1.5H6.5Zm1.25 5.25a.75.75 0 0 0 0-1.5h-6a.75.75 0 0 0 0 1.5h6ZM15 8a.75.75 0 0 1-.75.75H11.5V10a.75.75 0 1 1-1.5 0V6a.75.75 0 0 1 1.5 0v1.25h2.75A.75.75 0 0 1 15 8Zm-9 5.25v-2a.75.75 0 0 0-1.5 0v1.25H1.75a.75.75 0 0 0 0 1.5H4.5v1.25a.75.75 0 0 0 1.5 0v-2Zm9 0a.75.75 0 0 1-.75.75h-6a.75.75 0 0 1 0-1.5h6a.75.75 0 0 1 .75.75Z"></path>
</svg>
</button><tool-tip id="tooltip-3aa036b2-2966-4e75-abfd-a6f4bf48be93" for="icon-button-af7497f5-26ee-44bf-a7d8-87dc4eb2690a" popover="manual" data-direction="s" data-type="label" data-view-component="true" class="sr-only position-absolute">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.fbf61c683a537e8f92d5.module.css" />
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.753d458774a2f782559b.module.css" />

<react-partial
  partial-name="appearance-settings"
  data-ssr="false"
  data-attempted-ssr="false"
  data-react-profiling="false"
>
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>


      </template>
    </react-partial-anchor>
  </div>

      </div>
    </div>


    <div class="HeaderMenu js-header-menu height-fit position-lg-relative d-lg-flex flex-column flex-auto top-0">
      <div class="HeaderMenu-wrapper d-flex flex-column flex-self-start flex-lg-row flex-auto rounded rounded-lg-0">
            <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.fbf61c683a537e8f92d5.module.css" />
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/marketing-navigation.8284bdfe1ee4804a58c1.module.css" />

<react-partial
  partial-name="marketing-navigation"
  data-ssr="true"
  data-attempted-ssr="true"
  data-react-profiling="false"
>
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{"should_use_dotcom_links":true}}</script>
  <div data-target="react-partial.reactRoot"><nav class="MarketingNavigation-module__nav--jA9Zq" aria-label="Global"><ul class="MarketingNavigation-module__list--r_vr2"><li><div class="NavDropdown-module__container--bmXM2 js-details-container js-header-menu-item"><button type="button" class="NavDropdown-module__button--Hq9UR js-details-target" aria-expanded="false">Platform<svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavDropdown-module__buttonIcon--SR0Ke" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></button><div class="NavDropdown-module__dropdown--Ig57Y"><ul class="NavDropdown-module__list--RwSSK"><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">AI CODE CREATION</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/features/copilot" data-analytics-event="{&quot;action&quot;:&quot;github_copilot&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_copilot_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-copilot NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M23.922 16.992c-.861 1.495-5.859 5.023-11.922 5.023-6.063 0-11.061-3.528-11.922-5.023A.641.641 0 0 1 0 16.736v-2.869a.841.841 0 0 1 .053-.22c.372-.935 1.347-2.292 2.605-2.656.167-.429.414-1.055.644-1.517a10.195 10.195 0 0 1-.052-1.086c0-1.331.282-2.499 1.132-3.368.397-.406.89-.717 1.474-.952 1.399-1.136 3.392-2.093 6.122-2.093 2.731 0 4.767.957 6.166 2.093.584.235 1.077.546 1.474.952.85.869 1.132 2.037 1.132 3.368 0 .368-.014.733-.052 1.086.23.462.477 1.088.644 1.517 1.258.364 2.233 1.721 2.605 2.656a.832.832 0 0 1 .053.22v2.869a.641.641 0 0 1-.078.256ZM12.172 11h-.344a4.323 4.323 0 0 1-.355.508C10.703 12.455 9.555 13 7.965 13c-1.725 0-2.989-.359-3.782-1.259a2.005 2.005 0 0 1-.085-.104L4 11.741v6.585c1.435.779 4.514 2.179 8 2.179 3.486 0 6.565-1.4 8-2.179v-6.585l-.098-.104s-.033.045-.085.104c-.793.9-2.057 1.259-3.782 1.259-1.59 0-2.738-.545-3.508-1.492a4.323 4.323 0 0 1-.355-.508h-.016.016Zm.641-2.935c.136 1.057.403 1.913.878 2.497.442.544 1.134.938 2.344.938 1.573 0 2.292-.337 2.657-.751.384-.435.558-1.15.558-2.361 0-1.14-.243-1.847-.705-2.319-.477-.488-1.319-.862-2.824-1.025-1.487-.161-2.192.138-2.533.529-.269.307-.437.808-.438 1.578v.021c0 .265.021.562.063.893Zm-1.626 0c.042-.331.063-.628.063-.894v-.02c-.001-.77-.169-1.271-.438-1.578-.341-.391-1.046-.69-2.533-.529-1.505.163-2.347.537-2.824 1.025-.462.472-.705 1.179-.705 2.319 0 1.211.175 1.926.558 2.361.365.414 1.084.751 2.657.751 1.21 0 1.902-.394 2.344-.938.475-.584.742-1.44.878-2.497Z"></path><path d="M14.5 14.25a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Zm-5 0a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Z"></path></svg><span class="NavLink-module__title--xw3ok">GitHub Copilot</span><span class="NavLink-module__subtitle--qC15H">Write better code with AI</span></div></a></li><li><a href="https://github.com/features/spark" data-analytics-event="{&quot;action&quot;:&quot;github_spark&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_spark_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-sparkle-fill NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M11.296 1.924c.24-.656 1.168-.656 1.408 0l.717 1.958a11.25 11.25 0 0 0 6.697 6.697l1.958.717c.657.24.657 1.168 0 1.408l-1.958.717a11.25 11.25 0 0 0-6.697 6.697l-.717 1.958c-.24.657-1.168.657-1.408 0l-.717-1.958a11.25 11.25 0 0 0-6.697-6.697l-1.958-.717c-.656-.24-.656-1.168 0-1.408l1.958-.717a11.25 11.25 0 0 0 6.697-6.697l.717-1.958Z"></path></svg><span class="NavLink-module__title--xw3ok">GitHub Spark</span><span class="NavLink-module__subtitle--qC15H">Build and deploy intelligent apps</span></div></a></li><li><a href="https://github.com/features/models" data-analytics-event="{&quot;action&quot;:&quot;github_models&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_models_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-ai-model NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M19.375 8.5a3.25 3.25 0 1 1-3.163 4h-3a3.252 3.252 0 0 1-4.443 2.509L7.214 17.76a3.25 3.25 0 1 1-1.342-.674l1.672-2.957A3.238 3.238 0 0 1 6.75 12c0-.907.371-1.727.97-2.316L6.117 6.846A3.253 3.253 0 0 1 1.875 3.75a3.25 3.25 0 1 1 5.526 2.32l1.603 2.836A3.25 3.25 0 0 1 13.093 11h3.119a3.252 3.252 0 0 1 3.163-2.5ZM10 10.25a1.75 1.75 0 1 0-.001 3.499A1.75 1.75 0 0 0 10 10.25ZM5.125 2a1.75 1.75 0 1 0 0 3.5 1.75 1.75 0 0 0 0-3.5Zm12.5 9.75a1.75 1.75 0 1 0 3.5 0 1.75 1.75 0 0 0-3.5 0Zm-14.25 8.5a1.75 1.75 0 1 0 3.501-.001 1.75 1.75 0 0 0-3.501.001Z"></path></svg><span class="NavLink-module__title--xw3ok">GitHub Models</span><span class="NavLink-module__subtitle--qC15H">Manage and compare prompts</span></div></a></li><li><a href="https://github.com/mcp" data-analytics-event="{&quot;action&quot;:&quot;mcp_registry&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;mcp_registry_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-mcp NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M9.795 1.694a4.287 4.287 0 0 1 6.061 0 4.28 4.28 0 0 1 1.181 3.819 4.282 4.282 0 0 1 3.819 1.181 4.287 4.287 0 0 1 0 6.061l-6.793 6.793a.249.249 0 0 0 0 .353l2.617 2.618a.75.75 0 1 1-1.061 1.061l-2.617-2.618a1.75 1.75 0 0 1 0-2.475l6.793-6.793a2.785 2.785 0 1 0-3.939-3.939l-5.9 5.9a.734.734 0 0 1-.249.165.749.749 0 0 1-.812-1.225l5.9-5.901a2.785 2.785 0 1 0-3.939-3.939L2.931 10.68A.75.75 0 1 1 1.87 9.619l7.925-7.925Z"></path><path d="M12.42 4.069a.752.752 0 0 1 1.061 0 .752.752 0 0 1 0 1.061L7.33 11.28a2.788 2.788 0 0 0 0 3.94 2.788 2.788 0 0 0 3.94 0l6.15-6.151a.752.752 0 0 1 1.061 0 .752.752 0 0 1 0 1.061l-6.151 6.15a4.285 4.285 0 1 1-6.06-6.06l6.15-6.151Z"></path></svg><span class="NavLink-module__title--xw3ok">MCP Registry<sup class="NavLink-module__label--MrIhm">New</sup></span><span class="NavLink-module__subtitle--qC15H">Integrate external tools</span></div></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">DEVELOPER WORKFLOWS</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/features/actions" data-analytics-event="{&quot;action&quot;:&quot;actions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;actions_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-workflow NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M1 3a2 2 0 0 1 2-2h6.5a2 2 0 0 1 2 2v6.5a2 2 0 0 1-2 2H7v4.063C7 16.355 7.644 17 8.438 17H12.5v-2.5a2 2 0 0 1 2-2H21a2 2 0 0 1 2 2V21a2 2 0 0 1-2 2h-6.5a2 2 0 0 1-2-2v-2.5H8.437A2.939 2.939 0 0 1 5.5 15.562V11.5H3a2 2 0 0 1-2-2Zm2-.5a.5.5 0 0 0-.5.5v6.5a.5.5 0 0 0 .5.5h6.5a.5.5 0 0 0 .5-.5V3a.5.5 0 0 0-.5-.5ZM14.5 14a.5.5 0 0 0-.5.5V21a.5.5 0 0 0 .5.5H21a.5.5 0 0 0 .5-.5v-6.5a.5.5 0 0 0-.5-.5Z"></path></svg><span class="NavLink-module__title--xw3ok">Actions</span><span class="NavLink-module__subtitle--qC15H">Automate any workflow</span></div></a></li><li><a href="https://github.com/features/codespaces" data-analytics-event="{&quot;action&quot;:&quot;codespaces&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;codespaces_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-codespaces NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.5 3.75C3.5 2.784 4.284 2 5.25 2h13.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 18.75 13H5.25a1.75 1.75 0 0 1-1.75-1.75Zm-2 12c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v4a1.75 1.75 0 0 1-1.75 1.75H3.25a1.75 1.75 0 0 1-1.75-1.75ZM5.25 3.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h13.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Zm-2 12a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h17.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25Z"></path><path d="M10 17.75a.75.75 0 0 1 .75-.75h6.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z"></path></svg><span class="NavLink-module__title--xw3ok">Codespaces</span><span class="NavLink-module__subtitle--qC15H">Instant dev environments</span></div></a></li><li><a href="https://github.com/features/issues" data-analytics-event="{&quot;action&quot;:&quot;issues&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;issues_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-issue-opened NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1ZM2.5 12a9.5 9.5 0 0 0 9.5 9.5 9.5 9.5 0 0 0 9.5-9.5A9.5 9.5 0 0 0 12 2.5 9.5 9.5 0 0 0 2.5 12Zm9.5 2a2 2 0 1 1-.001-3.999A2 2 0 0 1 12 14Z"></path></svg><span class="NavLink-module__title--xw3ok">Issues</span><span class="NavLink-module__subtitle--qC15H">Plan and track work</span></div></a></li><li><a href="https://github.com/features/code-review" data-analytics-event="{&quot;action&quot;:&quot;code_review&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;code_review_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-code NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M15.22 4.97a.75.75 0 0 1 1.06 0l6.5 6.5a.75.75 0 0 1 0 1.06l-6.5 6.5a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L21.19 12l-5.97-5.97a.75.75 0 0 1 0-1.06Zm-6.44 0a.75.75 0 0 1 0 1.06L2.81 12l5.97 5.97a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215l-6.5-6.5a.75.75 0 0 1 0-1.06l6.5-6.5a.75.75 0 0 1 1.06 0Z"></path></svg><span class="NavLink-module__title--xw3ok">Code Review</span><span class="NavLink-module__subtitle--qC15H">Manage code changes</span></div></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">APPLICATION SECURITY</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/security/advanced-security" data-analytics-event="{&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_advanced_security_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-shield-check NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M16.53 9.78a.75.75 0 0 0-1.06-1.06L11 13.19l-1.97-1.97a.75.75 0 0 0-1.06 1.06l2.5 2.5a.75.75 0 0 0 1.06 0l5-5Z"></path><path d="m12.54.637 8.25 2.675A1.75 1.75 0 0 1 22 4.976V10c0 6.19-3.771 10.704-9.401 12.83a1.704 1.704 0 0 1-1.198 0C5.77 20.705 2 16.19 2 10V4.976c0-.758.489-1.43 1.21-1.664L11.46.637a1.748 1.748 0 0 1 1.08 0Zm-.617 1.426-8.25 2.676a.249.249 0 0 0-.173.237V10c0 5.46 3.28 9.483 8.43 11.426a.199.199 0 0 0 .14 0C17.22 19.483 20.5 15.461 20.5 10V4.976a.25.25 0 0 0-.173-.237l-8.25-2.676a.253.253 0 0 0-.154 0Z"></path></svg><span class="NavLink-module__title--xw3ok">GitHub Advanced Security</span><span class="NavLink-module__subtitle--qC15H">Find and fix vulnerabilities</span></div></a></li><li><a href="https://github.com/security/advanced-security/code-security" data-analytics-event="{&quot;action&quot;:&quot;code_security&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;code_security_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-code-square NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M10.3 8.24a.75.75 0 0 1-.04 1.06L7.352 12l2.908 2.7a.75.75 0 1 1-1.02 1.1l-3.5-3.25a.75.75 0 0 1 0-1.1l3.5-3.25a.75.75 0 0 1 1.06.04Zm3.44 1.06a.75.75 0 1 1 1.02-1.1l3.5 3.25a.75.75 0 0 1 0 1.1l-3.5 3.25a.75.75 0 1 1-1.02-1.1l2.908-2.7-2.908-2.7Z"></path><path d="M2 3.75C2 2.784 2.784 2 3.75 2h16.5c.966 0 1.75.784 1.75 1.75v16.5A1.75 1.75 0 0 1 20.25 22H3.75A1.75 1.75 0 0 1 2 20.25Zm1.75-.25a.25.25 0 0 0-.25.25v16.5c0 .138.112.25.25.25h16.5a.25.25 0 0 0 .25-.25V3.75a.25.25 0 0 0-.25-.25Z"></path></svg><span class="NavLink-module__title--xw3ok">Code security</span><span class="NavLink-module__subtitle--qC15H">Secure your code as you build</span></div></a></li><li><a href="https://github.com/security/advanced-security/secret-protection" data-analytics-event="{&quot;action&quot;:&quot;secret_protection&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;secret_protection_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-lock NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6 9V7.25C6 3.845 8.503 1 12 1s6 2.845 6 6.25V9h.5a2.5 2.5 0 0 1 2.5 2.5v8a2.5 2.5 0 0 1-2.5 2.5h-13A2.5 2.5 0 0 1 3 19.5v-8A2.5 2.5 0 0 1 5.5 9Zm-1.5 2.5v8a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-8a1 1 0 0 0-1-1h-13a1 1 0 0 0-1 1Zm3-4.25V9h9V7.25c0-2.67-1.922-4.75-4.5-4.75-2.578 0-4.5 2.08-4.5 4.75Z"></path></svg><span class="NavLink-module__title--xw3ok">Secret protection</span><span class="NavLink-module__subtitle--qC15H">Stop leaks before they start</span></div></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n NavGroup-module__hasSeparator--AJeNz"><span class="NavGroup-module__title--TdKyz">EXPLORE</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/why-github" data-analytics-event="{&quot;action&quot;:&quot;why_github&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;why_github_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Why GitHub</span></a></li><li><a href="https://docs.github.com" data-analytics-event="{&quot;action&quot;:&quot;documentation&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;documentation_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">Documentation</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li><li><a href="https://github.blog" data-analytics-event="{&quot;action&quot;:&quot;blog&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;blog_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">Blog</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li><li><a href="https://github.blog/changelog" data-analytics-event="{&quot;action&quot;:&quot;changelog&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;changelog_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">Changelog</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li><li><a href="https://github.com/marketplace" data-analytics-event="{&quot;action&quot;:&quot;marketplace&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;marketplace_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Marketplace</span></a></li></ul></div></li></ul><div class="NavDropdown-module__trailingLinkContainer--MNB5T"><a href="https://github.com/features" data-analytics-event="{&quot;action&quot;:&quot;view_all_features&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_features_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">View all features</span><svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavLink-module__arrowIcon--g6Lip" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></a></div></div></div></li><li><div class="NavDropdown-module__container--bmXM2 js-details-container js-header-menu-item"><button type="button" class="NavDropdown-module__button--Hq9UR js-details-target" aria-expanded="false">Solutions<svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavDropdown-module__buttonIcon--SR0Ke" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></button><div class="NavDropdown-module__dropdown--Ig57Y"><ul class="NavDropdown-module__list--RwSSK"><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">BY COMPANY SIZE</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/enterprise" data-analytics-event="{&quot;action&quot;:&quot;enterprises&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;enterprises_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Enterprises</span></a></li><li><a href="https://github.com/team" data-analytics-event="{&quot;action&quot;:&quot;small_and_medium_teams&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;small_and_medium_teams_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Small and medium teams</span></a></li><li><a href="https://github.com/enterprise/startups" data-analytics-event="{&quot;action&quot;:&quot;startups&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;startups_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Startups</span></a></li><li><a href="https://github.com/solutions/industry/nonprofits" data-analytics-event="{&quot;action&quot;:&quot;nonprofits&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;nonprofits_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Nonprofits</span></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">BY USE CASE</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/solutions/use-case/app-modernization" data-analytics-event="{&quot;action&quot;:&quot;app_modernization&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;app_modernization_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">App Modernization</span></a></li><li><a href="https://github.com/solutions/use-case/devsecops" data-analytics-event="{&quot;action&quot;:&quot;devsecops&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;devsecops_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">DevSecOps</span></a></li><li><a href="https://github.com/solutions/use-case/devops" data-analytics-event="{&quot;action&quot;:&quot;devops&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;devops_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">DevOps</span></a></li><li><a href="https://github.com/solutions/use-case/ci-cd" data-analytics-event="{&quot;action&quot;:&quot;ci/cd&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;ci/cd_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">CI/CD</span></a></li><li><a href="https://github.com/solutions/use-case" data-analytics-event="{&quot;action&quot;:&quot;view_all_use_cases&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_use_cases_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">View all use cases</span><svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavLink-module__arrowIcon--g6Lip" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">BY INDUSTRY</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/solutions/industry/healthcare" data-analytics-event="{&quot;action&quot;:&quot;healthcare&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;healthcare_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Healthcare</span></a></li><li><a href="https://github.com/solutions/industry/financial-services" data-analytics-event="{&quot;action&quot;:&quot;financial_services&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;financial_services_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Financial services</span></a></li><li><a href="https://github.com/solutions/industry/manufacturing" data-analytics-event="{&quot;action&quot;:&quot;manufacturing&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;manufacturing_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Manufacturing</span></a></li><li><a href="https://github.com/solutions/industry/government" data-analytics-event="{&quot;action&quot;:&quot;government&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;government_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Government</span></a></li><li><a href="https://github.com/solutions/industry" data-analytics-event="{&quot;action&quot;:&quot;view_all_industries&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_industries_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">View all industries</span><svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavLink-module__arrowIcon--g6Lip" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></a></li></ul></div></li></ul><div class="NavDropdown-module__trailingLinkContainer--MNB5T"><a href="https://github.com/solutions" data-analytics-event="{&quot;action&quot;:&quot;view_all_solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_solutions_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">View all solutions</span><svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavLink-module__arrowIcon--g6Lip" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></a></div></div></div></li><li><div class="NavDropdown-module__container--bmXM2 js-details-container js-header-menu-item"><button type="button" class="NavDropdown-module__button--Hq9UR js-details-target" aria-expanded="false">Resources<svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavDropdown-module__buttonIcon--SR0Ke" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></button><div class="NavDropdown-module__dropdown--Ig57Y"><ul class="NavDropdown-module__list--RwSSK"><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">EXPLORE BY TOPIC</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/resources/articles?topic=ai" data-analytics-event="{&quot;action&quot;:&quot;ai&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;ai_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">AI</span></a></li><li><a href="https://github.com/resources/articles?topic=software-development" data-analytics-event="{&quot;action&quot;:&quot;software_development&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;software_development_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Software Development</span></a></li><li><a href="https://github.com/resources/articles?topic=devops" data-analytics-event="{&quot;action&quot;:&quot;devops&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;devops_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">DevOps</span></a></li><li><a href="https://github.com/resources/articles?topic=security" data-analytics-event="{&quot;action&quot;:&quot;security&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;security_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Security</span></a></li><li><a href="https://github.com/resources/articles" data-analytics-event="{&quot;action&quot;:&quot;view_all_topics&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_topics_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">View all topics</span><svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavLink-module__arrowIcon--g6Lip" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">EXPLORE BY TYPE</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/customer-stories" data-analytics-event="{&quot;action&quot;:&quot;customer_stories&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Customer stories</span></a></li><li><a href="https://github.com/resources/events" data-analytics-event="{&quot;action&quot;:&quot;events__webinars&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;events__webinars_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Events &amp; webinars</span></a></li><li><a href="https://github.com/resources/whitepapers" data-analytics-event="{&quot;action&quot;:&quot;ebooks__reports&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;ebooks__reports_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Ebooks &amp; reports</span></a></li><li><a href="https://github.com/solutions/executive-insights" data-analytics-event="{&quot;action&quot;:&quot;business_insights&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;business_insights_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Business insights</span></a></li><li><a href="https://skills.github.com" data-analytics-event="{&quot;action&quot;:&quot;github_skills&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_skills_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">GitHub Skills</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">SUPPORT &amp; SERVICES</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://docs.github.com" data-analytics-event="{&quot;action&quot;:&quot;documentation&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;documentation_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">Documentation</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li><li><a href="https://support.github.com" data-analytics-event="{&quot;action&quot;:&quot;customer_support&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;customer_support_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">Customer support</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li><li><a href="https://github.com/orgs/community/discussions" data-analytics-event="{&quot;action&quot;:&quot;community_forum&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;community_forum_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Community forum</span></a></li><li><a href="https://github.com/trust-center" data-analytics-event="{&quot;action&quot;:&quot;trust_center&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;trust_center_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Trust center</span></a></li><li><a href="https://github.com/partners" data-analytics-event="{&quot;action&quot;:&quot;partners&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Partners</span></a></li></ul></div></li></ul></div></div></li><li><div class="NavDropdown-module__container--bmXM2 js-details-container js-header-menu-item"><button type="button" class="NavDropdown-module__button--Hq9UR js-details-target" aria-expanded="false">Open Source<svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavDropdown-module__buttonIcon--SR0Ke" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></button><div class="NavDropdown-module__dropdown--Ig57Y"><ul class="NavDropdown-module__list--RwSSK"><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">COMMUNITY</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/sponsors" data-analytics-event="{&quot;action&quot;:&quot;github_sponsors&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-sponsor-tiers NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M16.004 1.25C18.311 1.25 20 3.128 20 5.75c0 2.292-1.23 4.464-3.295 6.485-.481.47-.98.909-1.482 1.31l.265 1.32 1.375 7.5a.75.75 0 0 1-.982.844l-3.512-1.207a.75.75 0 0 0-.488 0L8.37 23.209a.75.75 0 0 1-.982-.844l1.378-7.512.261-1.309c-.5-.4-1-.838-1.481-1.31C5.479 10.215 4.25 8.043 4.25 5.75c0-2.622 1.689-4.5 3.996-4.5 1.55 0 2.947.752 3.832 1.967l.047.067.047-.067a4.726 4.726 0 0 1 3.612-1.962l.22-.005ZM13.89 14.531c-.418.285-.828.542-1.218.77l-.18.103a.75.75 0 0 1-.734 0l-.071-.04-.46-.272c-.282-.173-.573-.36-.868-.562l-.121.605-1.145 6.239 2.3-.79a2.248 2.248 0 0 1 1.284-.054l.18.053 2.299.79-1.141-6.226-.125-.616ZM16.004 2.75c-1.464 0-2.731.983-3.159 2.459-.209.721-1.231.721-1.44 0-.428-1.476-1.695-2.459-3.16-2.459-1.44 0-2.495 1.173-2.495 3 0 1.811 1.039 3.647 2.844 5.412a19.624 19.624 0 0 0 3.734 2.84l-.019-.011-.184-.111.147-.088a19.81 19.81 0 0 0 3.015-2.278l.37-.352C17.46 9.397 18.5 7.561 18.5 5.75c0-1.827-1.055-3-2.496-3Z"></path></svg><span class="NavLink-module__title--xw3ok">GitHub Sponsors</span><span class="NavLink-module__subtitle--qC15H">Fund open source developers</span></div></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">PROGRAMS</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://securitylab.github.com" data-analytics-event="{&quot;action&quot;:&quot;security_lab&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;security_lab_link_open_source_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">Security Lab</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li><li><a href="https://maintainers.github.com" data-analytics-event="{&quot;action&quot;:&quot;maintainer_community&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;maintainer_community_link_open_source_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">Maintainer Community</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li><li><a href="https://github.com/accelerator" data-analytics-event="{&quot;action&quot;:&quot;accelerator&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;accelerator_link_open_source_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Accelerator</span></a></li><li><a href="https://archiveprogram.github.com" data-analytics-event="{&quot;action&quot;:&quot;archive_program&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;archive_program_link_open_source_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">Archive Program</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">REPOSITORIES</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/topics" data-analytics-event="{&quot;action&quot;:&quot;topics&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;topics_link_open_source_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Topics</span></a></li><li><a href="https://github.com/trending" data-analytics-event="{&quot;action&quot;:&quot;trending&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;trending_link_open_source_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Trending</span></a></li><li><a href="https://github.com/collections" data-analytics-event="{&quot;action&quot;:&quot;collections&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;collections_link_open_source_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Collections</span></a></li></ul></div></li></ul></div></div></li><li><div class="NavDropdown-module__container--bmXM2 js-details-container js-header-menu-item"><button type="button" class="NavDropdown-module__button--Hq9UR js-details-target" aria-expanded="false">Enterprise<svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavDropdown-module__buttonIcon--SR0Ke" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></button><div class="NavDropdown-module__dropdown--Ig57Y"><ul class="NavDropdown-module__list--RwSSK"><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">ENTERPRISE SOLUTIONS</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/enterprise" data-analytics-event="{&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-stack NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M11.063 1.456a1.749 1.749 0 0 1 1.874 0l8.383 5.316a1.751 1.751 0 0 1 0 2.956l-8.383 5.316a1.749 1.749 0 0 1-1.874 0L2.68 9.728a1.751 1.751 0 0 1 0-2.956Zm1.071 1.267a.25.25 0 0 0-.268 0L3.483 8.039a.25.25 0 0 0 0 .422l8.383 5.316a.25.25 0 0 0 .268 0l8.383-5.316a.25.25 0 0 0 0-.422Z"></path><path d="M1.867 12.324a.75.75 0 0 1 1.035-.232l8.964 5.685a.25.25 0 0 0 .268 0l8.964-5.685a.75.75 0 0 1 .804 1.267l-8.965 5.685a1.749 1.749 0 0 1-1.874 0l-8.965-5.685a.75.75 0 0 1-.231-1.035Z"></path><path d="M1.867 16.324a.75.75 0 0 1 1.035-.232l8.964 5.685a.25.25 0 0 0 .268 0l8.964-5.685a.75.75 0 0 1 .804 1.267l-8.965 5.685a1.749 1.749 0 0 1-1.874 0l-8.965-5.685a.75.75 0 0 1-.231-1.035Z"></path></svg><span class="NavLink-module__title--xw3ok">Enterprise platform</span><span class="NavLink-module__subtitle--qC15H">AI-powered developer platform</span></div></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">AVAILABLE ADD-ONS</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/security/advanced-security" data-analytics-event="{&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_advanced_security_link_enterprise_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-shield-check NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M16.53 9.78a.75.75 0 0 0-1.06-1.06L11 13.19l-1.97-1.97a.75.75 0 0 0-1.06 1.06l2.5 2.5a.75.75 0 0 0 1.06 0l5-5Z"></path><path d="m12.54.637 8.25 2.675A1.75 1.75 0 0 1 22 4.976V10c0 6.19-3.771 10.704-9.401 12.83a1.704 1.704 0 0 1-1.198 0C5.77 20.705 2 16.19 2 10V4.976c0-.758.489-1.43 1.21-1.664L11.46.637a1.748 1.748 0 0 1 1.08 0Zm-.617 1.426-8.25 2.676a.249.249 0 0 0-.173.237V10c0 5.46 3.28 9.483 8.43 11.426a.199.199 0 0 0 .14 0C17.22 19.483 20.5 15.461 20.5 10V4.976a.25.25 0 0 0-.173-.237l-8.25-2.676a.253.253 0 0 0-.154 0Z"></path></svg><span class="NavLink-module__title--xw3ok">GitHub Advanced Security</span><span class="NavLink-module__subtitle--qC15H">Enterprise-grade security features</span></div></a></li><li><a href="https://github.com/features/copilot/copilot-business" data-analytics-event="{&quot;action&quot;:&quot;copilot_for_business&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;copilot_for_business_link_enterprise_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-copilot NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M23.922 16.992c-.861 1.495-5.859 5.023-11.922 5.023-6.063 0-11.061-3.528-11.922-5.023A.641.641 0 0 1 0 16.736v-2.869a.841.841 0 0 1 .053-.22c.372-.935 1.347-2.292 2.605-2.656.167-.429.414-1.055.644-1.517a10.195 10.195 0 0 1-.052-1.086c0-1.331.282-2.499 1.132-3.368.397-.406.89-.717 1.474-.952 1.399-1.136 3.392-2.093 6.122-2.093 2.731 0 4.767.957 6.166 2.093.584.235 1.077.546 1.474.952.85.869 1.132 2.037 1.132 3.368 0 .368-.014.733-.052 1.086.23.462.477 1.088.644 1.517 1.258.364 2.233 1.721 2.605 2.656a.832.832 0 0 1 .053.22v2.869a.641.641 0 0 1-.078.256ZM12.172 11h-.344a4.323 4.323 0 0 1-.355.508C10.703 12.455 9.555 13 7.965 13c-1.725 0-2.989-.359-3.782-1.259a2.005 2.005 0 0 1-.085-.104L4 11.741v6.585c1.435.779 4.514 2.179 8 2.179 3.486 0 6.565-1.4 8-2.179v-6.585l-.098-.104s-.033.045-.085.104c-.793.9-2.057 1.259-3.782 1.259-1.59 0-2.738-.545-3.508-1.492a4.323 4.323 0 0 1-.355-.508h-.016.016Zm.641-2.935c.136 1.057.403 1.913.878 2.497.442.544 1.134.938 2.344.938 1.573 0 2.292-.337 2.657-.751.384-.435.558-1.15.558-2.361 0-1.14-.243-1.847-.705-2.319-.477-.488-1.319-.862-2.824-1.025-1.487-.161-2.192.138-2.533.529-.269.307-.437.808-.438 1.578v.021c0 .265.021.562.063.893Zm-1.626 0c.042-.331.063-.628.063-.894v-.02c-.001-.77-.169-1.271-.438-1.578-.341-.391-1.046-.69-2.533-.529-1.505.163-2.347.537-2.824 1.025-.462.472-.705 1.179-.705 2.319 0 1.211.175 1.926.558 2.361.365.414 1.084.751 2.657.751 1.21 0 1.902-.394 2.344-.938.475-.584.742-1.44.878-2.497Z"></path><path d="M14.5 14.25a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Zm-5 0a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Z"></path></svg><span class="NavLink-module__title--xw3ok">Copilot for Business</span><span class="NavLink-module__subtitle--qC15H">Enterprise-grade AI features</span></div></a></li><li><a href="https://github.com/premium-support" data-analytics-event="{&quot;action&quot;:&quot;premium_support&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;premium_support_link_enterprise_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-comment-discussion NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 14.25 14H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 15.543V14H1.75A1.75 1.75 0 0 1 0 12.25v-9.5C0 1.784.784 1 1.75 1ZM1.5 2.75v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Z"></path><path d="M22.5 8.75a.25.25 0 0 0-.25-.25h-3.5a.75.75 0 0 1 0-1.5h3.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 22.25 20H21v1.543a1.457 1.457 0 0 1-2.487 1.03L15.939 20H10.75A1.75 1.75 0 0 1 9 18.25v-1.465a.75.75 0 0 1 1.5 0v1.465c0 .138.112.25.25.25h5.5a.75.75 0 0 1 .53.22l2.72 2.72v-2.19a.75.75 0 0 1 .75-.75h2a.25.25 0 0 0 .25-.25v-9.5Z"></path></svg><span class="NavLink-module__title--xw3ok">Premium Support</span><span class="NavLink-module__subtitle--qC15H">Enterprise-grade 24/7 support</span></div></a></li></ul></div></li></ul></div></div></li><li><a href="https://github.com/pricing" data-analytics-event="{&quot;action&quot;:&quot;pricing&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;pricing&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;pricing_link_pricing_navbar&quot;}" class="NavLink-module__link--n48VB MarketingNavigation-module__navLink--U9Uuk"><span class="NavLink-module__title--xw3ok">Pricing</span></a></li></ul></nav><script type="application/json" id="__PRIMER_DATA_:R0:__">{"resolvedServerColorMode":"day"}</script></div>
</react-partial>



        <div class="d-flex flex-column flex-lg-row width-full flex-justify-end flex-lg-items-center text-center mt-3 mt-lg-0 text-lg-left ml-lg-3">
                


<qbsearch-input class="search-input" data-scope="repo:HCPLab-SYSU/Embodied_AI_Paper_List" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="t1gop0jRNsGe5lU7lbIPO9fUnBwkVdylgHDjmgK9bHHijbmdO34veHWsxPu3eG-fxP_tKbLUQll2NQyI9_YdNg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="HCPLab-SYSU/Embodied_AI_Paper_List" data-current-org="" data-current-owner="HCPLab-SYSU" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div
    class="search-input-container search-with-dialog position-relative d-flex flex-row flex-items-center mr-4 rounded"
    data-action="click:qbsearch-input#searchInputContainerClicked"
  >
      <button
        type="button"
        class="header-search-button placeholder  input-button form-control d-flex flex-1 flex-self-stretch flex-items-center no-wrap width-full py-0 pl-2 pr-0 text-left border-0 box-shadow-none"
        data-target="qbsearch-input.inputButton"
        aria-label="Search or jump to"
        aria-haspopup="dialog"
        placeholder="Search or jump to..."
        data-hotkey=s,/
        autocapitalize="off"
        data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;searchbar&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;input&quot;,&quot;label&quot;:&quot;searchbar_input_global_navbar&quot;}"
        data-action="click:qbsearch-input#handleExpand"
      >
        <div class="mr-2 color-fg-muted">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-search">
    <path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z"></path>
</svg>
        </div>
        <span class="flex-1" data-target="qbsearch-input.inputButtonText">Search or jump to...</span>
          <div class="d-flex" data-target="qbsearch-input.hotkeyIndicator">
            <svg xmlns="http://www.w3.org/2000/svg" width="22" height="20" aria-hidden="true" class="mr-1"><path fill="none" stroke="#979A9C" opacity=".4" d="M3.5.5h12c1.7 0 3 1.3 3 3v13c0 1.7-1.3 3-3 3h-12c-1.7 0-3-1.3-3-3v-13c0-1.7 1.3-3 3-3z"></path><path fill="#979A9C" d="M11.8 6L8 15.1h-.9L10.8 6h1z"></path></svg>
          </div>
      </button>

    <input type="hidden" name="type" class="js-site-search-type-field">

    
<div class="Overlay--hidden " data-modal-dialog-overlay>
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true" class="Overlay Overlay--width-large Overlay--height-auto">
      <h1 id="search-suggestions-dialog-header" class="sr-only">Search code, repositories, users, issues, pull requests...</h1>
    <div class="Overlay-body Overlay-body--paddingNone">
      
          <div data-view-component="true">        <div class="search-suggestions position-fixed width-full color-shadow-large border color-fg-default color-bg-default overflow-hidden d-flex flex-column query-builder-container"
          style="border-radius: 12px;"
          data-target="qbsearch-input.queryBuilderContainer"
          hidden
        >
          <!-- '"` --><!-- </textarea></xmp> --></option></form><form id="query-builder-test-form" action="" accept-charset="UTF-8" method="get">
  <query-builder data-target="qbsearch-input.queryBuilder" id="query-builder-query-builder-test" data-filter-key=":" data-view-component="true" class="QueryBuilder search-query-builder">
    <div class="FormControl FormControl--fullWidth">
      <label id="query-builder-test-label" for="query-builder-test" class="FormControl-label sr-only">
        Search
      </label>
      <div
        class="QueryBuilder-StyledInput width-fit "
        data-target="query-builder.styledInput"
      >
          <span id="query-builder-test-leadingvisual-wrap" class="FormControl-input-leadingVisualWrap QueryBuilder-leadingVisualWrap">
            <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-search FormControl-input-leadingVisual">
    <path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z"></path>
</svg>
          </span>
        <div data-target="query-builder.styledInputContainer" class="QueryBuilder-StyledInputContainer">
          <div
            aria-hidden="true"
            class="QueryBuilder-StyledInputContent"
            data-target="query-builder.styledInputContent"
          ></div>
          <div class="QueryBuilder-InputWrapper">
            <div aria-hidden="true" class="QueryBuilder-Sizer" data-target="query-builder.sizer"></div>
            <input id="query-builder-test" name="query-builder-test" value="" autocomplete="off" type="text" role="combobox" spellcheck="false" aria-expanded="false" aria-describedby="validation-c578652e-ee61-4e64-b254-db84bbfe9bb3" data-target="query-builder.input" data-action="
          input:query-builder#inputChange
          blur:query-builder#inputBlur
          keydown:query-builder#inputKeydown
          focus:query-builder#inputFocus
        " data-view-component="true" class="FormControl-input QueryBuilder-Input FormControl-medium" />
          </div>
        </div>
          <span class="sr-only" id="query-builder-test-clear">Clear</span>
          <button role="button" id="query-builder-test-clear-button" aria-labelledby="query-builder-test-clear query-builder-test-label" data-target="query-builder.clearButton" data-action="
                click:query-builder#clear
                focus:query-builder#clearButtonFocus
                blur:query-builder#clearButtonBlur
              " variant="small" hidden="hidden" type="button" data-view-component="true" class="Button Button--iconOnly Button--invisible Button--medium mr-1 px-2 py-0 d-flex flex-items-center rounded-1 color-fg-muted">  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x-circle-fill Button-visual">
    <path d="M2.343 13.657A8 8 0 1 1 13.658 2.343 8 8 0 0 1 2.343 13.657ZM6.03 4.97a.751.751 0 0 0-1.042.018.751.751 0 0 0-.018 1.042L6.94 8 4.97 9.97a.749.749 0 0 0 .326 1.275.749.749 0 0 0 .734-.215L8 9.06l1.97 1.97a.749.749 0 0 0 1.275-.326.749.749 0 0 0-.215-.734L9.06 8l1.97-1.97a.749.749 0 0 0-.326-1.275.749.749 0 0 0-.734.215L8 6.94Z"></path>
</svg>
</button>

      </div>
      <template id="search-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-search">
    <path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z"></path>
</svg>
</template>

<template id="code-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-code">
    <path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path>
</svg>
</template>

<template id="file-code-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-file-code">
    <path d="M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0 1 14.25 15h-9a.75.75 0 0 1 0-1.5h9a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 10 4.25V1.5H5.75a.25.25 0 0 0-.25.25v2.5a.75.75 0 0 1-1.5 0Zm1.72 4.97a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734l1.47-1.47-1.47-1.47a.75.75 0 0 1 0-1.06ZM3.28 7.78 1.81 9.25l1.47 1.47a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-2-2a.75.75 0 0 1 0-1.06l2-2a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Zm8.22-6.218V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z"></path>
</svg>
</template>

<template id="history-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-history">
    <path d="m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z"></path>
</svg>
</template>

<template id="repo-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo">
    <path d="M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z"></path>
</svg>
</template>

<template id="bookmark-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-bookmark">
    <path d="M3 2.75C3 1.784 3.784 1 4.75 1h6.5c.966 0 1.75.784 1.75 1.75v11.5a.75.75 0 0 1-1.227.579L8 11.722l-3.773 3.107A.751.751 0 0 1 3 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.91l3.023-2.489a.75.75 0 0 1 .954 0l3.023 2.49V2.75a.25.25 0 0 0-.25-.25Z"></path>
</svg>
</template>

<template id="plus-circle-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-plus-circle">
    <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm7.25-3.25v2.5h2.5a.75.75 0 0 1 0 1.5h-2.5v2.5a.75.75 0 0 1-1.5 0v-2.5h-2.5a.75.75 0 0 1 0-1.5h2.5v-2.5a.75.75 0 0 1 1.5 0Z"></path>
</svg>
</template>

<template id="circle-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-dot-fill">
    <path d="M8 4a4 4 0 1 1 0 8 4 4 0 0 1 0-8Z"></path>
</svg>
</template>

<template id="trash-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-trash">
    <path d="M11 1.75V3h2.25a.75.75 0 0 1 0 1.5H2.75a.75.75 0 0 1 0-1.5H5V1.75C5 .784 5.784 0 6.75 0h2.5C10.216 0 11 .784 11 1.75ZM4.496 6.675l.66 6.6a.25.25 0 0 0 .249.225h5.19a.25.25 0 0 0 .249-.225l.66-6.6a.75.75 0 0 1 1.492.149l-.66 6.6A1.748 1.748 0 0 1 10.595 15h-5.19a1.75 1.75 0 0 1-1.741-1.575l-.66-6.6a.75.75 0 1 1 1.492-.15ZM6.5 1.75V3h3V1.75a.25.25 0 0 0-.25-.25h-2.5a.25.25 0 0 0-.25.25Z"></path>
</svg>
</template>

<template id="team-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-people">
    <path d="M2 5.5a3.5 3.5 0 1 1 5.898 2.549 5.508 5.508 0 0 1 3.034 4.084.75.75 0 1 1-1.482.235 4 4 0 0 0-7.9 0 .75.75 0 0 1-1.482-.236A5.507 5.507 0 0 1 3.102 8.05 3.493 3.493 0 0 1 2 5.5ZM11 4a3.001 3.001 0 0 1 2.22 5.018 5.01 5.01 0 0 1 2.56 3.012.749.749 0 0 1-.885.954.752.752 0 0 1-.549-.514 3.507 3.507 0 0 0-2.522-2.372.75.75 0 0 1-.574-.73v-.352a.75.75 0 0 1 .416-.672A1.5 1.5 0 0 0 11 5.5.75.75 0 0 1 11 4Zm-5.5-.5a2 2 0 1 0-.001 3.999A2 2 0 0 0 5.5 3.5Z"></path>
</svg>
</template>

<template id="project-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-project">
    <path d="M1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0ZM1.5 1.75v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25ZM11.75 3a.75.75 0 0 1 .75.75v7.5a.75.75 0 0 1-1.5 0v-7.5a.75.75 0 0 1 .75-.75Zm-8.25.75a.75.75 0 0 1 1.5 0v5.5a.75.75 0 0 1-1.5 0ZM8 3a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 3Z"></path>
</svg>
</template>

<template id="pencil-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-pencil">
    <path d="M11.013 1.427a1.75 1.75 0 0 1 2.474 0l1.086 1.086a1.75 1.75 0 0 1 0 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 0 1-.927-.928l.929-3.25c.081-.286.235-.547.445-.758l8.61-8.61Zm.176 4.823L9.75 4.81l-6.286 6.287a.253.253 0 0 0-.064.108l-.558 1.953 1.953-.558a.253.253 0 0 0 .108-.064Zm1.238-3.763a.25.25 0 0 0-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 0 0 0-.354Z"></path>
</svg>
</template>

<template id="copilot-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copilot">
    <path d="M7.998 15.035c-4.562 0-7.873-2.914-7.998-3.749V9.338c.085-.628.677-1.686 1.588-2.065.013-.07.024-.143.036-.218.029-.183.06-.384.126-.612-.201-.508-.254-1.084-.254-1.656 0-.87.128-1.769.693-2.484.579-.733 1.494-1.124 2.724-1.261 1.206-.134 2.262.034 2.944.765.05.053.096.108.139.165.044-.057.094-.112.143-.165.682-.731 1.738-.899 2.944-.765 1.23.137 2.145.528 2.724 1.261.566.715.693 1.614.693 2.484 0 .572-.053 1.148-.254 1.656.066.228.098.429.126.612.012.076.024.148.037.218.924.385 1.522 1.471 1.591 2.095v1.872c0 .766-3.351 3.795-8.002 3.795Zm0-1.485c2.28 0 4.584-1.11 5.002-1.433V7.862l-.023-.116c-.49.21-1.075.291-1.727.291-1.146 0-2.059-.327-2.71-.991A3.222 3.222 0 0 1 8 6.303a3.24 3.24 0 0 1-.544.743c-.65.664-1.563.991-2.71.991-.652 0-1.236-.081-1.727-.291l-.023.116v4.255c.419.323 2.722 1.433 5.002 1.433ZM6.762 2.83c-.193-.206-.637-.413-1.682-.297-1.019.113-1.479.404-1.713.7-.247.312-.369.789-.369 1.554 0 .793.129 1.171.308 1.371.162.181.519.379 1.442.379.853 0 1.339-.235 1.638-.54.315-.322.527-.827.617-1.553.117-.935-.037-1.395-.241-1.614Zm4.155-.297c-1.044-.116-1.488.091-1.681.297-.204.219-.359.679-.242 1.614.091.726.303 1.231.618 1.553.299.305.784.54 1.638.54.922 0 1.28-.198 1.442-.379.179-.2.308-.578.308-1.371 0-.765-.123-1.242-.37-1.554-.233-.296-.693-.587-1.713-.7Z"></path><path d="M6.25 9.037a.75.75 0 0 1 .75.75v1.501a.75.75 0 0 1-1.5 0V9.787a.75.75 0 0 1 .75-.75Zm4.25.75v1.501a.75.75 0 0 1-1.5 0V9.787a.75.75 0 0 1 1.5 0Z"></path>
</svg>
</template>

<template id="copilot-error-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copilot-error">
    <path d="M16 11.24c0 .112-.072.274-.21.467L13 9.688V7.862l-.023-.116c-.49.21-1.075.291-1.727.291-.198 0-.388-.009-.571-.029L6.833 5.226a4.01 4.01 0 0 0 .17-.782c.117-.935-.037-1.395-.241-1.614-.193-.206-.637-.413-1.682-.297-.683.076-1.115.231-1.395.415l-1.257-.91c.579-.564 1.413-.877 2.485-.996 1.206-.134 2.262.034 2.944.765.05.053.096.108.139.165.044-.057.094-.112.143-.165.682-.731 1.738-.899 2.944-.765 1.23.137 2.145.528 2.724 1.261.566.715.693 1.614.693 2.484 0 .572-.053 1.148-.254 1.656.066.228.098.429.126.612.012.076.024.148.037.218.924.385 1.522 1.471 1.591 2.095Zm-5.083-8.707c-1.044-.116-1.488.091-1.681.297-.204.219-.359.679-.242 1.614.091.726.303 1.231.618 1.553.299.305.784.54 1.638.54.922 0 1.28-.198 1.442-.379.179-.2.308-.578.308-1.371 0-.765-.123-1.242-.37-1.554-.233-.296-.693-.587-1.713-.7Zm2.511 11.074c-1.393.776-3.272 1.428-5.43 1.428-4.562 0-7.873-2.914-7.998-3.749V9.338c.085-.628.677-1.686 1.588-2.065.013-.07.024-.143.036-.218.029-.183.06-.384.126-.612-.18-.455-.241-.963-.252-1.475L.31 4.107A.747.747 0 0 1 0 3.509V3.49a.748.748 0 0 1 .625-.73c.156-.026.306.047.435.139l14.667 10.578a.592.592 0 0 1 .227.264.752.752 0 0 1 .046.249v.022a.75.75 0 0 1-1.19.596Zm-1.367-.991L5.635 7.964a5.128 5.128 0 0 1-.889.073c-.652 0-1.236-.081-1.727-.291l-.023.116v4.255c.419.323 2.722 1.433 5.002 1.433 1.539 0 3.089-.505 4.063-.934Z"></path>
</svg>
</template>

<template id="workflow-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-workflow">
    <path d="M0 1.75C0 .784.784 0 1.75 0h3.5C6.216 0 7 .784 7 1.75v3.5A1.75 1.75 0 0 1 5.25 7H4v4a1 1 0 0 0 1 1h4v-1.25C9 9.784 9.784 9 10.75 9h3.5c.966 0 1.75.784 1.75 1.75v3.5A1.75 1.75 0 0 1 14.25 16h-3.5A1.75 1.75 0 0 1 9 14.25v-.75H5A2.5 2.5 0 0 1 2.5 11V7h-.75A1.75 1.75 0 0 1 0 5.25Zm1.75-.25a.25.25 0 0 0-.25.25v3.5c0 .138.112.25.25.25h3.5a.25.25 0 0 0 .25-.25v-3.5a.25.25 0 0 0-.25-.25Zm9 9a.25.25 0 0 0-.25.25v3.5c0 .138.112.25.25.25h3.5a.25.25 0 0 0 .25-.25v-3.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
</template>

<template id="book-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-book">
    <path d="M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z"></path>
</svg>
</template>

<template id="code-review-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-code-review">
    <path d="M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 13H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25v-8.5C0 1.784.784 1 1.75 1ZM1.5 2.75v8.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-8.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Zm5.28 1.72a.75.75 0 0 1 0 1.06L5.31 7l1.47 1.47a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-2-2a.75.75 0 0 1 0-1.06l2-2a.75.75 0 0 1 1.06 0Zm2.44 0a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L10.69 7 9.22 5.53a.75.75 0 0 1 0-1.06Z"></path>
</svg>
</template>

<template id="codespaces-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-codespaces">
    <path d="M0 11.25c0-.966.784-1.75 1.75-1.75h12.5c.966 0 1.75.784 1.75 1.75v3A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25Zm2-9.5C2 .784 2.784 0 3.75 0h8.5C13.216 0 14 .784 14 1.75v5a1.75 1.75 0 0 1-1.75 1.75h-8.5A1.75 1.75 0 0 1 2 6.75Zm1.75-.25a.25.25 0 0 0-.25.25v5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-5a.25.25 0 0 0-.25-.25Zm-2 9.5a.25.25 0 0 0-.25.25v3c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-3a.25.25 0 0 0-.25-.25Z"></path><path d="M7 12.75a.75.75 0 0 1 .75-.75h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z"></path>
</svg>
</template>

<template id="comment-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-comment">
    <path d="M1 2.75C1 1.784 1.784 1 2.75 1h10.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 13.25 12H9.06l-2.573 2.573A1.458 1.458 0 0 1 4 13.543V12H2.75A1.75 1.75 0 0 1 1 10.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h4.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
</template>

<template id="comment-discussion-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-comment-discussion">
    <path d="M1.75 1h8.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 10.25 10H7.061l-2.574 2.573A1.458 1.458 0 0 1 2 11.543V10h-.25A1.75 1.75 0 0 1 0 8.25v-5.5C0 1.784.784 1 1.75 1ZM1.5 2.75v5.5c0 .138.112.25.25.25h1a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h3.5a.25.25 0 0 0 .25-.25v-5.5a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25Zm13 2a.25.25 0 0 0-.25-.25h-.5a.75.75 0 0 1 0-1.5h.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 14.25 12H14v1.543a1.458 1.458 0 0 1-2.487 1.03L9.22 12.28a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l2.22 2.22v-2.19a.75.75 0 0 1 .75-.75h1a.25.25 0 0 0 .25-.25Z"></path>
</svg>
</template>

<template id="organization-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-organization">
    <path d="M1.75 16A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0h8.5C11.216 0 12 .784 12 1.75v12.5c0 .085-.006.168-.018.25h2.268a.25.25 0 0 0 .25-.25V8.285a.25.25 0 0 0-.111-.208l-1.055-.703a.749.749 0 1 1 .832-1.248l1.055.703c.487.325.779.871.779 1.456v5.965A1.75 1.75 0 0 1 14.25 16h-3.5a.766.766 0 0 1-.197-.026c-.099.017-.2.026-.303.026h-3a.75.75 0 0 1-.75-.75V14h-1v1.25a.75.75 0 0 1-.75.75Zm-.25-1.75c0 .138.112.25.25.25H4v-1.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 .75.75v1.25h2.25a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25ZM3.75 6h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5ZM3 3.75A.75.75 0 0 1 3.75 3h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 3.75Zm4 3A.75.75 0 0 1 7.75 6h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 7 6.75ZM7.75 3h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5ZM3 9.75A.75.75 0 0 1 3.75 9h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 9.75ZM7.75 9h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5Z"></path>
</svg>
</template>

<template id="rocket-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-rocket">
    <path d="M14.064 0h.186C15.216 0 16 .784 16 1.75v.186a8.752 8.752 0 0 1-2.564 6.186l-.458.459c-.314.314-.641.616-.979.904v3.207c0 .608-.315 1.172-.833 1.49l-2.774 1.707a.749.749 0 0 1-1.11-.418l-.954-3.102a1.214 1.214 0 0 1-.145-.125L3.754 9.816a1.218 1.218 0 0 1-.124-.145L.528 8.717a.749.749 0 0 1-.418-1.11l1.71-2.774A1.748 1.748 0 0 1 3.31 4h3.204c.288-.338.59-.665.904-.979l.459-.458A8.749 8.749 0 0 1 14.064 0ZM8.938 3.623h-.002l-.458.458c-.76.76-1.437 1.598-2.02 2.5l-1.5 2.317 2.143 2.143 2.317-1.5c.902-.583 1.74-1.26 2.499-2.02l.459-.458a7.25 7.25 0 0 0 2.123-5.127V1.75a.25.25 0 0 0-.25-.25h-.186a7.249 7.249 0 0 0-5.125 2.123ZM3.56 14.56c-.732.732-2.334 1.045-3.005 1.148a.234.234 0 0 1-.201-.064.234.234 0 0 1-.064-.201c.103-.671.416-2.273 1.15-3.003a1.502 1.502 0 1 1 2.12 2.12Zm6.94-3.935c-.088.06-.177.118-.266.175l-2.35 1.521.548 1.783 1.949-1.2a.25.25 0 0 0 .119-.213ZM3.678 8.116 5.2 5.766c.058-.09.117-.178.176-.266H3.309a.25.25 0 0 0-.213.119l-1.2 1.95ZM12 5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
</template>

<template id="shield-check-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-shield-check">
    <path d="m8.533.133 5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667l5.25-1.68a1.748 1.748 0 0 1 1.066 0Zm-.61 1.429.001.001-5.25 1.68a.251.251 0 0 0-.174.237V7c0 1.36.275 2.666 1.057 3.859.784 1.194 2.121 2.342 4.366 3.298a.196.196 0 0 0 .154 0c2.245-.957 3.582-2.103 4.366-3.297C13.225 9.666 13.5 8.358 13.5 7V3.48a.25.25 0 0 0-.174-.238l-5.25-1.68a.25.25 0 0 0-.153 0ZM11.28 6.28l-3.5 3.5a.75.75 0 0 1-1.06 0l-1.5-1.5a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l.97.97 2.97-2.97a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z"></path>
</svg>
</template>

<template id="heart-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-heart">
    <path d="m8 14.25.345.666a.75.75 0 0 1-.69 0l-.008-.004-.018-.01a7.152 7.152 0 0 1-.31-.17 22.055 22.055 0 0 1-3.434-2.414C2.045 10.731 0 8.35 0 5.5 0 2.836 2.086 1 4.25 1 5.797 1 7.153 1.802 8 3.02 8.847 1.802 10.203 1 11.75 1 13.914 1 16 2.836 16 5.5c0 2.85-2.045 5.231-3.885 6.818a22.066 22.066 0 0 1-3.744 2.584l-.018.01-.006.003h-.002ZM4.25 2.5c-1.336 0-2.75 1.164-2.75 3 0 2.15 1.58 4.144 3.365 5.682A20.58 20.58 0 0 0 8 13.393a20.58 20.58 0 0 0 3.135-2.211C12.92 9.644 14.5 7.65 14.5 5.5c0-1.836-1.414-3-2.75-3-1.373 0-2.609.986-3.029 2.456a.749.749 0 0 1-1.442 0C6.859 3.486 5.623 2.5 4.25 2.5Z"></path>
</svg>
</template>

<template id="server-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-server">
    <path d="M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v4c0 .372-.116.717-.314 1 .198.283.314.628.314 1v4a1.75 1.75 0 0 1-1.75 1.75H1.75A1.75 1.75 0 0 1 0 12.75v-4c0-.358.109-.707.314-1a1.739 1.739 0 0 1-.314-1v-4C0 1.784.784 1 1.75 1ZM1.5 2.75v4c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Zm.25 5.75a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25ZM7 4.75A.75.75 0 0 1 7.75 4h4.5a.75.75 0 0 1 0 1.5h-4.5A.75.75 0 0 1 7 4.75ZM7.75 10h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM3 4.75A.75.75 0 0 1 3.75 4h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 4.75ZM3.75 10h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5Z"></path>
</svg>
</template>

<template id="globe-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-globe">
    <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM5.78 8.75a9.64 9.64 0 0 0 1.363 4.177c.255.426.542.832.857 1.215.245-.296.551-.705.857-1.215A9.64 9.64 0 0 0 10.22 8.75Zm4.44-1.5a9.64 9.64 0 0 0-1.363-4.177c-.307-.51-.612-.919-.857-1.215a9.927 9.927 0 0 0-.857 1.215A9.64 9.64 0 0 0 5.78 7.25Zm-5.944 1.5H1.543a6.507 6.507 0 0 0 4.666 5.5c-.123-.181-.24-.365-.352-.552-.715-1.192-1.437-2.874-1.581-4.948Zm-2.733-1.5h2.733c.144-2.074.866-3.756 1.58-4.948.12-.197.237-.381.353-.552a6.507 6.507 0 0 0-4.666 5.5Zm10.181 1.5c-.144 2.074-.866 3.756-1.58 4.948-.12.197-.237.381-.353.552a6.507 6.507 0 0 0 4.666-5.5Zm2.733-1.5a6.507 6.507 0 0 0-4.666-5.5c.123.181.24.365.353.552.714 1.192 1.436 2.874 1.58 4.948Z"></path>
</svg>
</template>

<template id="issue-opened-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-issue-opened">
    <path d="M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path><path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z"></path>
</svg>
</template>

<template id="device-mobile-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-device-mobile">
    <path d="M3.75 0h8.5C13.216 0 14 .784 14 1.75v12.5A1.75 1.75 0 0 1 12.25 16h-8.5A1.75 1.75 0 0 1 2 14.25V1.75C2 .784 2.784 0 3.75 0ZM3.5 1.75v12.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25ZM8 13a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path>
</svg>
</template>

<template id="package-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-package">
    <path d="m8.878.392 5.25 3.045c.54.314.872.89.872 1.514v6.098a1.75 1.75 0 0 1-.872 1.514l-5.25 3.045a1.75 1.75 0 0 1-1.756 0l-5.25-3.045A1.75 1.75 0 0 1 1 11.049V4.951c0-.624.332-1.201.872-1.514L7.122.392a1.75 1.75 0 0 1 1.756 0ZM7.875 1.69l-4.63 2.685L8 7.133l4.755-2.758-4.63-2.685a.248.248 0 0 0-.25 0ZM2.5 5.677v5.372c0 .09.047.171.125.216l4.625 2.683V8.432Zm6.25 8.271 4.625-2.683a.25.25 0 0 0 .125-.216V5.677L8.75 8.432Z"></path>
</svg>
</template>

<template id="credit-card-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-credit-card">
    <path d="M10.75 9a.75.75 0 0 0 0 1.5h1.5a.75.75 0 0 0 0-1.5h-1.5Z"></path><path d="M0 3.75C0 2.784.784 2 1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 14H1.75A1.75 1.75 0 0 1 0 12.25ZM14.5 6.5h-13v5.75c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25Zm0-2.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25V5h13Z"></path>
</svg>
</template>

<template id="play-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-play">
    <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z"></path>
</svg>
</template>

<template id="gift-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-gift">
    <path d="M2 2.75A2.75 2.75 0 0 1 4.75 0c.983 0 1.873.42 2.57 1.232.268.318.497.668.68 1.042.183-.375.411-.725.68-1.044C9.376.42 10.266 0 11.25 0a2.75 2.75 0 0 1 2.45 4h.55c.966 0 1.75.784 1.75 1.75v2c0 .698-.409 1.301-1 1.582v4.918A1.75 1.75 0 0 1 13.25 16H2.75A1.75 1.75 0 0 1 1 14.25V9.332C.409 9.05 0 8.448 0 7.75v-2C0 4.784.784 4 1.75 4h.55c-.192-.375-.3-.8-.3-1.25ZM7.25 9.5H2.5v4.75c0 .138.112.25.25.25h4.5Zm1.5 0v5h4.5a.25.25 0 0 0 .25-.25V9.5Zm0-4V8h5.5a.25.25 0 0 0 .25-.25v-2a.25.25 0 0 0-.25-.25Zm-7 0a.25.25 0 0 0-.25.25v2c0 .138.112.25.25.25h5.5V5.5h-5.5Zm3-4a1.25 1.25 0 0 0 0 2.5h2.309c-.233-.818-.542-1.401-.878-1.793-.43-.502-.915-.707-1.431-.707ZM8.941 4h2.309a1.25 1.25 0 0 0 0-2.5c-.516 0-1 .205-1.43.707-.337.392-.646.975-.879 1.793Z"></path>
</svg>
</template>

<template id="code-square-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-code-square">
    <path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25Zm7.47 3.97a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L10.69 8 9.22 6.53a.75.75 0 0 1 0-1.06ZM6.78 6.53 5.31 8l1.47 1.47a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215l-2-2a.75.75 0 0 1 0-1.06l2-2a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z"></path>
</svg>
</template>

<template id="device-desktop-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-device-desktop">
    <path d="M14.25 1c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 14.25 12h-3.727c.099 1.041.52 1.872 1.292 2.757A.752.752 0 0 1 11.25 16h-6.5a.75.75 0 0 1-.565-1.243c.772-.885 1.192-1.716 1.292-2.757H1.75A1.75 1.75 0 0 1 0 10.25v-7.5C0 1.784.784 1 1.75 1ZM1.75 2.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25ZM9.018 12H6.982a5.72 5.72 0 0 1-.765 2.5h3.566a5.72 5.72 0 0 1-.765-2.5Z"></path>
</svg>
</template>

        <div class="position-relative">
                <ul
                  role="listbox"
                  class="ActionListWrap QueryBuilder-ListWrap"
                  aria-label="Suggestions"
                  data-action="
                    combobox-commit:query-builder#comboboxCommit
                    mousedown:query-builder#resultsMousedown
                  "
                  data-target="query-builder.resultsList"
                  data-persist-list=false
                  id="query-builder-test-results"
                  tabindex="-1"
                ></ul>
        </div>
      <div class="FormControl-inlineValidation" id="validation-c578652e-ee61-4e64-b254-db84bbfe9bb3" hidden="hidden">
        <span class="FormControl-inlineValidation--visual">
          <svg aria-hidden="true" height="12" viewBox="0 0 12 12" version="1.1" width="12" data-view-component="true" class="octicon octicon-alert-fill">
    <path d="M4.855.708c.5-.896 1.79-.896 2.29 0l4.675 8.351a1.312 1.312 0 0 1-1.146 1.954H1.33A1.313 1.313 0 0 1 .183 9.058ZM7 7V3H5v4Zm-1 3a1 1 0 1 0 0-2 1 1 0 0 0 0 2Z"></path>
</svg>
        </span>
        <span></span>
</div>    </div>
    <div data-target="query-builder.screenReaderFeedback" aria-live="polite" aria-atomic="true" class="sr-only"></div>
</query-builder></form>
          <div class="d-flex flex-row color-fg-muted px-3 text-small color-bg-default search-feedback-prompt">
            <a target="_blank" href="https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax" data-view-component="true" class="Link color-fg-accent text-normal ml-2">Search syntax tips</a>            <div class="d-flex flex-1"></div>
          </div>
        </div>
</div>

    </div>
</modal-dialog></div>
  </div>
  <div data-action="click:qbsearch-input#retract" class="dark-backdrop position-fixed" hidden data-target="qbsearch-input.darkBackdrop"></div>
  <div class="color-fg-default">
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true" class="Overlay Overlay-whenNarrow Overlay--size-medium Overlay--motion-scaleFade Overlay--disableScroll">
    <div data-view-component="true" class="Overlay-header">
  <div class="Overlay-headerContentWrap">
    <div class="Overlay-titleWrap">
      <h1 class="Overlay-title " id="feedback-dialog-title">
        Provide feedback
      </h1>
        
    </div>
    <div class="Overlay-actionWrap">
      <button data-close-dialog-id="feedback-dialog" aria-label="Close" aria-label="Close" type="button" data-view-component="true" class="close-button Overlay-closeButton"><svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg></button>
    </div>
  </div>
  
</div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        <div data-view-component="true" class="Overlay-body">        <!-- '"` --><!-- </textarea></xmp> --></option></form><form id="code-search-feedback-form" data-turbo="false" action="/search/feedback" accept-charset="UTF-8" method="post"><input type="hidden" data-csrf="true" name="authenticity_token" value="17QrhGnIN3YoGoGe3AyIdlK8i18URC6F99MOiAp7uYAujHU7vUdry8QqoB9nfxevmxmq8705/f971C99NFDIRA==" />
          <p>We read every piece of feedback, and take your input very seriously.</p>
          <textarea name="feedback" class="form-control width-full mb-2" style="height: 120px" id="feedback"></textarea>
          <input name="include_email" id="include_email" aria-label="Include my email address so I can be contacted" class="form-control mr-2" type="checkbox">
          <label for="include_email" style="font-weight: normal">Include my email address so I can be contacted</label>
</form></div>
      </scrollable-region>
      <div data-view-component="true" class="Overlay-footer Overlay-footer--alignEnd">          <button data-close-dialog-id="feedback-dialog" type="button" data-view-component="true" class="btn">    Cancel
</button>
          <button form="code-search-feedback-form" data-action="click:qbsearch-input#submitFeedback" type="submit" data-view-component="true" class="btn-primary btn">    Submit feedback
</button>
</div>
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true" class="Overlay Overlay-whenNarrow Overlay--size-medium Overlay--motion-scaleFade Overlay--disableScroll">
    <div data-view-component="true" class="Overlay-header Overlay-header--divided">
  <div class="Overlay-headerContentWrap">
    <div class="Overlay-titleWrap">
      <h1 class="Overlay-title " id="custom-scopes-dialog-title">
        Saved searches
      </h1>
        <h2 id="custom-scopes-dialog-description" class="Overlay-description">Use saved searches to filter your results more quickly</h2>
    </div>
    <div class="Overlay-actionWrap">
      <button data-close-dialog-id="custom-scopes-dialog" aria-label="Close" aria-label="Close" type="button" data-view-component="true" class="close-button Overlay-closeButton"><svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg></button>
    </div>
  </div>
  
</div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        <div data-view-component="true" class="Overlay-body">        <div data-target="custom-scopes.customScopesModalDialogFlash"></div>

        <div hidden class="create-custom-scope-form" data-target="custom-scopes.createCustomScopeForm">
        <!-- '"` --><!-- </textarea></xmp> --></option></form><form id="custom-scopes-dialog-form" data-turbo="false" action="/search/custom_scopes" accept-charset="UTF-8" method="post"><input type="hidden" data-csrf="true" name="authenticity_token" value="VWwNeF8ZV8T0u7mKTpdBZ6ptHfS1tK49BtDmRofy/DKCa8Mav9DOxOTd9G/7QiltaaKn7rDKToiRljmEJB5j5A==" />
          <div data-target="custom-scopes.customScopesModalDialogFlash"></div>

          <input type="hidden" id="custom_scope_id" name="custom_scope_id" data-target="custom-scopes.customScopesIdField">

          <div class="form-group">
            <label for="custom_scope_name">Name</label>
            <auto-check src="/search/custom_scopes/check_name" required>
              <input
                type="text"
                name="custom_scope_name"
                id="custom_scope_name"
                data-target="custom-scopes.customScopesNameField"
                class="form-control"
                autocomplete="off"
                placeholder="github-ruby"
                required
                maxlength="50">
              <input type="hidden" data-csrf="true" value="QhDsrAP1Arm2hisVa2S6Erw1ARHrkZT0fSRJtTtmUsnnmVcwnF3VCYLmq5OtBPhhXEhK+YQRnC500p0wagl/mg==" />
            </auto-check>
          </div>

          <div class="form-group">
            <label for="custom_scope_query">Query</label>
            <input
              type="text"
              name="custom_scope_query"
              id="custom_scope_query"
              data-target="custom-scopes.customScopesQueryField"
              class="form-control"
              autocomplete="off"
              placeholder="(repo:mona/a OR repo:mona/b) AND lang:python"
              required
              maxlength="500">
          </div>

          <p class="text-small color-fg-muted">
            To see all available qualifiers, see our <a class="Link--inTextBlock" href="https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax">documentation</a>.
          </p>
</form>        </div>

        <div data-target="custom-scopes.manageCustomScopesForm">
          <div data-target="custom-scopes.list"></div>
        </div>

</div>
      </scrollable-region>
      <div data-view-component="true" class="Overlay-footer Overlay-footer--alignEnd Overlay-footer--divided">          <button data-action="click:custom-scopes#customScopesCancel" type="button" data-view-component="true" class="btn">    Cancel
</button>
          <button form="custom-scopes-dialog-form" data-action="click:custom-scopes#customScopesSubmit" data-target="custom-scopes.customScopesSubmitButton" type="submit" data-view-component="true" class="btn-primary btn">    Create saved search
</button>
</div>
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            <div class="position-relative HeaderMenu-link-wrap d-lg-inline-block">
              <a
                href="/login?return_to=https%3A%2F%2Fgithub.com%2FHCPLab-SYSU%2FEmbodied_AI_Paper_List"
                class="HeaderMenu-link HeaderMenu-link--sign-in HeaderMenu-button flex-shrink-0 no-underline d-none d-lg-inline-flex border border-lg-0 rounded px-2 py-1"
                style="margin-left: 12px;"
                data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="b3731461444c6fe5a28723de1b21edc08737ccf020676bb63d9dfef3ebc3d605"
                data-analytics-event="{&quot;category&quot;:&quot;Marketing nav&quot;,&quot;action&quot;:&quot;click to go to homepage&quot;,&quot;label&quot;:&quot;ref_page:Marketing;ref_cta:Sign in;ref_loc:Header&quot;}"
              >
                Sign in
              </a>
            </div>

              <a href="/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=HCPLab-SYSU%2FEmbodied_AI_Paper_List"
                class="HeaderMenu-link HeaderMenu-link--sign-up HeaderMenu-button flex-shrink-0 d-flex d-lg-inline-flex no-underline border color-border-default rounded px-2 py-1"
                data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="b3731461444c6fe5a28723de1b21edc08737ccf020676bb63d9dfef3ebc3d605"
                data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/&lt;user-name&gt;/&lt;repo-name&gt;;ref_cta:Sign up;ref_loc:header logged out&quot;}"
              >
                Sign up
              </a>

                <div class="AppHeader-appearanceSettings">
    <react-partial-anchor>
      <button data-target="react-partial-anchor.anchor" id="icon-button-12070868-1a4f-458e-aacb-9a96c9677802" aria-labelledby="tooltip-29f060f3-f97a-4887-bafc-24146a16f2dd" type="button" disabled="disabled" data-view-component="true" class="Button Button--iconOnly Button--invisible Button--medium AppHeader-button HeaderMenu-link border cursor-wait">  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-sliders Button-visual">
    <path d="M15 2.75a.75.75 0 0 1-.75.75h-4a.75.75 0 0 1 0-1.5h4a.75.75 0 0 1 .75.75Zm-8.5.75v1.25a.75.75 0 0 0 1.5 0v-4a.75.75 0 0 0-1.5 0V2H1.75a.75.75 0 0 0 0 1.5H6.5Zm1.25 5.25a.75.75 0 0 0 0-1.5h-6a.75.75 0 0 0 0 1.5h6ZM15 8a.75.75 0 0 1-.75.75H11.5V10a.75.75 0 1 1-1.5 0V6a.75.75 0 0 1 1.5 0v1.25h2.75A.75.75 0 0 1 15 8Zm-9 5.25v-2a.75.75 0 0 0-1.5 0v1.25H1.75a.75.75 0 0 0 0 1.5H4.5v1.25a.75.75 0 0 0 1.5 0v-2Zm9 0a.75.75 0 0 1-.75.75h-6a.75.75 0 0 1 0-1.5h6a.75.75 0 0 1 .75.75Z"></path>
</svg>
</button><tool-tip id="tooltip-29f060f3-f97a-4887-bafc-24146a16f2dd" for="icon-button-12070868-1a4f-458e-aacb-9a96c9677802" popover="manual" data-direction="s" data-type="label" data-view-component="true" class="sr-only position-absolute">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.fbf61c683a537e8f92d5.module.css" />
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.753d458774a2f782559b.module.css" />

<react-partial
  partial-name="appearance-settings"
  data-ssr="false"
  data-attempted-ssr="false"
  data-react-profiling="false"
>
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>


      </template>
    </react-partial-anchor>
  </div>

          <button type="button" class="sr-only js-header-menu-focus-trap d-block d-lg-none">Resetting focus</button>
        </div>
      </div>
    </div>
  </div>
</header>

      <div hidden="hidden" data-view-component="true" class="js-stale-session-flash stale-session-flash flash flash-warn flash-full">
  
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-alert">
    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
        <span class="js-stale-session-flash-signed-in" hidden>You signed in with another tab or window. <a class="Link--inTextBlock" href="">Reload</a> to refresh your session.</span>
        <span class="js-stale-session-flash-signed-out" hidden>You signed out in another tab or window. <a class="Link--inTextBlock" href="">Reload</a> to refresh your session.</span>
        <span class="js-stale-session-flash-switched" hidden>You switched accounts on another tab or window. <a class="Link--inTextBlock" href="">Reload</a> to refresh your session.</span>

    <button id="icon-button-9e0d933a-8464-4135-ab34-2798a328d3b6" aria-labelledby="tooltip-832fc9cc-5191-4e62-b851-31e4e11053fe" type="button" data-view-component="true" class="Button Button--iconOnly Button--invisible Button--medium flash-close js-flash-close">  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x Button-visual">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
</button><tool-tip id="tooltip-832fc9cc-5191-4e62-b851-31e4e11053fe" for="icon-button-9e0d933a-8464-4135-ab34-2798a328d3b6" popover="manual" data-direction="s" data-type="label" data-view-component="true" class="sr-only position-absolute">Dismiss alert</tool-tip>


  
</div>
    </div>

  <div id="start-of-content" class="show-on-focus"></div>








    <div id="js-flash-container" class="flash-container" data-turbo-replace>




  <template class="js-flash-template">
    
<div class="flash flash-full   {{ className }}">
  <div >
    <button autofocus class="flash-close js-flash-close" type="button" aria-label="Dismiss this message">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
    </button>
    <div aria-atomic="true" role="alert" class="js-flash-alert">
      
      <div>{{ message }}</div>

    </div>
  </div>
</div>
  </template>
</div>


    






  <div
    class="application-main "
    data-commit-hovercards-enabled
    data-discussion-hovercards-enabled
    data-issue-and-pr-hovercards-enabled
    data-project-hovercards-enabled
  >
        <div itemscope itemtype="http://schema.org/SoftwareSourceCode" class="">
    <main id="js-repo-pjax-container" >
      
  





    






  

  <div id="repository-container-header"  class="pt-3 hide-full-screen" style="background-color: var(--page-header-bgColor, var(--color-page-header-bg));" data-turbo-replace>

      <div class="d-flex flex-nowrap flex-justify-end mb-3  px-3 px-lg-5" style="gap: 1rem;">

        <div class="flex-auto min-width-0 width-fit">
            
  <div class=" d-flex flex-wrap flex-items-center wb-break-word f3 text-normal">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo color-fg-muted mr-2">
    <path d="M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z"></path>
</svg>
    
    <span class="author flex-self-stretch" itemprop="author">
      <a class="url fn" rel="author" data-hovercard-type="user" data-hovercard-url="/users/HCPLab-SYSU/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="/HCPLab-SYSU">
        HCPLab-SYSU
</a>    </span>
    <span class="mx-1 flex-self-stretch color-fg-muted">/</span>
    <strong itemprop="name" class="mr-2 flex-self-stretch">
      <a data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" href="/HCPLab-SYSU/Embodied_AI_Paper_List">Embodied_AI_Paper_List</a>
    </strong>

    <span></span><span class="Label Label--secondary v-align-middle mr-1">Public</span>
  </div>


        </div>

        <div id="repository-details-container" class="flex-shrink-0" data-turbo-replace style="max-width: 70%;">
            <ul class="pagehead-actions flex-shrink-0 d-none d-md-inline" style="padding: 2px 0;">
    
      

  <li>
            <a href="/login?return_to=%2FHCPLab-SYSU%2FEmbodied_AI_Paper_List" rel="nofollow" id="repository-details-watch-button" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="2be1585bcc3577db36d83a830e35382fe288729a18a8f4468d12e1eb3cc9893d" aria-label="You must be signed in to change notification settings" data-view-component="true" class="btn-sm btn">    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-bell mr-2">
    <path d="M8 16a2 2 0 0 0 1.985-1.75c.017-.137-.097-.25-.235-.25h-3.5c-.138 0-.252.113-.235.25A2 2 0 0 0 8 16ZM3 5a5 5 0 0 1 10 0v2.947c0 .05.015.098.042.139l1.703 2.555A1.519 1.519 0 0 1 13.482 13H2.518a1.516 1.516 0 0 1-1.263-2.36l1.703-2.554A.255.255 0 0 0 3 7.947Zm5-3.5A3.5 3.5 0 0 0 4.5 5v2.947c0 .346-.102.683-.294.97l-1.703 2.556a.017.017 0 0 0-.003.01l.001.006c0 .002.002.004.004.006l.006.004.007.001h10.964l.007-.001.006-.004.004-.006.001-.007a.017.017 0 0 0-.003-.01l-1.703-2.554a1.745 1.745 0 0 1-.294-.97V5A3.5 3.5 0 0 0 8 1.5Z"></path>
</svg>Notifications
</a>    <tool-tip id="tooltip-bf68c89a-e7a8-4c02-b1e0-02dfaff80839" for="repository-details-watch-button" popover="manual" data-direction="s" data-type="description" data-view-component="true" class="sr-only position-absolute">You must be signed in to change notification settings</tool-tip>

  </li>

  <li>
          <a icon="repo-forked" id="fork-button" href="/login?return_to=%2FHCPLab-SYSU%2FEmbodied_AI_Paper_List" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;repo details fork button&quot;,&quot;repository_id&quot;:811638645,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="e0ab2a92090cf15bf16b8eb16a3b54f61c80d55b8fe36927eee49686c4dcd6c8" data-view-component="true" class="btn-sm btn">    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo-forked mr-2">
    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
</svg>Fork
    <span id="repo-network-counter" data-pjax-replace="true" data-turbo-replace="true" title="128" data-view-component="true" class="Counter">128</span>
</a>
  </li>

  <li>
        <div data-view-component="true" class="BtnGroup d-flex">
        <a href="/login?return_to=%2FHCPLab-SYSU%2FEmbodied_AI_Paper_List" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:811638645,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="f553f886d99283f92bfaf5ac9016fa6cb35a8ee9654c1c5d545177b2e4d076cf" aria-label="You must be signed in to star a repository" data-view-component="true" class="tooltipped tooltipped-sw btn-sm btn">    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-star v-align-text-bottom d-inline-block mr-2">
    <path d="M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z"></path>
</svg><span data-view-component="true" class="d-inline">
          Star
</span>          <span id="repo-stars-counter-star" aria-label="1811 users starred this repository" data-singular-suffix="user starred this repository" data-plural-suffix="users starred this repository" data-turbo-replace="true" title="1,811" data-view-component="true" class="Counter js-social-count">1.8k</span>
</a></div>
  </li>

</ul>

        </div>
      </div>

        <div id="responsive-meta-container" data-turbo-replace>
      <div class="d-block d-md-none mb-2 px-3 px-md-4 px-lg-5">
      <p class="f4 mb-3 ">
        [Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI
      </p>

    

    <div class="mb-3">
        <a class="Link--secondary no-underline mr-3" href="/HCPLab-SYSU/Embodied_AI_Paper_List/stargazers">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-star mr-1">
    <path d="M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z"></path>
</svg>
          <span class="text-bold">1.8k</span>
          stars
</a>        <a class="Link--secondary no-underline mr-3" href="/HCPLab-SYSU/Embodied_AI_Paper_List/forks">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo-forked mr-1">
    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
</svg>
          <span class="text-bold">128</span>
          forks
</a>        <a class="Link--secondary no-underline mr-3 d-inline-block" href="/HCPLab-SYSU/Embodied_AI_Paper_List/branches">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-git-branch mr-1">
    <path d="M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z"></path>
</svg>
          <span>Branches</span>
</a>        <a class="Link--secondary no-underline d-inline-block" href="/HCPLab-SYSU/Embodied_AI_Paper_List/tags">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-tag mr-1">
    <path d="M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z"></path>
</svg>
          <span>Tags</span>
</a>        <a class="Link--secondary no-underline d-inline-block" href="/HCPLab-SYSU/Embodied_AI_Paper_List/activity">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-pulse mr-1">
    <path d="M6 2c.306 0 .582.187.696.471L10 10.731l1.304-3.26A.751.751 0 0 1 12 7h3.25a.75.75 0 0 1 0 1.5h-2.742l-1.812 4.528a.751.751 0 0 1-1.392 0L6 4.77 4.696 8.03A.75.75 0 0 1 4 8.5H.75a.75.75 0 0 1 0-1.5h2.742l1.812-4.529A.751.751 0 0 1 6 2Z"></path>
</svg>
          <span>Activity</span>
</a>    </div>
      <div class="d-flex flex-wrap gap-2">
        <div class="flex-1">
            <div data-view-component="true" class="BtnGroup d-flex">
        <a href="/login?return_to=%2FHCPLab-SYSU%2FEmbodied_AI_Paper_List" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:811638645,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="f553f886d99283f92bfaf5ac9016fa6cb35a8ee9654c1c5d545177b2e4d076cf" aria-label="You must be signed in to star a repository" data-view-component="true" class="tooltipped tooltipped-sw btn-sm btn btn-block">    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-star v-align-text-bottom d-inline-block mr-2">
    <path d="M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z"></path>
</svg><span data-view-component="true" class="d-inline">
          Star
</span>
</a></div>
        </div>
        <div class="flex-1">
                <a href="/login?return_to=%2FHCPLab-SYSU%2FEmbodied_AI_Paper_List" rel="nofollow" id="files-overview-watch-button" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="2be1585bcc3577db36d83a830e35382fe288729a18a8f4468d12e1eb3cc9893d" aria-label="You must be signed in to change notification settings" data-view-component="true" class="btn-sm btn btn-block">    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-bell mr-2">
    <path d="M8 16a2 2 0 0 0 1.985-1.75c.017-.137-.097-.25-.235-.25h-3.5c-.138 0-.252.113-.235.25A2 2 0 0 0 8 16ZM3 5a5 5 0 0 1 10 0v2.947c0 .05.015.098.042.139l1.703 2.555A1.519 1.519 0 0 1 13.482 13H2.518a1.516 1.516 0 0 1-1.263-2.36l1.703-2.554A.255.255 0 0 0 3 7.947Zm5-3.5A3.5 3.5 0 0 0 4.5 5v2.947c0 .346-.102.683-.294.97l-1.703 2.556a.017.017 0 0 0-.003.01l.001.006c0 .002.002.004.004.006l.006.004.007.001h10.964l.007-.001.006-.004.004-.006.001-.007a.017.017 0 0 0-.003-.01l-1.703-2.554a1.745 1.745 0 0 1-.294-.97V5A3.5 3.5 0 0 0 8 1.5Z"></path>
</svg>Notifications
</a>    <tool-tip id="tooltip-1223e254-4e79-4c40-9e6f-f51974e4378c" for="files-overview-watch-button" popover="manual" data-direction="s" data-type="description" data-view-component="true" class="sr-only position-absolute">You must be signed in to change notification settings</tool-tip>

        </div>
        <span>
          

        </span>
      </div>
  </div>

</div>


          <nav data-pjax="#js-repo-pjax-container" aria-label="Repository" data-view-component="true" class="js-repo-nav js-sidenav-container-pjax js-responsive-underlinenav overflow-hidden UnderlineNav px-3 px-md-4 px-lg-5">

  <ul data-view-component="true" class="UnderlineNav-body list-style-none">
      <li data-view-component="true" class="d-inline-flex">
  <a id="code-tab" href="/HCPLab-SYSU/Embodied_AI_Paper_List" data-tab-item="i0code-tab" data-selected-links="repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages repo_deployments repo_attestations /HCPLab-SYSU/Embodied_AI_Paper_List" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g c" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Code&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" aria-current="page" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item selected">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-code UnderlineNav-octicon d-none d-sm-inline">
    <path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path>
</svg>
        <span data-content="Code">Code</span>
          <span id="code-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true" class="Counter"></span>


    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="issues-tab" href="/HCPLab-SYSU/Embodied_AI_Paper_List/issues" data-tab-item="i1issues-tab" data-selected-links="repo_issues repo_labels repo_milestones /HCPLab-SYSU/Embodied_AI_Paper_List/issues" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g i" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Issues&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-issue-opened UnderlineNav-octicon d-none d-sm-inline">
    <path d="M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path><path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z"></path>
</svg>
        <span data-content="Issues">Issues</span>
          <span id="issues-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="0" hidden="hidden" data-view-component="true" class="Counter">0</span>


    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="pull-requests-tab" href="/HCPLab-SYSU/Embodied_AI_Paper_List/pulls" data-tab-item="i2pull-requests-tab" data-selected-links="repo_pulls checks /HCPLab-SYSU/Embodied_AI_Paper_List/pulls" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g p" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Pull requests&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-git-pull-request UnderlineNav-octicon d-none d-sm-inline">
    <path d="M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z"></path>
</svg>
        <span data-content="Pull requests">Pull requests</span>
          <span id="pull-requests-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="0" hidden="hidden" data-view-component="true" class="Counter">0</span>


    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="actions-tab" href="/HCPLab-SYSU/Embodied_AI_Paper_List/actions" data-tab-item="i3actions-tab" data-selected-links="repo_actions /HCPLab-SYSU/Embodied_AI_Paper_List/actions" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g a" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Actions&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-play UnderlineNav-octicon d-none d-sm-inline">
    <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z"></path>
</svg>
        <span data-content="Actions">Actions</span>
          <span id="actions-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true" class="Counter"></span>


    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="projects-tab" href="/HCPLab-SYSU/Embodied_AI_Paper_List/projects" data-tab-item="i4projects-tab" data-selected-links="repo_projects new_repo_project repo_project /HCPLab-SYSU/Embodied_AI_Paper_List/projects" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g b" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Projects&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-table UnderlineNav-octicon d-none d-sm-inline">
    <path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25ZM6.5 6.5v8h7.75a.25.25 0 0 0 .25-.25V6.5Zm8-1.5V1.75a.25.25 0 0 0-.25-.25H6.5V5Zm-13 1.5v7.75c0 .138.112.25.25.25H5v-8ZM5 5V1.5H1.75a.25.25 0 0 0-.25.25V5Z"></path>
</svg>
        <span data-content="Projects">Projects</span>
          <span id="projects-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="0" hidden="hidden" data-view-component="true" class="Counter">0</span>


    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="security-tab" href="/HCPLab-SYSU/Embodied_AI_Paper_List/security" data-tab-item="i5security-tab" data-selected-links="security overview alerts policy token_scanning code_scanning /HCPLab-SYSU/Embodied_AI_Paper_List/security" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g s" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Security&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-shield UnderlineNav-octicon d-none d-sm-inline">
    <path d="M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
        <span data-content="Security">Security</span>
          <include-fragment src="/HCPLab-SYSU/Embodied_AI_Paper_List/security/overall-count" accept="text/fragment+html" data-nonce="v2:b0864956-446a-ccd6-cec6-a695a0cbc5f5" data-view-component="true">
  
  <div data-show-on-forbidden-error hidden>
    <div class="Box">
  <div class="blankslate-container">
    <div data-view-component="true" class="blankslate blankslate-spacious color-bg-default rounded-2">
      

      <h3 data-view-component="true" class="blankslate-heading">        Uh oh!
</h3>
      <p data-view-component="true">        <p class="color-fg-muted my-2 mb-2 ws-normal">There was an error while loading. <a class="Link--inTextBlock" data-turbo="false" href="" aria-label="Please reload this page">Please reload this page</a>.</p>
</p>

</div>  </div>
</div>  </div>
</include-fragment>

    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="insights-tab" href="/HCPLab-SYSU/Embodied_AI_Paper_List/pulse" data-tab-item="i6insights-tab" data-selected-links="repo_graphs repo_contributors dependency_graph dependabot_updates pulse people community /HCPLab-SYSU/Embodied_AI_Paper_List/pulse" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Insights&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-graph UnderlineNav-octicon d-none d-sm-inline">
    <path d="M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z"></path>
</svg>
        <span data-content="Insights">Insights</span>
          <span id="insights-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true" class="Counter"></span>


    
</a></li>
</ul>
    <div style="visibility:hidden;" data-view-component="true" class="UnderlineNav-actions js-responsive-underlinenav-overflow position-absolute pr-3 pr-md-4 pr-lg-5 right-0">      <action-menu data-select-variant="none" data-view-component="true">
  <focus-group direction="vertical" mnemonics retain>
    <button id="action-menu-b54f51bd-d6f6-4c9a-884b-2b65dd120bb7-button" popovertarget="action-menu-b54f51bd-d6f6-4c9a-884b-2b65dd120bb7-overlay" aria-controls="action-menu-b54f51bd-d6f6-4c9a-884b-2b65dd120bb7-list" aria-haspopup="true" aria-labelledby="tooltip-75814aab-57b3-4e14-8035-bdc6670135dc" type="button" data-view-component="true" class="Button Button--iconOnly Button--secondary Button--medium UnderlineNav-item">  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-kebab-horizontal Button-visual">
    <path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path>
</svg>
</button><tool-tip id="tooltip-75814aab-57b3-4e14-8035-bdc6670135dc" for="action-menu-b54f51bd-d6f6-4c9a-884b-2b65dd120bb7-button" popover="manual" data-direction="s" data-type="label" data-view-component="true" class="sr-only position-absolute">Additional navigation options</tool-tip>


<anchored-position data-target="action-menu.overlay" id="action-menu-b54f51bd-d6f6-4c9a-884b-2b65dd120bb7-overlay" anchor="action-menu-b54f51bd-d6f6-4c9a-884b-2b65dd120bb7-button" align="start" side="outside-bottom" anchor-offset="normal" popover="auto" data-view-component="true">
  <div data-view-component="true" class="Overlay Overlay--size-auto">
    
      <div data-view-component="true" class="Overlay-body Overlay-body--paddingNone">          <action-list>
  <div data-view-component="true">
    <ul aria-labelledby="action-menu-b54f51bd-d6f6-4c9a-884b-2b65dd120bb7-button" id="action-menu-b54f51bd-d6f6-4c9a-884b-2b65dd120bb7-list" role="menu" data-view-component="true" class="ActionListWrap--inset ActionListWrap">
        <li hidden="hidden" data-menu-item="i0code-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
    
    
    <a tabindex="-1" id="item-29d3186f-d05f-4866-a42f-1bf1aabcd02a" href="/HCPLab-SYSU/Embodied_AI_Paper_List" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
        <span class="ActionListItem-visual ActionListItem-visual--leading">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-code">
    <path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path>
</svg>
        </span>
      
        <span data-view-component="true" class="ActionListItem-label">
          Code
</span>      
</a>
  
</li>
        <li hidden="hidden" data-menu-item="i1issues-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
    
    
    <a tabindex="-1" id="item-c8acbfd6-4f2a-4d67-92d4-3443c379af7e" href="/HCPLab-SYSU/Embodied_AI_Paper_List/issues" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
        <span class="ActionListItem-visual ActionListItem-visual--leading">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-issue-opened">
    <path d="M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path><path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z"></path>
</svg>
        </span>
      
        <span data-view-component="true" class="ActionListItem-label">
          Issues
</span>      
</a>
  
</li>
        <li hidden="hidden" data-menu-item="i2pull-requests-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
    
    
    <a tabindex="-1" id="item-37c96192-b258-4567-b690-8a5a39680adb" href="/HCPLab-SYSU/Embodied_AI_Paper_List/pulls" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
        <span class="ActionListItem-visual ActionListItem-visual--leading">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-git-pull-request">
    <path d="M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z"></path>
</svg>
        </span>
      
        <span data-view-component="true" class="ActionListItem-label">
          Pull requests
</span>      
</a>
  
</li>
        <li hidden="hidden" data-menu-item="i3actions-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
    
    
    <a tabindex="-1" id="item-7d43f366-3785-42b2-8dec-c82c52b2d05a" href="/HCPLab-SYSU/Embodied_AI_Paper_List/actions" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
        <span class="ActionListItem-visual ActionListItem-visual--leading">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-play">
    <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z"></path>
</svg>
        </span>
      
        <span data-view-component="true" class="ActionListItem-label">
          Actions
</span>      
</a>
  
</li>
        <li hidden="hidden" data-menu-item="i4projects-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
    
    
    <a tabindex="-1" id="item-ee2a18fd-d07f-451e-8870-0781dc0ea51c" href="/HCPLab-SYSU/Embodied_AI_Paper_List/projects" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
        <span class="ActionListItem-visual ActionListItem-visual--leading">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-table">
    <path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25ZM6.5 6.5v8h7.75a.25.25 0 0 0 .25-.25V6.5Zm8-1.5V1.75a.25.25 0 0 0-.25-.25H6.5V5Zm-13 1.5v7.75c0 .138.112.25.25.25H5v-8ZM5 5V1.5H1.75a.25.25 0 0 0-.25.25V5Z"></path>
</svg>
        </span>
      
        <span data-view-component="true" class="ActionListItem-label">
          Projects
</span>      
</a>
  
</li>
        <li hidden="hidden" data-menu-item="i5security-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
    
    
    <a tabindex="-1" id="item-e0e592dc-4b4c-4b64-8345-c9e609a83af8" href="/HCPLab-SYSU/Embodied_AI_Paper_List/security" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
        <span class="ActionListItem-visual ActionListItem-visual--leading">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-shield">
    <path d="M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
        </span>
      
        <span data-view-component="true" class="ActionListItem-label">
          Security
</span>      
</a>
  
</li>
        <li hidden="hidden" data-menu-item="i6insights-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
    
    
    <a tabindex="-1" id="item-b517b381-17b0-4794-ba24-03a50b6d245a" href="/HCPLab-SYSU/Embodied_AI_Paper_List/pulse" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
        <span class="ActionListItem-visual ActionListItem-visual--leading">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-graph">
    <path d="M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z"></path>
</svg>
        </span>
      
        <span data-view-component="true" class="ActionListItem-label">
          Insights
</span>      
</a>
  
</li>
</ul>    
</div></action-list>


</div>
      
</div></anchored-position>  </focus-group>
</action-menu></div>
</nav>

  </div>
  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance" class="">
    <div id="repo-content-pjax-container" class="repository-content " >
    



    
      
  <h1 class='sr-only'>HCPLab-SYSU/Embodied_AI_Paper_List</h1>
  <div class="clearfix container-xl px-md-4 px-lg-5 px-3">
    <div>

  <div style="max-width: 100%" data-view-component="true" class="Layout Layout--flowRow-until-md react-repos-overview-margin Layout--sidebarPosition-end Layout--sidebarPosition-flowRow-end">
  <div data-view-component="true" class="Layout-main">      <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.fbf61c683a537e8f92d5.module.css" />
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/67240.8050999de9202f04fc44.module.css" />

<react-partial
  partial-name="repos-overview"
  data-ssr="true"
  data-attempted-ssr="true"
  data-react-profiling="false"
>
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{"initialPayload":{"allShortcutsEnabled":false,"path":"/","repo":{"id":811638645,"defaultBranch":"main","name":"Embodied_AI_Paper_List","ownerLogin":"HCPLab-SYSU","currentUserCanPush":false,"isFork":false,"isEmpty":false,"createdAt":"2024-06-07T02:10:47.000Z","ownerAvatar":"https://avatars.githubusercontent.com/u/38847349?v=4","public":true,"private":false,"isOrgOwned":false},"currentUser":null,"refInfo":{"name":"main","listCacheKey":"v0:1717726247.0","canEdit":false,"refType":"branch","currentOid":"e5fffd9e948de9dd940768f931954613ebd9b03c"},"tree":{"items":[{"name":"EmbodiedAI.jpg","path":"EmbodiedAI.jpg","contentType":"file"},{"name":"EmbodiedAI_Review.pdf","path":"EmbodiedAI_Review.pdf","contentType":"file"},{"name":"README.md","path":"README.md","contentType":"file"},{"name":"Survey.png","path":"Survey.png","contentType":"file"},{"name":"teaser.png","path":"teaser.png","contentType":"file"}],"templateDirectorySuggestionUrl":null,"readme":null,"totalCount":5,"showBranchInfobar":false},"fileTree":null,"fileTreeProcessingTime":null,"foldersToFetch":[],"treeExpanded":false,"symbolsExpanded":false,"copilotSWEAgentEnabled":false,"isOverview":true,"overview":{"banners":{"shouldRecommendReadme":false,"isPersonalRepo":false,"showUseActionBanner":false,"actionSlug":null,"actionId":null,"showProtectBranchBanner":false,"publishBannersInfo":{"dismissActionNoticePath":"/settings/dismiss-notice/publish_action_from_repo","releasePath":"/HCPLab-SYSU/Embodied_AI_Paper_List/releases/new?marketplace=true","showPublishActionBanner":false},"interactionLimitBanner":null,"showInvitationBanner":false,"inviterName":null,"actionsMigrationBannerInfo":{"releaseTags":[],"showImmutableActionsMigrationBanner":false,"initialMigrationStatus":null},"showDeployBanner":false,"detectedStack":{"framework":null,"packageManager":null}},"codeButton":{"contactPath":"/contact","isEnterprise":false,"local":{"protocolInfo":{"httpAvailable":true,"sshAvailable":null,"httpUrl":"https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.git","showCloneWarning":null,"sshUrl":null,"sshCertificatesRequired":null,"sshCertificatesAvailable":null,"ghCliUrl":"gh repo clone HCPLab-SYSU/Embodied_AI_Paper_List","defaultProtocol":"http","newSshKeyUrl":"/settings/ssh/new","setProtocolPath":"/users/set_protocol"},"platformInfo":{"cloneUrl":"https://desktop.github.com","showVisualStudioCloneButton":false,"visualStudioCloneUrl":"https://windows.github.com","showXcodeCloneButton":false,"xcodeCloneUrl":"xcode://clone?repo=https%3A%2F%2Fgithub.com%2FHCPLab-SYSU%2FEmbodied_AI_Paper_List","zipballUrl":"/HCPLab-SYSU/Embodied_AI_Paper_List/archive/refs/heads/main.zip"}},"newCodespacePath":"/codespaces/new?hide_repo_select=true\u0026repo=811638645"},"popovers":{"rename":null,"renamedParentRepo":null},"commitCount":"211","overviewFiles":[{"displayName":"README.md","repoName":"Embodied_AI_Paper_List","refName":"main","path":"README.md","preferredFileType":"readme","tabName":"README","richText":"\u003carticle class=\"markdown-body entry-content container-lg\" itemprop=\"text\"\u003e\u003cbr\u003e\n\u003cp align=\"center\" dir=\"auto\"\u003e\n\u003c/p\u003e\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch1 align=\"center\" tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e\u003cstrong\u003ePaper List  and Resource Repository for Embodied AI\u003c/strong\u003e\u003c/h1\u003e\u003ca id=\"user-content-paper-list--and-resource-repository-for-embodied-ai\" class=\"anchor\" aria-label=\"Permalink: Paper List  and Resource Repository for Embodied AI\" href=\"#paper-list--and-resource-repository-for-embodied-ai\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n  \u003cp align=\"center\" dir=\"auto\"\u003e\n    \u003ca href=\"https://www.sysu-hcp.net/\" rel=\"nofollow\"\u003eHCPLab\u003c/a\u003e\n    \u003cbr\u003e\n    SYSU HCP Lab and Pengcheng Laboratory\n    \u003cbr\u003e\n  \u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003c/p\u003e\n\u003cp align=\"center\" dir=\"auto\"\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/EmbodiedAI.jpg\"\u003e\u003cimg src=\"/HCPLab-SYSU/Embodied_AI_Paper_List/raw/main/EmbodiedAI.jpg\" width=\"250\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca href=\"https://arxiv.org/abs/2407.06886\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c0553ae02c65b5a63d6c1151b3b8daa66e616c1507fa48020913fd1c7c4b1b85/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323430372e30363838362d6f72616e6765\" alt=\"arXiv\" data-canonical-src=\"https://img.shields.io/badge/arXiv-2407.06886-orange\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/EmbodiedAI_Review.pdf\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7a912aa4be033314e25afdcb5d53a629ec68b831a6025150f0a285348f902cdb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617065722d2546302539462539332539362d79656c6c6f77\" alt=\"\" data-canonical-src=\"https://img.shields.io/badge/Paper-%F0%9F%93%96-yellow\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/df7e91dc57b9025906b2ae1bd1f72640281a0dc4c38e500268d3427776e5553c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50726f6a6563742d2546302539462539412538302d70696e6b\" alt=\"\" data-canonical-src=\"https://img.shields.io/badge/Project-%F0%9F%9A%80-pink\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eWe appreciate any useful suggestions for improvement of this paper list or survey from peers. Please raise issues or send an email to \u003cstrong\u003e\u003ca href=\"mailto:liuy856@mail.sysu.edu.cn\"\u003eliuy856@mail.sysu.edu.cn\u003c/a\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003ca href=\"mailto:chen867820261@gmail.com\"\u003echen867820261@gmail.com\u003c/a\u003e\u003c/strong\u003e. Thanks for your cooperation! We also welcome your pull requests for this project!\u003c/h4\u003e\u003ca id=\"user-content-we-appreciate-any-useful-suggestions-for-improvement-of-this-paper-list-or-survey-from-peers-please-raise-issues-or-send-an-email-to-liuy856mailsysueducn-and-chen867820261gmailcom-thanks-for-your-cooperation-we-also-welcome-your-pull-requests-for-this-project\" class=\"anchor\" aria-label=\"Permalink: We appreciate any useful suggestions for improvement of this paper list or survey from peers. Please raise issues or send an email to liuy856@mail.sysu.edu.cn and chen867820261@gmail.com. Thanks for your cooperation! We also welcome your pull requests for this project!\" href=\"#we-appreciate-any-useful-suggestions-for-improvement-of-this-paper-list-or-survey-from-peers-please-raise-issues-or-send-an-email-to-liuy856mailsysueducn-and-chen867820261gmailcom-thanks-for-your-cooperation-we-also-welcome-your-pull-requests-for-this-project\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/teaser.png\"\u003e\u003cimg src=\"/HCPLab-SYSU/Embodied_AI_Paper_List/raw/main/teaser.png\" alt=\"Teaser\" title=\"demo\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp dir=\"auto\"\u003e\u003ca href=\"https://arxiv.org/pdf/2407.06886\" rel=\"nofollow\"\u003e\u003cstrong\u003eAligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI, IEEE/ASME Transactions on Mechatronics 2025\u003c/strong\u003e\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://yangliu9208.github.io\" rel=\"nofollow\"\u003eYang Liu\u003c/a\u003e, Weixing Chen, Yongjie Bai, \u003ca href=\"https://lemondan.github.io\" rel=\"nofollow\"\u003eXiaodan Liang\u003c/a\u003e, \u003ca href=\"http://guanbinli.com/\" rel=\"nofollow\"\u003eGuanbin Li\u003c/a\u003e, \u003ca href=\"https://idm.pku.edu.cn/info/1017/1041.htm\" rel=\"nofollow\"\u003eWen Gao\u003c/a\u003e, \u003ca href=\"http://www.linliang.net/\" rel=\"nofollow\"\u003eLiang Lin\u003c/a\u003e\u003c/p\u003e\n\u003cp align=\"center\" dir=\"auto\"\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/Survey.png\"\u003e\u003cimg src=\"/HCPLab-SYSU/Embodied_AI_Paper_List/raw/main/Survey.png\" width=\"800\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e  \n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e About\u003c/h2\u003e\u003ca id=\"user-content--about\" class=\"anchor\" aria-label=\"Permalink:  About\" href=\"#-about\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eEmbodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications (e.g., intelligent mechatronics systems, smart manufacturing) that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for embodied agents. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in digital and physical environments.  Finally, we summarize the challenges and limitations of embodied AI and discuss potential future directions. We hope this survey will serve as a foundational reference for the research community.\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e Update Log\u003c/h2\u003e\u003ca id=\"user-content-collision-update-log\" class=\"anchor\" aria-label=\"Permalink: :collision: Update Log\" href=\"#collision-update-log\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e[2025.05.27] Our Embodied AI Survey paper is accepted by IEEE/ASME Transactions on Mechatronics!\u003c/li\u003e\n\u003cli\u003e[2024.09.08] We are constantly updating the Dataset section!\u003c/li\u003e\n\u003cli\u003e[2024.08.31] We added the Datasets section and classified the useful projects!\u003c/li\u003e\n\u003cli\u003e[2024.08.19] To make readers focus on newest works, we have arranged papers in chronological order!\u003c/li\u003e\n\u003cli\u003e[2024.08.02] We regularly update the project weekly!\u003c/li\u003e\n\u003cli\u003e[2024.07.29] We have updated the project!\u003c/li\u003e\n\u003cli\u003e[2024.07.22] We have updated the paper list and other useful embodied projects!\u003c/li\u003e\n\u003cli\u003e[2024.07.10] We release the first version of the survey on Embodied AI \u003ca href=\"https://arxiv.org/pdf/2407.06886\" rel=\"nofollow\"\u003ePDF\u003c/a\u003e!\u003c/li\u003e\n\u003cli\u003e[2024.07.10] We release the first version of the paper list for Embodied AI. This page is continually updating!\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e\u003ca id=\"user-content-table-of-contents\"\u003e Table of Contents \u003c/a\u003e\u003c/h2\u003e\u003ca id=\"user-content--table-of-contents-\" class=\"anchor\" aria-label=\"Permalink:  Table of Contents \" href=\"#-table-of-contents-\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"#books-surveys\"\u003eBooks \u0026amp; Surveys\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#simulators\"\u003eEmbodied Simulators\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#perception\"\u003eEmbodied Perception\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#interaction\"\u003eEmbodied Interaction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#agent\"\u003eEmbodied Agent\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#sim-to-real\"\u003eSim-to-Real Adaptation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#datasets\"\u003eDatasets\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e\u003ca id=\"user-content-books-surveys\"\u003e Books \u0026amp; Surveys \u003c/a\u003e\u003ca href=\"#table-of-contents\"\u003e\u003c/a\u003e \u003c/h2\u003e\u003ca id=\"user-content--books--surveys--\" class=\"anchor\" aria-label=\"Permalink:  Books \u0026amp; Surveys \" href=\"#-books--surveys--\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eA Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI\u003c/strong\u003e, arXiv:2505.01458, 2025\u003cbr\u003e\nLik Hang Kenny Wong, Xueyang Kang, Kaixin Bai, Jianwei Zhang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2505.01458\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMultimodal Large Models: The New Paradigm of Artificial General Intelligence\u003c/strong\u003e, Publishing House of Electronics Industry (PHE), 2024\u003cbr\u003e\nYang Liu, Liang Lin\u003cbr\u003e\n[\u003ca href=\"https://hcplab-sysu.github.io/Book-of-MLM/\" rel=\"nofollow\"\u003ePage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eAligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI\u003c/strong\u003e, arXiv:2407.06886, 2024\u003cbr\u003e\nYang Liu, Weixing Chen, Yongjie Bai, Guanbin Li, Wen Gao, Liang Lin.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2407.06886\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eAll Robots in One: A New Standard and Unified Dataset for Versatile, General-Purpose Embodied Agents\u003c/strong\u003e, arXiv:2408.10899, 2024\u003cbr\u003e\nZhiqiang Wang, Hao Zheng, Yunshuang Nie, Wenjun Xu, Qingwei Wang, Hua Ye, Zhe Li, Kaidong Zhang, Xuewen Cheng, Wanxi Dong, Chang Cai, Liang Lin, Feng Zheng, Xiaodan Liang\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2408.10899\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e][\u003ca href=\"https://imaei.github.io/project_pages/ario/\" rel=\"nofollow\"\u003eProject\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEmbodied intelligence toward future smart manufacturing in the era of AI foundation model\u003c/strong\u003e, IEEE/ASME Transactions on Mechatronics, 2024\u003cbr\u003e\nLei Ren, Jiabao Dong, Shuai Liu, Lin Zhang, and Lihui Wang.\u003cbr\u003e\n[\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/10697107\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eA Survey of Embodied Learning for Object-Centric Robotic Manipulation\u003c/strong\u003e, arXiv:2408.11537, 2024\u003cbr\u003e\nYing Zheng, Lei Yao, Yuejiao Su, Yi Zhang, Yi Wang, Sicheng Zhao, Yiyi Zhang, Lap-Pui Chau\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2408.11537\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTeleoperation of Humanoid Robots: A Survey\u003c/strong\u003e, IEEE Transactions on Robotics, 2024\u003cbr\u003e\nKourosh Darvish, Luigi Penco, Joao Ramos, Rafael Cisneros, Jerry Pratt, Eiichi Yoshida, Serena Ivaldi, Daniele Pucci.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2301.04317\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eA Survey on Vision-Language-Action Models for Embodied AI\u003c/strong\u003e, arXiv:2405.14093, 2024\u003cbr\u003e\nYueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2405.14093\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTowards Generalist Robot Learning from Internet Video: A Survey\u003c/strong\u003e, arXiv:2404.19664, 2024\u003cbr\u003e\nMcCarthy, Robert, Daniel CH Tan, Dominik Schmidt, Fernando Acero, Nathan Herr, Yilun Du, Thomas G. Thuruthel, and Zhibin Li.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2404.19664\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eA Survey on Robotics with Foundation Models: toward Embodied AI\u003c/strong\u003e, arXiv:2402.02385, 2024\u003cbr\u003e\nZhiyuan Xu, Kun Wu, Junjie Wen, Jinming Li, Ning Liu, Zhengping Che, and Jian Tang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2402.02385\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eToward general-purpose robots via foundation models: A survey and meta-analysis\u003c/strong\u003e, Machines, 2023\u003cbr\u003e\nYafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, Jay Patrikar, Nikhil Keetha, Seungchan Kim, Yaqi Xie, Tianyi Zhang, Shibo Zhao, Yu Quan Chong, Chen Wang, Katia Sycara, Matthew Johnson-Roberson, Dhruv Batra, Xiaolong Wang, Sebastian Scherer, Zsolt Kira, Fei Xia, Yonatan Bisk.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2312.08782\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDeformable Object Manipulation in Caregiving Scenarios: A Review\u003c/strong\u003e, Machines, 2023\u003cbr\u003e\nLiman Wang, Jihong Zhu.\u003cbr\u003e\n[[Paper]\u003ca href=\"https://www.mdpi.com/2075-1702/11/11/1013\" rel=\"nofollow\"\u003ehttps://www.mdpi.com/2075-1702/11/11/1013\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eA survey of embodied ai: From simulators to research tasks\u003c/strong\u003e, IEEE Transactions on Emerging Topics in Computational Intelligence, 2022\u003cbr\u003e\nJiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, Cheston Tan\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2103.04918\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eThe development of embodied cognition: Six lessons from babies\u003c/strong\u003e, Artificial life, 2005\u003cbr\u003e\nLinda Smith, Michael Gasser\u003cbr\u003e\n[\u003ca href=\"https://cogdev.sitehost.iu.edu/labwork/6_lessons.pdf\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEmbodied artificial intelligence: Trends and challenges\u003c/strong\u003e, Lecture notes in computer science, 2004\u003cbr\u003e\nRolf Pfeifer, Fumiya Iida\u003cbr\u003e\n[\u003ca href=\"https://people.csail.mit.edu/iida/papers/PfeiferIidaEAIDags.pdf\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e\u003ca id=\"user-content-simulators\"\u003e Embodied Simulators \u003c/a\u003e\u003ca href=\"#table-of-contents\"\u003e\u003c/a\u003e \u003c/h2\u003e\u003ca id=\"user-content--embodied-simulators--\" class=\"anchor\" aria-label=\"Permalink:  Embodied Simulators \" href=\"#-embodied-simulators--\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eGeneral Simulator\u003c/h3\u003e\u003ca id=\"user-content-general-simulator\" class=\"anchor\" aria-label=\"Permalink: General Simulator\" href=\"#general-simulator\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDesign and use paradigms for gazebo, an open-source multi-robot simulator\u003c/strong\u003e, IROS, 2004\u003cbr\u003e\nKoenig, Nathan, Andrew, Howard.\u003cbr\u003e\n[\u003ca href=\"https://citeseerx.ist.psu.edu/document?repid=rep1\u0026amp;type=pdf\u0026amp;doi=79f91c1c95271a075b91e9fdca43d6c31e4cbe17\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eNvidia isaac sim: Robotics simulation and synthetic data\u003c/strong\u003e, NVIDIA, 2023\u003cbr\u003e\n[\u003ca href=\"https://developer.nvidia.com/isaac/sim\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eAerial Gym -- Isaac Gym Simulator for Aerial Robots\u003c/strong\u003e, ArXiv, 2023\u003cbr\u003e\nMihir Kulkarni and Theodor J. L. Forgaard and Kostas Alexis.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/abs/2305.16510\" rel=\"nofollow\"\u003epaper\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eWebots: open-source robot simulator\u003c/strong\u003e, 2018\u003cbr\u003e\nCyberbotics\u003cbr\u003e\n[\u003ca href=\"https://cyberbotics.com/doc/reference/index\" rel=\"nofollow\"\u003epage\u003c/a\u003e, \u003ca href=\"https://github.com/cyberbotics/webots\"\u003ecode\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eUnity: A general platform for intelligent agents\u003c/strong\u003e, ArXiv, 2020\u003cbr\u003e\nJuliani, Arthur, Vincent-Pierre, Berges, Ervin, Teng, Andrew, Cohen, Jonathan, Harper, Chris, Elion, Chris, Goy, Yuan, Gao, Hunter, Henry, Marwan, Mattar, Danny, Lange.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1809.02627\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eAirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles\u003c/strong\u003e, Field and Service Robotics, 2017\u003cbr\u003e\nShital Shah, , Debadeepta Dey, Chris Lovett, Ashish Kapoor.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1705.05065.pdf%20http://arxiv.org/abs/1705.05065\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePybullet, a python module for physics simulation for games, robotics and machine learning\u003c/strong\u003e, 2016\u003cbr\u003e\nCoumans, Erwin, Yunfei, Bai.\u003cbr\u003e\n[\u003ca href=\"https://github.com/bulletphysics/bullet3\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eV-REP: A versatile and scalable robot simulation framework\u003c/strong\u003e, IROS, 2013\u003cbr\u003e\nRohmer, Eric, Surya PN, Singh, Marc, Freese.\u003cbr\u003e\n[\u003ca href=\"https://coppeliarobotics.com/coppeliaSim_v-rep_iros2013.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMuJoCo: A physics engine for model-based control\u003c/strong\u003e, IROS, 2012\u003cbr\u003e\nTodorov, Emanuel, Tom, Erez, Yuval, Tassa.\u003cbr\u003e\n[\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/6386109/\" rel=\"nofollow\"\u003epage\u003c/a\u003e, \u003ca href=\"https://github.com/google-deepmind/mujoco\"\u003ecode\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eModular open robots simulation engine: Morse\u003c/strong\u003e, ICRA, 2011\u003cbr\u003e\nEcheverria, Gilberto and Lassabe, Nicolas and Degroote, Arnaud and Lemaignan, S{'e}verin\u003cbr\u003e\n[\u003ca href=\"https://www.openrobots.org/morse/material/media/pdf/paper-icra.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eReal-Scene Based Simulators\u003c/h3\u003e\u003ca id=\"user-content-real-scene-based-simulators\" class=\"anchor\" aria-label=\"Permalink: Real-Scene Based Simulators\" href=\"#real-scene-based-simulators\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eInfiniteWorld: A Unified Scalable Simulation Framework for General Visual-Language Robot Interaction\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nPengzhen Ren, Min Li, Zhen Luo, Xinshuai Song, Ziwei Chen, Weijia Liufu, Yixuan Yang, Hao Zheng, Rongtao Xu, Zitong Huang, Tongsheng Ding, Luyang Xie, Kaidong Zhang, Changfei Fu, Yang Liu, Liang Lin, Feng Zheng, Xiaodan Liang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2412.05789\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eManiSkill3: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nStone Tao, Fanbo Xiang, Arth Shukla, Yuzhe Qin, Xander Hinrichsen, Xiaodi Yuan, Chen Bao, Xinsong Lin, Yulin Liu, Tse-kai Chan, Yuan Gao, Xuanlin Li, Tongzhou Mu, Nan Xiao, Arnav Gurha, Zhiao Huang, Roberto Calandra, Rui Chen, Shan Luo, Hao Su.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2410.00425\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nYang, Yandan, Baoxiong, Jia, Peiyuan, Zhi, Siyuan, Huang.\u003cbr\u003e\n[\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_PhyScene_Physically_Interactable_3D_Scene_Synthesis_for_Embodied_AI_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eHolodeck: Language Guided Generation of 3D Embodied AI Environments\u003c/strong\u003e, CVPR, 2024\u003cbr\u003e\nYue Yang, , Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Mark Yatskar, Aniruddha Kembhavi, Christopher Clark.\u003cbr\u003e\n[\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Holodeck_Language_Guided_Generation_of_3D_Embodied_AI_Environments_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation\u003c/strong\u003e, arXiv, 2023\u003cbr\u003e\nWang, Yufei, Zhou, Xian, Feng, Chen, Tsun-Hsuan, Wang, Yian, Wang, Katerina, Fragkiadaki, Zackory, Erickson, David, Held, Chuang, Gan.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2311.01455\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eProcTHOR: Large-Scale Embodied AI Using Procedural Generation\u003c/strong\u003e, NeurIPS, 2022\u003cbr\u003e\nDeitke, VanderBilt, Herrasti, Weihs, Salvador, Ehsani, Han, Kolve, Farhadi, Kembhavi, Mottaghi\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2206.06994\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation\u003c/strong\u003e, NeurIPS, 2021\u003cbr\u003e\nGan, Chuang, J., Schwartz, Seth, Alter, Martin, Schrimpf, James, Traer, JulianDe, Freitas, Jonas, Kubilius, Abhishek, Bhandwaldar, Nick, Haber, Megumi, Sano, Kuno, Kim, Elias, Wang, Damian, Mrowca, Michael, Lingelbach, Aidan, Curtis, KevinT., Feigelis, DavidM., Bear, Dan, Gutfreund, DavidD., Cox, JamesJ., DiCarlo, JoshH., McDermott, JoshuaB., Tenenbaum, Daniel, Yamins.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2007.04954\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eiGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes\u003c/strong\u003e, IROS, 2021\u003cbr\u003e\nShen, Bokui, Fei, Xia, Chengshu, Li, Roberto, Martn-Martn, Linxi, Fan, Guanzhi, Wang, Claudia, Prez-DArpino, Shyamal, Buch, Sanjana, Srivastava, Lyne, Tchapmi, Micael, Tchapmi, Kent, Vainio, Josiah, Wong, Li, Fei-Fei, Silvio, Savarese.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2012.02924\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSAPIEN: A SimulAted Part-Based Interactive ENvironment\u003c/strong\u003e, CVPR, 2020\u003cbr\u003e\nXiang, Fanbo, Yuzhe, Qin, Kaichun, Mo, Yikuan, Xia, Hao, Zhu, Fangchen, Liu, Minghua, Liu, Hanxiao, Jiang, Yifu, Yuan, He, Wang, Li, Yi, Angel X., Chang, Leonidas J., Guibas, Hao, Su.\u003cbr\u003e\n[\u003ca href=\"http://openaccess.thecvf.com/content_CVPR_2020/papers/Xiang_SAPIEN_A_SimulAted_Part-Based_Interactive_ENvironment_CVPR_2020_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eHabitat: A Platform for Embodied AI Research\u003c/strong\u003e, ICCV, 2019\u003cbr\u003e\nSavva, Manolis, Abhishek, Kadian, Oleksandr, Maksymets, Yili, Zhao, Erik, Wmans, Bhavana, Jain, Julian, Straub, Jia, Liu, Vladlen, Koltun, Jitendra, Malik, Devi, Parikh, Dhruv, Batra.\u003cbr\u003e\n[\u003ca href=\"http://openaccess.thecvf.com/content_ICCV_2019/papers/Savva_Habitat_A_Platform_for_Embodied_AI_Research_ICCV_2019_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVirtualHome: Simulating Household Activities Via Programs\u003c/strong\u003e, CVPR, 2018\u003cbr\u003e\nPuig, Xavier, Kevin, Ra, Marko, Boben, Jiaman, Li, Tingwu, Wang, Sanja, Fidler, Antonio, Torralba.\u003cbr\u003e\n[\u003ca href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMatterport3D: Learning from RGB-D Data in Indoor Environments\u003c/strong\u003e, 3DV, 2017\u003cbr\u003e\nChang, Angel, Angela, Dai, Thomas, Funkhouser, Maciej, Halber, Matthias, Niebner, Manolis, Savva, Shuran, Song, Andy, Zeng, Yinda, Zhang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1709.06158\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eAI2-THOR: An Interactive 3D Environment for Visual AI\u003c/strong\u003e. arXiv, 2017\u003cbr\u003e\nKolve, Eric, Roozbeh, Mottaghi, Daniel, Gordon, Yuke, Zhu, Abhinav, Gupta, Ali, Farhadi.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1712.05474\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e\u003ca id=\"user-content-perception\"\u003e  Embodied Perception \u003c/a\u003e\u003ca href=\"#table-of-contents\"\u003e\u003c/a\u003e \u003c/h2\u003e\u003ca id=\"user-content---embodied-perception--\" class=\"anchor\" aria-label=\"Permalink:   Embodied Perception \" href=\"#--embodied-perception--\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eActive Visual Exploration\u003c/h3\u003e\u003ca id=\"user-content-active-visual-exploration\" class=\"anchor\" aria-label=\"Permalink: Active Visual Exploration\" href=\"#active-visual-exploration\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics\u003c/strong\u003e, arxiv, 2025.\u003cbr\u003e\nEnshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, Shanghang Zhang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/abs/2506.04308\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e] [\u003ca href=\"https://zhoues.github.io/RoboRefer/\" rel=\"nofollow\"\u003eProject\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003e3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians\u003c/strong\u003e, arxiv, 2025.\u003cbr\u003e\nZeming Wei, Junyi Lin, Yang Liu, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2504.11218\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e] [\u003ca href=\"https://github.com/HCPLab-SYSU/3DAffordSplat\"\u003eProject\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eCode-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection\u003c/strong\u003e, CVPR, 2025.\u003cbr\u003e\nEnshen Zhou, Qi Su, Cheng Chi, Zhizheng Zhang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, He Wang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/abs/2412.04455\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e] [\u003ca href=\"https://zhoues.github.io/Code-as-Monitor/\" rel=\"nofollow\"\u003eProject\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSnapMem: Snapshot-based 3D Scene Memory for Embodied Exploration and Reasoning\u003c/strong\u003e, arxiv, 2024.\u003cbr\u003e\nYuncong Yang, Han Yang, Jiachen Zhou, Peihao Chen, Hongxin Zhang, Yilun Du, Chuang Gan.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2411.17735\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eAIR-Embodied: An Efficient Active 3DGS-based Interaction and Reconstruction Framework with Embodied Large Language Model\u003c/strong\u003e, arxiv, 2024.\u003cbr\u003e\nZhenghao Qi, Shenghai Yuan, Fen Liu, Haozhi Cao, Tianchen Deng, Jianfei Yang, Lihua Xie.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2409.16019\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eBEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nYunhao Ge, Yihe Tang, Jiashu Xu, Cem Gokmen, Chengshu Li, Wensi Ai, Benjamin Jose Martinez, Arman Aydin, Mona Anvari, Ayush K Chakravarthy, Hong-Xing Yu, Josiah Wong, Sanjana Srivastava, Sharon Lee, Shengxin Zha, Laurent Itti, Yunzhu Li, Roberto Martn-Martn, Miao Liu, Pengchuan Zhang, Ruohan Zhang, Li Fei-Fei, Jiajun Wu.\u003cbr\u003e\n[\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Ge_BEHAVIOR_Vision_Suite_Customizable_Dataset_Generation_via_Simulation_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eCoarse-to-Fine Detection of Multiple Seams for Robotic Welding\u003c/strong\u003e, arxiv, 2024.\u003cbr\u003e\nPengkun Wei, Shuo Cheng, Dayou Li, Ran Song, Yipeng Zhang, Wei Zhang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2408.10710\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEvidential Active Recognition: Intelligent and Prudent Open-World Embodied Perception\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nFan, Lei, Mingfu, Liang, Yunxuan, Li, Gang, Hua, Ying, Wu.\u003cbr\u003e\n[\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Fan_Evidential_Active_Recognition_Intelligent_and_Prudent_Open-World_Embodied_Perception_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSpatialBot: Precise Spatial Understanding with Vision Language Models\u003c/strong\u003e, arxiv, 2024.\u003cbr\u003e\nWenxiao Cai, Yaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, Bo Zhao.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2406.13642\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEmbodied Uncertainty-Aware Object Segmentations\u003c/strong\u003e, IROS, 2024.\u003cbr\u003e\nXiaolin Fang, Leslie Pack Kaelbling, Tom as Lozano-P erez.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2408.04760\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePoint Transformer V3: Simpler Faster Stronger\u003c/strong\u003e, CVPR, 2024.\nWu, Xiaoyang, Li, Jiang, Peng-Shuai, Wang, Zhijian, Liu, Xihui, Liu, Yu, Qiao, Wanli, Ouyang, Tong, He, Hengshuang, Zhao.\u003cbr\u003e\n[\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Point_Transformer_V3_Simpler_Faster_Stronger_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePointMamba: A Simple State Space Model for Point Cloud Analysis\u003c/strong\u003e, arXiv, 2024.\u003cbr\u003e\nLiang, Dingkang, Xin, Zhou, Xinyu, Wang, Xingkui, Zhu, Wei, Xu, Zhikang, Zou, Xiaoqing, Ye, Xiang, Bai.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2402.10739\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePoint Could Mamba: Point Cloud Learning via State Space Model\u003c/strong\u003e, arXiv, 2024.\u003cbr\u003e\nZhang, Tao, Xiangtai, Li, Haobo, Yuan, Shunping, Ji, Shuicheng, Yan.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2403.00762\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMamba3d: Enhancing local features for 3d point cloud analysis via state space model\u003c/strong\u003e, arXiv, 2024.\u003cbr\u003e\nHan, Xu, Yuan, Tang, Zhaoxuan, Wang, Xianzhi, Li.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2404.14966\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGs-slam: Dense visual slam with 3d gaussian splatting\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nYan, Chi, Delin, Qu, Dan, Xu, Bin, Zhao, Zhigang, Wang, Dong, Wang, Xuelong, Li.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_GS-SLAM_Dense_Visual_SLAM_with_3D_Gaussian_Splatting_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGOReloc: Graph-based Object-Level Relocalization for Visual SLAM\u003c/strong\u003e, IEEE RAL, 2024.\u003cbr\u003e\nYutong Wang, Chaoyang Jiang, Xieyuanli Chen.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2408.07917\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEmbodiedscan: A holistic multi-modal 3d perception suite towards embodied ai\u003c/strong\u003e CVPR, 2024.\u003cbr\u003e\nWang, Tai, Xiaohan, Mao, Chenming, Zhu, Runsen, Xu, Ruiyuan, Lyu, Peisen, Li, Xiao, Chen, Wenwei, Zhang, Kai, Chen, Tianfan, Xue, others.\u003cbr\u003e\n[\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_EmbodiedScan_A_Holistic_Multi-Modal_3D_Perception_Suite_Towards_Embodied_AI_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eNeu-nbv: Next best view planning using uncertainty estimation in image-based neural rendering\u003c/strong\u003e, IROS, 2023.\u003cbr\u003e\nJin, Liren, Xieyuanli, Chen, Julius, Rckin, Marija, Popovi'c.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2303.01284\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eOff-policy evaluation with online adaptation for robot exploration in challenging environments\u003c/strong\u003e, IEEE Robotics and Automation Letters, 2023.\u003cbr\u003e\nHu, Yafei, Junyi, Geng, Chen, Wang, John, Keller, Sebastian, Scherer.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2204.03140\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eOVD-SLAM: An online visual SLAM for dynamic environments\u003c/strong\u003e, IEEE Sensors Journal, 2023.\u003cbr\u003e\nHe, Jiaming, Mingrui, Li, Yangyang, Wang, Hongyu, Wang.\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/10113832\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTransferring implicit knowledge of non-visual object properties across heterogeneous robot morphologies\u003c/strong\u003e, ICRA, 2023.\u003cbr\u003e\nTatiya, Gyan, Jonathan, Francis, Jivko, Sinapov.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2209.06890\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSwin3d: A pretrained transformer backbone for 3d indoor scene understanding\u003c/strong\u003e, arXiv, 2023.\u003cbr\u003e\nYang, Yu-Qi, Yu-Xiao, Guo, Jian-Yu, Xiong, Yang, Liu, Hao, Pan, Peng-Shuai, Wang, Xin, Tong, Baining, Guo.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2304.06906\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePoint transformer v2: Grouped vector attention and partition-based pooling\u003c/strong\u003e, NeurIPS, 2022.\u003cbr\u003e\nWu, Xiaoyang, Yixing, Lao, Li, Jiang, Xihui, Liu, Hengshuang, Zhao.\u003cbr\u003e\n[\u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2022/file/d78ece6613953f46501b958b7bb4582f-Paper-Conference.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRethinking network design and local geometry in point cloud: A simple residual MLP framework\u003c/strong\u003e, arXiv, 2022.\nMa, Xu, Can, Qin, Haoxuan, You, Haoxi, Ran, Yun, Fu.\n[\u003ca href=\"https://arxiv.org/pdf/2202.07123\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSo-slam: Semantic object slam with scale proportional and symmetrical texture constraints\u003c/strong\u003e. IEEE Robotics and Automation Letters 7. 2(2022): 40084015.\u003cbr\u003e\nLiao, Ziwei, Yutong, Hu, Jiadong, Zhang, Xianyu, Qi, Xiaoyu, Zhang, Wei, Wang.\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/9705562\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSG-SLAM: A real-time RGB-D visual SLAM toward dynamic scenes with semantic and geometric information\u003c/strong\u003e, IEEE Transactions on Instrumentation and Measurement 72. (2022): 112.\u003cbr\u003e\nCheng, Shuhong, Changhe, Sun, Shun, Zhang, Dianfan, Zhang.\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/9978699\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePoint transformer\u003c/strong\u003e, ICCV, 2021.\nZhao, Hengshuang, Li, Jiang, Jiaya, Jia, Philip HS, Torr, Vladlen, Koltun.\u003cbr\u003e\n[\u003ca href=\"https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Point_Transformer_ICCV_2021_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePointpillars: Fast encoders for object detection from point clouds\u003c/strong\u003e, CVPR, 2019.\u003cbr\u003e\nLang, Alex H, Sourabh, Vora, Holger, Caesar, Lubing, Zhou, Jiong, Yang, Oscar, Beijbom.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content_CVPR_2019/papers/Lang_PointPillars_Fast_Encoders_for_Object_Detection_From_Point_Clouds_CVPR_2019_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003e4d spatio-temporal convnets: Minkowski convolutional neural networks\u003c/strong\u003e, CVPR, 2019.\u003cbr\u003e\nChoy, Christopher, JunYoung, Gwak, Silvio, Savarese.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content_CVPR_2019/papers/Choy_4D_Spatio-Temporal_ConvNets_Minkowski_Convolutional_Neural_Networks_CVPR_2019_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eCubeslam: Monocular 3-d object slam\u003c/strong\u003e, IEEE T-RO 35. 4(2019): 925938\u003cbr\u003e\nYang, Shichao, Sebastian, Scherer.\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/8708251\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eHierarchical topic model based object association for semantic SLAM\u003c/strong\u003e, IEEE T-VCG 25. 11(2019): 30523062\u003cbr\u003e\nZhang, Jianhua, Mengping, Gui, Qichao, Wang, Ruyu, Liu, Junzhe, Xu, Shengyong, Chen.\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/8794595\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDS-SLAM: A semantic visual SLAM towards dynamic environments\u003c/strong\u003e, IROS, 2018\u003cbr\u003e\nYu, Chao, Zuxin, Liu, Xin-Jun, Liu, Fugui, Xie, Yi, Yang, Qi, Wei, Qiao, Fei.\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/8593691\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDynaSLAM: Tracking, mapping, and inpainting in dynamic scenes\u003c/strong\u003e, IEEE Robotics and Automation Letters 3. 4(2018): 40764083\u003cbr\u003e\nBescos, Berta, Jos M, Facil, Javier, Civera, Jos, Neira.\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/8421015\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eQuadricslam: Dual quadrics from object detections as landmarks in object-oriented slam\u003c/strong\u003e, IEEE Robotics and Automation Letters 4. 1(2018): 18.\u003cbr\u003e\nNicholson, Lachlan, Michael, Milford, Niko, Snderhauf.\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/8440105\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003e3d semantic segmentation with submanifold sparse convolutional networks\u003c/strong\u003e, CVPR, 2018.\u003cbr\u003e\nGraham, Benjamin, Martin, Engelcke, Laurens, Van Der Maaten.\u003cbr\u003e\n[\u003ca href=\"https://openaccess.thecvf.com/content_cvpr_2018/papers/Graham_3D_Semantic_Segmentation_CVPR_2018_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLearning to look around: Intelligently exploring unseen environments for unknown tasks\u003c/strong\u003e, CVPR, 2018.\u003cbr\u003e\nJayaraman, Dinesh, Kristen, Grauman.\u003cbr\u003e\n[\u003ca href=\"https://openaccess.thecvf.com/content_cvpr_2018/papers/Jayaraman_Learning_to_Look_CVPR_2018_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMulti-view 3d object detection network for autonomous driving\u003c/strong\u003e, CVPR, 2017.\u003cbr\u003e\nChen, Xiaozhi, Huimin, Ma, Ji, Wan, Bo, Li, Tian, Xia.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Multi-View_3D_Object_CVPR_2017_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSemantic scene completion from a single depth image\u003c/strong\u003e, CVPR, 2017.\u003cbr\u003e\nSong, Shuran, Fisher, Yu, Andy, Zeng, Angel X, Chang, Manolis, Savva, Thomas, Funkhouser.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content_cvpr_2017/papers/Song_Semantic_Scene_Completion_CVPR_2017_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePointnet: Deep learning on point sets for 3d classification and segmentation\u003c/strong\u003e, CVPR, 2017.\u003cbr\u003e\nQi, Charles R, Hao, Su, Kaichun, Mo, Leonidas J, Guibas.\u003cbr\u003e\n[[page](Pointnet: Deep learning on point sets for 3d classification and segmentation)]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePointnet++: Deep hierarchical feature learning on point sets in a metric space\u003c/strong\u003e, NeurIPS, 2017.\u003cbr\u003e\nQi, Charles Ruizhongtai, Li, Yi, Hao, Su, Leonidas J, Guibas.\u003cbr\u003e\n[\u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eThe curious robot: Learning visual representations via physical interactions\u003c/strong\u003e, ECCV, 2016.\u003cbr\u003e\nPinto, Lerrel, Dhiraj, Gandhi, Yuanfeng, Han, Yong-Lae, Park, Abhinav, Gupta.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1604.01360\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMulti-view convolutional neural networks for 3d shape recognition\u003c/strong\u003e, ICCV, 2015.\u003cbr\u003e\nSu, Hang, Subhransu, Maji, Evangelos, Kalogerakis, Erik, Learned-Miller.\u003cbr\u003e\n\u003ca href=\"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Su_Multi-View_Convolutional_Neural_ICCV_2015_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVoxnet: A 3d convolutional neural network for real-time object recognition\u003c/strong\u003e, IROS, 2015.\u003cbr\u003e\nMaturana, Daniel, Sebastian, Scherer.\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/7353481\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eORB-SLAM: a versatile and accurate monocular SLAM system\u003c/strong\u003e IEEE T-RO 31. 5(2015): 11471163\u003cbr\u003e\nMur-Artal, Raul, Jose Maria Martinez, Montiel, Juan D, Tardos.\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/7219438/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLSD-SLAM: Large-scale direct monocular SLAM\u003c/strong\u003e, ECCV, 2014\u003cbr\u003e\nEngel, Jakob, Thomas, Schops, Daniel, Cremers.\u003cbr\u003e\n\u003ca href=\"https://link.springer.com/chapter/10.1007/978-3-319-10605-2_54\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSlam++: Simultaneous localisation and mapping at the level of objects\u003c/strong\u003e, CVPR, 2013\u003cbr\u003e\nSalas-Moreno, Renato F, Richard A, Newcombe, Hauke, Strasdat, Paul HJ, Kelly, Andrew J, Davison.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content_cvpr_2013/papers/Salas-Moreno_SLAM_Simultaneous_Localisation_2013_CVPR_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDTAM: Dense tracking and mapping in real-time\u003c/strong\u003e, ICCV, 2011\u003cbr\u003e\nNewcombe, Richard A, Steven J, Lovegrove, Andrew J, Davison.\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/6126513/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMonoSLAM: Real-time single camera SLAM\u003c/strong\u003e, IEEE T-PAMI, 2007.\u003cbr\u003e\nDavison, Andrew J, Ian D, Reid, Nicholas D, Molton, Olivier, Stasse.\u003cbr\u003e\n\u003ca href=\"http://www.doc.ic.ac.uk/~ajd/Publications/davison_etal_pami2007.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eA multi-state constraint Kalman filter for vision-aided inertial navigation\u003c/strong\u003e, IROS, 2007\u003cbr\u003e\nMourikis, Anastasios I, Stergios I, Roumeliotis.\u003cbr\u003e\n\u003ca href=\"https://intra.engr.ucr.edu/~mourikis/tech_reports/TR_MSCKF.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eParallel tracking and mapping for small AR workspaces\u003c/strong\u003e, ISMAR, 2007\u003cbr\u003e\nKlein, Georg, David, Murray.\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/4538852/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e3D Visual Perception and Grounding\u003c/h3\u003e\u003ca id=\"user-content-3d-visual-perception-and-grounding\" class=\"anchor\" aria-label=\"Permalink: 3D Visual Perception and Grounding\" href=\"#3d-visual-perception-and-grounding\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eUAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation\u003c/strong\u003e, ICRA, 2025\nYihe Tang, Wenlong Huang, Yingke Wang, Chengshu Li, Roy Yuan, Ruohan Zhang, Jiajun Wu, Li Fei-Fei\u003cbr\u003e\n\u003ca href=\"https://openreview.net/pdf?id=an953WOpo2\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGrounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions\u003c/strong\u003e, arxiv, 2025\u003cbr\u003e\nHe Zhu, Quyu Kong, Kechun Xu, Xunlong Xia, Bing Deng, Jieping Ye, Rong Xiong, Yue Wang\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2504.04744\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003e3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds\u003c/strong\u003e, arxiv, 2025\u003cbr\u003e\nHengshuo Chu, Xiang Deng, Qi Lv, Xiaoyang Chen, Yinchuan Li, Jianye Hao, Liqiang Nie\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2502.20041\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSeqAfford: Sequential 3D affordance reasoning via Multimodal Large Language Model\u003c/strong\u003e, CVPR, 2025\u003cbr\u003e\nHanqing Wang, Chunlin Yu, Haoyang Luo, Jingyi Yu, Ye Shi, Jingya Wang\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2412.01550\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency\u003c/strong\u003e, CVPR, 2025\u003cbr\u003e\nDongyue Lu, Lingdong Kong, Tianxin Huang, Gim Hee Lee\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2412.09511\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nYawen Shao, Wei Zhai, Yuhang Yang, Hongchen Luo, Yang Cao, Zheng-Jun Zha, CVPR, 2025\n\u003ca href=\"https://arxiv.org/pdf/2411.19626\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLASO: Language-guided affordance segmentation on 3d object\u003c/strong\u003e, CVPR, 2024\u003cbr\u003e\nYicong Li, Na Zhao, Junbin Xiao, Chun Feng, Xiang Wang, Tat-seng Chua\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Li_LASO_Language-guided_Affordance_Segmentation_on_3D_Object_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSceneFun3D: fine-grained functionality and affordance understanding in 3D scenes\u003c/strong\u003e, CVPR, 2024\u003cbr\u003e\nAlexandros Delitzas, Ayca Takmaz, Federico Tombari, Robert Sumner, Marc Pollefeys, Francis Engelmann\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/html/Delitzas_SceneFun3D_Fine-Grained_Functionality_and_Affordance_Understanding_in_3D_Scenes_CVPR_2024_paper.html\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLanguage-conditioned affordance-pose detection in 3d point clouds\u003c/strong\u003e, ICRA, 2024\u003cbr\u003e\nToan Nguyen, Minh Nhat Vu, Baoru Huang, Tuan Van Vo, Vy Truong, Ngan Le, Thieu Vo, Bac Le, Anh Nguyen\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2309.10911\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDSPNet: Dual-vision Scene Perception for Robust 3D Question Answering\u003c/strong\u003e, CVPR, 2025\u003cbr\u003e\nJingzhou Luo, Yang Liu, Weixing Chen, Zhen Li, Yaowei Wang, Guanbin Li, Liang Lin\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2503.03190\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003ca href=\"https://github.com/LZ-CH/DSPNet\"\u003eproject\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLearning 2D Invariant Affordance Knowledge for 3D Affordance Grounding\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nXianqiang Gao, Pingrui Zhang, Delin Qu, Dong Wang, Zhigang Wang, Yan Ding, Bin Zhao, Xuelong Li\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2408.13024\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEmbodiedSAM: Online Segment Any 3D Thing in Real Time\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nXiuwei Xu, Huangxing Chen, Linqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2408.11811\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eOpenScan: A Benchmark for Generalized Open-Vocabulary 3D Scene Understanding\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nYoujun Zhao, Jiaying Lin, Shuquan Ye, Qianshi Pang, Rynson W.H. Lau\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2408.11030\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLLMI3D: Empowering LLM with 3D Perception from a Single 2D Image\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nFan Yang, Sicheng Zhao, Yanhao Zhang, Haoxiang Chen, Hui Chen, Wenbo Tang, Haonan Lu, Pengfei Xu, Zhenyu Yang, Jungong Han, Guiguang Ding\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2408.07422\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nRuiyuan Lyu, Tai Wang, Jingli Lin, Shuai Yang, Xiaohan Mao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu, Dahua Lin, Jiangmiao Pang\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2406.09401\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eShapeLLM: Universal 3D Object Understanding for Embodied Interaction\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nZekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, He Wang, Li Yi, Kaisheng Ma\u003cbr\u003e\n\u003ca href=\"https://qizekun.github.io/shapellm/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLEO: An Embodied Generalist Agent in 3D World\u003c/strong\u003e, ICML, 2024\u003cbr\u003e\nJiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang\u003cbr\u003e\n\u003ca href=\"https://embodied-generalist.github.io/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding\u003c/strong\u003e, ECCV, 2024\u003cbr\u003e\nBaoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, and Siyuan Huang\u003cbr\u003e\n\u003ca href=\"https://scene-verse.github.io/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePQ3D: Unifying 3D Vision-Language Understanding via Promptable Queries\u003c/strong\u003e, ECCV, 2024\u003cbr\u003e\nZiyu Zhu, Zhuofan Zhang, Xiaojian Ma, Xuesong Niu, Yixin Chen, Baoxiong Jia, Zhidong Deng, Siyuan Huang, and Qing Li\u003cbr\u003e\n\u003ca href=\"https://3d-vista.github.io/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World\u003c/strong\u003e, CVPR, 2024\u003cbr\u003e\nYining Hong, Zishuo Zheng, Peihao Chen, Yian Wang, Junyan Li, Chuang Gan\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Hong_MultiPLY_A_Multisensory_Object-Centric_Embodied_Large_Language_Model_in_3D_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception\u003c/strong\u003e, CVPR, 2024\u003cbr\u003e\nYiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, Jing Shao\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Qin_MP5_A_Multi-modal_Open-ended_Embodied_System_in_Minecraft_via_Active_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMaskClustering: View Consensus based Mask Graph Clustering for Open-Vocabulary 3D Instance Segmentation\u003c/strong\u003e, CVPR, 2024\u003cbr\u003e\nMi Yan, Jiazhao Zhang, Yan Zhu, He Wang\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2401.07745\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding\u003c/strong\u003e, CVPR, 2024\u003cbr\u003e\nYun Liu, Haolin Yang, Xu Si, Ling Liu, Zipeng Li, Yuxiang Zhang, Yebin Liu, Li Yi\u003cbr\u003e\n\u003ca href=\"https://taco2024.github.io/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding\u003c/strong\u003e, CVPR, 2023\u003cbr\u003e\nWu, Yanmin and Cheng, Xinhua and Zhang, Renrui and Cheng, Zesen and Zhang, Jian\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_EDA_Explicit_Text-Decoupling_and_Dense_Alignment_for_3D_Visual_Grounding_CVPR_2023_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eAffordpose: A large-scale dataset of hand-object interactions with affordance-driven hand pose\u003c/strong\u003e, ICCV, 2023\u003cbr\u003e\nJuntao Jian, Xiuping Liu, Manyi Li, Ruizhen Hu, Jian Liu\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/ICCV2023/html/Jian_AffordPose_A_Large-Scale_Dataset_of_Hand-Object_Interactions_with_Affordance-Driven_Hand_ICCV_2023_paper.html\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGrounding 3d object affordance from 2d interactions in images\u003c/strong\u003e, ICCV, 2023\u003cbr\u003e\nYuhang Yang, Wei Zhai, Hongchen Luo, Yang Cao, Jiebo Luo, Zheng-Jun Zha\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Grounding_3D_Object_Affordance_from_2D_Interactions_in_Images_ICCV_2023_paper.html\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003e3d-vista: Pre-trained transformer for 3d vision and text alignment\u003c/strong\u003e, ICCV, 2023\u003cbr\u003e\nZiyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li\u003cbr\u003e\n\u003ca href=\"https://3d-vista.github.io/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLeaF: Learning Frames for 4D Point Cloud Sequence Understanding\u003c/strong\u003e, ICCV, 2023\u003cbr\u003e\nYunze Liu, Junyu Chen, Zekai Zhang, Li Yi\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_LeaF_Learning_Frames_for_4D_Point_Cloud_Sequence_Understanding_ICCV_2023_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSQA3D: Situated Question Answering in 3D Scenes\u003c/strong\u003e, ICLR, 2023\u003cbr\u003e\nXiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang\u003cbr\u003e\n[page]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent\u003c/strong\u003e, arXix, 2023\u003cbr\u003e\nYang, Jianing and Chen, Xuweiyi and Qian, Shengyi and Madaan, Nikhil and Iyengar, Madhavan and Fouhey, David F and Chai, Joyce\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2309.12311\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVisual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding\u003c/strong\u003e, arXix, 2023\u003cbr\u003e\nYuan, Zhihao and Ren, Jinke and Feng, Chun-Mei and Zhao, Hengshuang and Cui, Shuguang and Li, Zhen\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2311.15383\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMulti-view transformer for 3D visual grounding\u003c/strong\u003e, CVPR, 2022\u003cbr\u003e\nHuang, Shijia and Chen, Yilun and Jia, Jiaya and Wang, Liwei\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2204.02174\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLook Around and Refer: 2D Synthetic Semantics Knowledge Distillation for 3D Visual Grounding\u003c/strong\u003e, CVPR, 2022\u003cbr\u003e\nBakr, Eslam and Alsaedy, Yasmeen and Elhoseiny, Mohamed\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2211.14241\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003e3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection\u003c/strong\u003e, CVPR, 2022\u003cbr\u003e\nLuo, Junyu and Fu, Jiahui and Kong, Xianghao and Gao, Chen and Ren, Haibing and Shen, Hao and Xia, Huaxia and Liu, Si\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2204.06272\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eBottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds\u003c/strong\u003e, ECCV, 2022\u003cbr\u003e\nJain, Ayush and Gkanatsios, Nikolaos and Mediratta, Ishita and Fragkiadaki, Katerina\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2112.08879\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003e3d affordancenet: A benchmark for visual object affordance understanding\u003c/strong\u003e, CVPR, 2021\u003cbr\u003e\nShengheng Deng, Xun Xu, Chaozheng Wu, Ke Chen, Kui Jia\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2021/html/Deng_3D_AffordanceNet_A_Benchmark_for_Visual_Object_Affordance_Understanding_CVPR_2021_paper.html\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eText-guided graph neural networks for referring 3D instance segmentation\u003c/strong\u003e, AAAI, 2021\u003cbr\u003e\nHuang, Pin-Hao and Lee, Han-Hung and Chen, Hwann-Tzong and Liu, Tyng-Luh\u003cbr\u003e\n\u003ca href=\"https://ojs.aaai.org/index.php/AAAI/article/view/16253/16060\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eInstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring\u003c/strong\u003e, ICCV, 2021\u003cbr\u003e\nYuan, Zhihao and Yan, Xu and Liao, Yinghong and Zhang, Ruimao and Wang, Sheng and Li, Zhen and Cui, Shuguang\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2103.01128\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eFree-form Description Guided 3D Visual Graph Network for Object Grounding in Point Cloud\u003c/strong\u003e, CVPR, 2021\u003cbr\u003e\nFeng, Mingtao and Li, Zhen and Li, Qi and Zhang, Liang and Zhang, XiangDong and Zhu, Guangming and Zhang, Hui and Wang, Yaonan and Mian, Ajmal\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2103.16381\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSAT: 2D Semantics Assisted Training for 3D Visual Grounding\u003c/strong\u003e, CVPR, 2021\u003cbr\u003e\nYang, Zhengyuan and Zhang, Songyang and Wang, Liwei and Luo, Jiebo\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2105.11450\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLanguageRefer: Spatiallanguage model for 3D visual grounding\u003c/strong\u003e, CVPR, 2021\u003cbr\u003e\nRoh, Junha and Desingh, Karthik and Farhadi, Ali and Fox, Dieter\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2107.03438\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003e3DVG-Transformer: Relation Modeling for Visual Grounding on Point Clouds\u003c/strong\u003e, ICCV, 2021\u003cbr\u003e\nZhao, Lichen and Cai, Daigang and Sheng, Lu and Xu, Dong\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_3DVG-Transformer_Relation_Modeling_for_Visual_Grounding_on_Point_Clouds_ICCV_2021_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTransRefer3D: Entity-and-relation aware transformer for fine-grained 3D visual grounding\u003c/strong\u003e, CVPR, 2021\u003cbr\u003e\nHe, Dailan and Zhao, Yusheng and Luo, Junyu and Hui, Tianrui and Huang, Shaofei and Zhang, Aixi and Liu, Si\n\u003ca href=\"https://arxiv.org/pdf/2108.02388\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eScanRefer: 3D Object Localization in RGB-D Scans using Natural Language\u003c/strong\u003e, ECCV, 2020\u003cbr\u003e\nChen, Dave Zhenyu and Chang, Angel X and Nie{\\ss}ner, Matthias\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/1912.08830\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes\u003c/strong\u003e, ECCV, 2020\u003cbr\u003e\nAchlioptas, Panos and Abdelreheem, Ahmed and Xia, Fei and Elhoseiny, Mohamed and Guibas, Leonidas\u003cbr\u003e\n\u003ca href=\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460409.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eVisual Language Navigation\u003c/h3\u003e\u003ca id=\"user-content-visual-language-navigation\" class=\"anchor\" aria-label=\"Permalink: Visual Language Navigation\" href=\"#visual-language-navigation\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTowards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method\u003c/strong\u003e, CVPR, 2025.\u003cbr\u003e\nXinshuai Song, Weixing Chen, Yang Liu, Weikai Chen, Guanbin Li, Liang Lin.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2412.09082\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003ca href=\"https://hcplab-sysu.github.io/LH-VLN/\" rel=\"nofollow\"\u003eproject\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes and Objects\u003c/strong\u003e, arxiv, 2024.\u003cbr\u003e\nZhaowei Wang, Hongming Zhang, Tianqing Fang, Ye Tian, Yue Yang, Kaixin Ma, Xiaoman Pan, Yangqiu Song, Dong Yu.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/abs/2410.02730\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e] [\u003ca href=\"https://zhaowei-wang-nlp.github.io/divscene-project-page/\" rel=\"nofollow\"\u003eProject\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation\u003c/strong\u003e, ACL, 2024.\u003cbr\u003e\nJiaqi Chen, Bingqian Lin, Ran Xu, Zhenhua Chai, Xiaodan Liang, Kwan-Yee K. Wong.\u003cbr\u003e\n[\u003ca href=\"https://chen-judge.github.io/MapGPT/\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eNavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nBingqian Lin, Yunshuang Nie, Ziming Wei, Jiaqi Chen, Shikui Ma, Jianhua Han, Hang Xu, Xiaojun Chang, Xiaodan Liang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/abs/2403.07376\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eOMEGA: Efficient Occlusion-Aware Navigation for Air-Ground Robot in Dynamic Environments via State Space Model\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nJunming Wang, Dong Huang, Xiuxian Guan, Zekai Sun, Tianxiang Shen, Fangming Liu, Heming Cui.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2408.10618\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eCoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nHidehisa Arai, Keita Miwa, Kento Sasaki, Yu Yamaguchi, Kohei Watanabe, Shunsuke Aoki, Issei Yamamoto.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2408.10845\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eFLAME: Learning to Navigate with Multimodal LLM in Urban Environments\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nYunzhe Xu, Yiyuan Pan, Zhe Liu, Hesheng Wang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2408.11051\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eAffordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nJiaqi Chen, Bingqian Lin, Xinmin Liu, Xiaodan Liang, Kwan-Yee K Wong.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2407.05890\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEmbodied Instruction Following in Unknown Environments\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nWu, Wang, Xu, Lu, Yan.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2406.11818\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDISCO: Embodied Navigation and Interaction via Differentiable Scene Semantics and Dual-level Control\u003c/strong\u003e, arxiv, 2024.\u003cbr\u003e\nXinyu Xu, Shengcheng Luo, Yanchao Yang, Yong-Lu Li, Cewu Lu.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2407.14758\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eNOLO: Navigate Only Look Once\u003c/strong\u003e, arxiv, 2024.\u003cbr\u003e\nBohan Zhou, Jiangxing Wang, Zongqing Lu.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2408.01384\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTowards Learning a Generalist Model for Embodied Navigation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nDuo Zheng, , Shijia Huang, Lin Zhao, Yiwu Zhong, Liwei Wang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2312.02010\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eFast-Slow Test-time Adaptation for Online Vision-and-Language Navigation\u003c/strong\u003e ICML, 2024.\u003cbr\u003e\nJunyu Gao, , Xuan Yao, Changsheng Xu.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2311.13209\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDiscuss before moving: Visual language navigation via multi-expert discussions\u003c/strong\u003e, ICRA, 2024.\u003cbr\u003e\nLong, Yuxing, Xiaoqi, Li, Wenzhe, Cai, Hao, Dong.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2309.11382\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVision-and-Language Navigation via Causal Learning\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nLiuyi Wang, Qijun Chen.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2404.10241\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVolumetric Environment Representation for Vision-Language Navigation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nRui Liu, Yi Yang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2403.14158\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation\u003c/strong\u003e, CVPR 2024.\u003cbr\u003e\nWang, Zihan, Xiangyang, Li, Jiahao, Yang, Yeqi, Liu, Junjie, Hu, Ming, Jiang, Shuqiang, Jiang.\n[\u003ca href=\"https://arxiv.org/pdf/2404.01943\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eBridging zero-shot object navigation and foundation models through pixel-guided navigation skill\u003c/strong\u003e ICRA, 2024.\u003cbr\u003e\nWenzhe Cai, Siyuan Huang, Guangran Cheng, Yuxing Long, Peng Gao, Changyin Sun, and Hao Dong.\u003cbr\u003e\n\u003ca href=\"https://github.com/wzcai99/Pixel-Navigator\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eOVER-NAV: Elevating Iterative Vision-and-Language Navigation with Open-Vocabulary Detection and StructurEd Representation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nGanlong Zhao, Guanbin Li, Weikai Chen, Yizhou Yu.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_OVER-NAV_Elevating_Iterative_Vision-and-Language_Navigation_with_Open-Vocabulary_Detection_and_StructurEd_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRILA: Reflective and Imaginative Language Agent for Zero-Shot Semantic Audio-Visual Navigation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nZeyuan Yang, Jiageng Liu, Peihao Chen, Anoop Cherian, Tim K. Marks, Jonathan Le Roux, Chuang Gan.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_RILA_Reflective_and_Imaginative_Language_Agent_for_Zero-Shot_Semantic_Audio-Visual_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTowards Learning a Generalist Model for Embodied Navigation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nDuo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, Liwei Wang.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_Towards_Learning_a_Generalist_Model_for_Embodied_Navigation_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVision-and-Language Navigation via Causal Learning\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nLiuyi Wang, Zongtao He, Ronghao Dang, Mengjiao Shen, Chengju Liu, Qijun Chen.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Vision-and-Language_Navigation_via_Causal_Learning_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eInstance-aware Exploration-Verification-Exploitation for Instance ImageGoal Navigation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nXiaohan Lei, Min Wang, Wengang Zhou, Li Li, Houqiang Li.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Lei_Instance-aware_Exploration-Verification-Exploitation_for_Instance_ImageGoal_Navigation_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eHabitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nMukul Khanna, Yongsen Mao, Hanxiao Jiang, Sanjay Haresh, Brennan Shacklett, Dhruv Batra, Alexander Clegg, Eric Undersander, Angel X. Chang, Manolis Savva.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Khanna_Habitat_Synthetic_Scenes_Dataset_HSSD-200_An_Analysis_of_3D_Scene_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSchurVINS: Schur Complement-Based Lightweight Visual Inertial Navigation System\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nYunfei Fan, Tianyu Zhao, Guidong Wang.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Fan_SchurVINS_Schur_Complement-Based_Lightweight_Visual_Inertial_Navigation_System_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSPOC: Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nKiana Ehsani, Tanmay Gupta, Rose Hendrix, Jordi Salvador, Luca Weihs, Kuo-Hao Zeng, Kunal Pratap Singh, Yejin Kim, Winson Han, Alvaro Herrasti, Ranjay Krishna, Dustin Schwenk, Eli VanderBilt, Aniruddha Kembhavi.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Ehsani_SPOC_Imitating_Shortest_Paths_in_Simulation_Enables_Effective_Navigation_and_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVolumetric Environment Representation for Vision-Language Navigation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nRui Liu, Wenguan Wang, Yi Yang.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Volumetric_Environment_Representation_for_Vision-Language_Navigation_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nXiaohan Wang, Yuehu Liu, Xinhang Song, Yuyi Liu, Sixian Zhang, Shuqiang Jiang.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Khanna_GOAT-Bench_A_Benchmark_for_Multi-Modal_Lifelong_Navigation_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eAn Interactive Navigation Method with Effect-oriented Affordance\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nXiaohan Wang, Yuehu Liu, Xinhang Song, Yuyi Liu, Sixian Zhang, Shuqiang Jiang.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_An_Interactive_Navigation_Method_with_Effect-oriented_Affordance_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eImagine Before Go: Self-Supervised Generative Map for Object Goal Navigation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nSixian Zhang, Xinyao Yu, Xinhang Song, Xiaohan Wang, Shuqiang Jiang.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Imagine_Before_Go_Self-Supervised_Generative_Map_for_Object_Goal_Navigation_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMemoNav: Working Memory Model for Visual Navigation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nHongxin Li, Zeyu Wang, Xu Yang, Yuran Yang, Shuqi Mei, Zhaoxiang Zhang.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Li_MemoNav_Working_Memory_Model_for_Visual_Navigation_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVersatile Navigation Under Partial Observability via Value-guided Diffusion Policy\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nGengyu Zhang, Hao Tang, Yan Yan.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Versatile_Navigation_Under_Partial_Observability_via_Value-guided_Diffusion_Policy_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nZihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, Junjie Hu, Ming Jiang, Shuqiang Jiang.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Lookahead_Exploration_with_Neural_Radiance_Representation_for_Continuous_Vision-Language_Navigation_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSPIN: Simultaneous Perception Interaction and Navigation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nShagun Uppal, Ananye Agarwal, Haoyu Xiong, Kenneth Shaw, Deepak Pathak.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Uppal_SPIN_Simultaneous_Perception_Interaction_and_Navigation_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eCorrectable Landmark Discovery via Large Models for Vision-Language Navigation\u003c/strong\u003e, TPAMI, 2024.\u003cbr\u003e\nBingqian Lin, Yunshuang Nie, Ziming Wei, Yi Zhu, Hang Xu, Shikui Ma, Jianzhuang Liu, Xiaodan Liang.\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/10543121\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments\u003c/strong\u003e, IEEE T-PAMI, 2024.\u003cbr\u003e\nAn, Dong, Hanqing, Wang, Wenguan, Wang, Zun, Wang, Yan, Huang, Keji, He, Liang, Wang.\n[\u003ca href=\"https://arxiv.org/pdf/2304.03047\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eNaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation\u003c/strong\u003e, RSS, 2024.\u003cbr\u003e\nJiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, He Wang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2402.15852\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMarch in Chat: Interactive Prompting for Remote Embodied Referring Expression\u003c/strong\u003e, ICCV, 2023.\u003cbr\u003e\nQiao, Yanyuan, Yuankai, Qi, Zheng, Yu, Jing, Liu, Qi, Wu.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2308.10141\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMulti-level compositional reasoning for interactive instruction following\u003c/strong\u003e, AAAI, 2023.\u003cbr\u003e\nBhambri, Suvaansh, Byeonghwi, Kim, Jonghyun, Choi.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2308.09387\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVision and Language Navigation in the Real World via Online Visual Language Mapping\u003c/strong\u003e, ArXiv, 2023.\u003cbr\u003e\nChengguang Xu, , Hieu T. Nguyen, Christopher Amato, Lawson L.S. Wong.\n[\u003ca href=\"https://arxiv.org/pdf/2310.10822\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTowards Deviation-robust Agent Navigation via Perturbation-aware Contrastive Learning\u003c/strong\u003e, TPAMI, 2023.\u003cbr\u003e\nBingqian Lin, Yanxin Long, Yi Zhu, Fengda Zhu, Xiaodan Liang , Qixiang Ye, Liang Lin.\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/10120966/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eFind What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation\u003c/strong\u003e, NIPS, 2023.\u003cbr\u003e\nWang, Chen, Li, Wu, Dong.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2309.08138\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eHomeRobot: Open-Vocabulary Mobile Manipulation\u003c/strong\u003e, NIPS, 2023.\u003cbr\u003e\nYenamandra, Sriram, Arun, Ramachandran, Karmesh, Yadav, Austin, Wang, Mukul, Khanna, Theophile, Gervet, Tsung-Yen, Yang, Vidhi, Jain, AlexanderWilliam, Clegg, John, Turner, Zsolt, Kira, Manolis, Savva, Angel, Chang, DevendraSingh, Chaplot, Dhruv, Batra, Roozbeh, Mottaghi, Yonatan, Bisk, Chris, Paxton.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2306.11565\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eBehavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation\u003c/strong\u003e, Conference on Robot Learning. 2023.\u003cbr\u003e\nLi, Chengshu, Ruohan, Zhang, Josiah, Wong, Cem, Gokmen, Sanjana, Srivastava, Roberto, Mart\\in-Mart'\\in, Chen, Wang, Gabrael, Levine, Michael, Lingelbach, Jiankai, Sun, others.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2403.09227\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDialFRED: Dialogue-Enabled Agents for Embodied Instruction Following\u003c/strong\u003e, arXiv, 2022.\u003cbr\u003e\nGao, Xiaofeng, Qiaozi, Gao, Ran, Gong, Kaixiang, Lin, Govind, Thattai, GauravS., Sukhatme.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2202.13330\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eHOP: History-and-Order Aware Pretraining for Vision-and-Language Navigation\u003c/strong\u003e, CVPR, 2022.\u003cbr\u003e\nQiao, Yanyuan, Yuankai, Qi, Yicong, Hong, Zheng, Yu, Peng, Wang, Qi, Wu.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2203.11591\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eBridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation\u003c/strong\u003e, CVPR, 2022.\u003cbr\u003e\nHong, Yicong, Zun, Wang, Qi, Wu, Stephen, Gould.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2203.02764\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eFILM: Following Instructions in Language with Modular Methods\u003c/strong\u003e, ICLR, 2022.\u003cbr\u003e\nSo Yeon Min, , Devendra Singh Chaplot, Pradeep Kumar Ravikumar, Yonatan Bisk, Ruslan Salakhutdinov.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2110.07342\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action\u003c/strong\u003e, Conference on Robot Learning. 2022.\u003cbr\u003e\nDhruv Shah, , Blazej Osinski, Brian Ichter, Sergey Levine.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2207.04429\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSOON: Scenario Oriented Object Navigation with Graph-based Exploration\u003c/strong\u003e, CVPR, 2021.\u003cbr\u003e\nZhu, Fengda, Xiwen, Liang, Yi, Zhu, Qizhi, Yu, Xiaojun, Chang, Xiaodan, Liang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2103.17138\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVision-Language Navigation Policy Learning and Adaptation\u003c/strong\u003e, IEEE T-PAMI 43. 12(2021): 4205-4216.\u003cbr\u003e\nWang, Xin, Qiuyuan, Huang, Asli, Celikyilmaz, Jianfeng, Gao, Dinghan, Shen, Yuan-Fang, Wang, William Yang, Wang, Lei, Zhang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/https://ieeexplore.ieee.org/document/8986691\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eNeighbor-view enhanced model for vision and language navigation\u003c/strong\u003e, MM, 2021.\u003cbr\u003e\nAn, Dong, Yuankai, Qi, Yan, Huang, Qi, Wu, Liang, Wang, Tieniu, Tan.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2107.07201\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eBeyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments\u003c/strong\u003e, ECCV, 2020.\u003cbr\u003e\nKrantz, Jacob and Wijmans, Erik and Majumdar, Arjun and Batra, Dhruv and Lee, Stefan.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2004.02857\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eREVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments\u003c/strong\u003e, CVPR, 2020.\u003cbr\u003e\nQi, Yuankai, Qi, Wu, Peter, Anderson, Xin, Wang, William Yang, Wang, Chunhua, Shen, Anton, Hengel.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1904.10151\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks\u003c/strong\u003e, CVPR, 2020.\u003cbr\u003e\nShridhar, Mohit, Jesse, Thomason, Daniel, Gordon, Yonatan, Bisk, Winson, Han, Roozbeh, Mottaghi, Luke, Zettlemoyer, Dieter, Fox.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1912.01734\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVision-and-dialog navigation\u003c/strong\u003e, Conference on Robot Learning. 2020.\u003cbr\u003e\nThomason, Jesse, Michael, Murray, Maya, Cakmak, Luke, Zettlemoyer.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1907.04957\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLanguage and visual entity relationship graph for agent navigation\u003c/strong\u003e, NeurIPS, 2020.\u003cbr\u003e\nHong, Yicong, Cristian, Rodriguez, Yuankai, Qi, Qi, Wu, Stephen, Gould.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2010.09304\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLanguage-Guided Navigation via Cross-Modal Grounding and Alternate Adversarial Learning\u003c/strong\u003e, IEEE T-CSVT 31. (2020): 3469-3481.\u003cbr\u003e\nWeixia Zhang, , Chao Ma, Qi Wu, Xiaokang Yang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2011.10972\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eStay on the Path: Instruction Fidelity in Vision-and-Language Navigation\u003c/strong\u003e, ACL, 2019.\u003cbr\u003e\nJain, Vihan, Gabriel, Magalhaes, Alexander, Ku, Ashish, Vaswani, Eugene, Ie, Jason, Baldridge.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1905.12255\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments\u003c/strong\u003e, CVPR, 2019.\u003cbr\u003e\nChen, Howard, Alane, Suhr, Dipendra, Misra, Noah, Snavely, Yoav, Artzi.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1811.12354\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments\u003c/strong\u003e, CVPR, 2018.\u003cbr\u003e\nAnderson, Peter, Qi, Wu, Damien, Teney, Jake, Bruce, Mark, Johnson, Niko, Sunderhauf, Ian, Reid, Stephen, Gould, Anton, Hengel.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1711.07280\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLook Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation\u003c/strong\u003e, ECCV, 2018.\u003cbr\u003e\nXin Eric Wang, , Wenhan Xiong, Hongmin Wang, William Yang Wang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1803.07729\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eNon-Visual Perception: Tactile\u003c/h3\u003e\u003ca id=\"user-content-non-visual-perception-tactile\" class=\"anchor\" aria-label=\"Permalink: Non-Visual Perception: Tactile\" href=\"#non-visual-perception-tactile\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eWhen Vision Meets Touch: A Contemporary Review for Visuotactile Sensors from the Signal Processing Perspective\u003c/strong\u003e, Arxiv, 2024.\u003cbr\u003e\nLi, Shoujie and Wang, Zihan and Wu, Changsheng and Li, Xiang and Luo, Shan and Fang, Bin and Sun, Fuchun and Zhang, Xiao-Ping and Ding, Wenbo.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2406.12226\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEnhancing Generalizable 6D Pose Tracking of an In-Hand Object with Tactile Sensing\u003c/strong\u003e, RA-L, 2024.\u003cbr\u003e\nYun Liu, Xiaomeng Xu, Weihang Chen, Haocheng Yuan, He Wang, Jing Xu, Rui Chen, Li Yi.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/abs/2210.04026\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLearning visuotactile skills with two multifingered hands\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nLin, Toru and Zhang, Yu and Li, Qiyang and Qi, Haozhi and Yi, Brent and Levine, Sergey and Malik, Jitendra.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2404.16823\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eBinding touch to everything: Learning unified multimodal tactile representations\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nYang, Fengyu and Feng, Chao and Chen, Ziyang and Park, Hyoungseob and Wang, Daniel and Dou, Yiming and Zeng, Ziyao and Chen, Xien and Gangopadhyay, Rit and Owens, Andrew and others.\u003cbr\u003e\n[\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Binding_Touch_to_Everything_Learning_Unified_Multimodal_Tactile_Representations_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eBioinspired sensors and applications in intelligent robots: a review\u003c/strong\u003e, Robotic Intelligence and Automation, 2024.\u003cbr\u003e\nZhou, Yanmin and Yan, Zheng and Yang, Ye and Wang, Zhipeng and Lu, Ping and Yuan, Philip F and He, Bin.\u003cbr\u003e\n[\u003ca href=\"https://www.emerald.com/insight/content/doi/10.1108/RIA-07-2023-0088/full/pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGive Me a Sign: Using Data Gloves for Static Hand-Shape Recognition\u003c/strong\u003e, Sensors, 2023.\u003cbr\u003e\nAchenbach, Philipp and Laux, Sebastian and Purdack, Dennis and Mller, Philipp Niklas and Gbel, Stefan.\u003cbr\u003e\n[\u003ca href=\"https://www.mdpi.com/1424-8220/23/24/9847/pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSemantics-aware adaptive knowledge distillation for sensor-to-vision action recognition\u003c/strong\u003e, IEEE Transactions on Image Processing, 2021.\u003cbr\u003e\nLiu, Yang and Wang, Keze and Li, Guanbin and Lin, Liang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2009.00210\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eHand movements: A window into haptic object recognition\u003c/strong\u003e, Cognitive psychology, 1987.\u003cbr\u003e\nLederman, Susan J and Klatzky, Roberta L.\u003cbr\u003e\n[\u003ca href=\"https://www.sciencedirect.com/science/article/pii/0010028587900089\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eForce and tactile sensing\u003c/strong\u003e, Springer Handbook of Robotics, 2016.\u003cbr\u003e\nCutkosky, Mark R and Howe, Robert D and Provancher, William R.\u003cbr\u003e\n[\u003ca href=\"https://link.springer.com/chapter/10.1007/978-3-319-32552-1_28\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eHaptic perception: A tutorial\u003c/strong\u003e, Attention, Perception, \u0026amp; Psychophysics, 2009.\u003cbr\u003e\nLederman, Susan J and Klatzky, Roberta L.\u003cbr\u003e\n[\u003ca href=\"https://link.springer.com/content/pdf/10.3758/APP.71.7.1439.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eFlexible tactile sensing based on piezoresistive composites: A review\u003c/strong\u003e, Sensors, 2014.\u003cbr\u003e\nStassi, Stefano and Cauda, Valentina and Canavese, Giancarlo and Pirri, Candido Fabrizio.\u003cbr\u003e\n[\u003ca href=\"https://www.mdpi.com/1424-8220/14/3/5296.\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTactile sensing in dexterous robot hands\u003c/strong\u003e, Robotics and Autonomous Systems, 2015.\u003cbr\u003e\nKappassov, Zhanat and Corrales, Juan-Antonio and Perdereau, Vronique.\u003cbr\u003e\n[\u003ca href=\"https://uca.hal.science/hal-01680649/document\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGelLink: A Compact Multi-phalanx Finger with Vision-based Tactile Sensing and Proprioception\u003c/strong\u003e, arXiv, 2024.\u003cbr\u003e\nMa, Yuxiang and Adelson, Edward.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2403.14887\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eA Touch, Vision, and Language Dataset for Multimodal Alignment\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nFu, Letian and Datta, Gaurav and Huang, Huang and Panitch, William Chung-Ho and Drake, Jaimyn and Ortiz, Joseph and Mukadam, Mustafa and Lambeta, Mike and Calandra, Roberto and Goldberg, Ken.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2402.13232\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLarge-scale actionless video pre-training via discrete diffusion for efficient policy learning\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nHe, Haoran and Bai, Chenjia and Pan, Ling and Zhang, Weinan and Zhao, Bin and Li, Xuelong.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2402.14407\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSnap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nComi, Mauro and Tonioni, Alessio and Yang, Max and Tremblay, Jonathan and Blukis, Valts and Lin, Yijiong and Lepora, Nathan F and Aitchison, Laurence.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2403.20275\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTactile-augmented radiance fields\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nDou, Yiming and Yang, Fengyu and Liu, Yi and Loquercio, Antonio and Owens, Andrew.\u003cbr\u003e\n[\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Dou_Tactile-Augmented_Radiance_Fields_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eAnyRotate: Gravity-Invariant In-Hand Object Rotation with Sim-to-Real Touch\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nYang, Max and Lu, Chenghua and Church, Alex and Lin, Yijiong and Ford, Chris and Li, Haoran and Psomopoulou, Efi and Barton, David AW and Lepora, Nathan F.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2405.07391\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eFeature-level Sim2Real Regression of Tactile Images for Robot Manipulation\u003c/strong\u003e, ICRA ViTac, 2024.\u003cbr\u003e\nDuan, Boyi and Qian, Kun and Zhao, Yongqiang and Zhang, Dongyuan and Luo, Shan.\u003cbr\u003e\n[\u003ca href=\"https://shanluo.github.io/ViTacWorkshops/content/ViTac2024_Paper_09.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMAE4GM: Visuo-Tactile Learning for Property Estimation of Granular Material using Multimodal Autoencoder\u003c/strong\u003e,ICRA ViTac, 2024.\u003cbr\u003e\nZhang, Zeqing and Zheng, Guangze and Ji, Xuebo and Chen, Guanqi and Jia, Ruixing and Chen, Wentao and Chen, Guanhua and Zhang, Liangjun and Pan, Jia.\u003cbr\u003e\n[\u003ca href=\"https://shanluo.github.io/ViTacWorkshops/content/ViTac2024_Paper_01.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eOctopi: Object Property Reasoning with Large Tactile-Language Models\u003c/strong\u003e, arXiv preprint arXiv:2405.02794, 2024.\u003cbr\u003e\nYu, Samson and Lin, Kelvin and Xiao, Anxing and Duan, Jiafei and Soh, Harold.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2405.02794\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003e9dtact: A compact vision-based tactile sensor for accurate 3D shape reconstruction and generalizable 6D force estimation\u003c/strong\u003e, IEEE Robotics and Automation Letters, 2023.\u003cbr\u003e\nLin, Changyi and Zhang, Han and Xu, Jikai and Wu, Lei and Xu, Huazhe.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2308.14277\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eAllsight: A low-cost and high-resolution round tactile sensor with zero-shot learning capability\u003c/strong\u003e, IEEE Robotics and Automation Letters, 2023.\u003cbr\u003e\nAzulay, Osher and Curtis, Nimrod and Sokolovsky, Rotem and Levitski, Guy and Slomovik, Daniel and Lilling, Guy and Sintov, Avishai.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2307.02928\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVistac towards a unified multi-modal sensing finger for robotic manipulation\u003c/strong\u003e, IEEE Sensors Journal, 2023.\u003cbr\u003e\nAthar, Sheeraz and Patel, Gaurav and Xu, Zhengtong and Qiu, Qiang and She, Yu.\u003cbr\u003e\n[\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/10242327/\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMidastouch: Monte-carlo inference over distributions across sliding touch\u003c/strong\u003e, CoRL, 2023.\u003cbr\u003e\nSuresh, Sudharshan and Si, Zilin and Anderson, Stuart and Kaess, Michael and Mukadam, Mustafa.\u003cbr\u003e\n[\u003ca href=\"https://proceedings.mlr.press/v205/suresh23a/suresh23a.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eThe objectfolder benchmark: Multisensory learning with neural and real objects\u003c/strong\u003e, CVPR, 2023.\u003cbr\u003e\nGao, Ruohan and Dou, Yiming and Li, Hao and Agarwal, Tanmay and Bohg, Jeannette and Li, Yunzhu and Fei-Fei, Li and Wu, Jiajun.\n[\u003ca href=\"http://openaccess.thecvf.com/content/CVPR2023/papers/Gao_The_ObjectFolder_Benchmark_Multisensory_Learning_With_Neural_and_Real_Objects_CVPR_2023_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eImagebind: One embedding space to bind them all\u003c/strong\u003e, CVPR, 2023.\u003cbr\u003e\nGirdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan.\u003cbr\u003e\n[\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTouching a nerf: Leveraging neural radiance fields for tactile sensory data generation\u003c/strong\u003e, Conference on Robot Learning, pp. 1618-1628, 2023.\u003cbr\u003e\nZhong, Shaohong and Albini, Alessandro and Jones, Oiwi Parker and Maiolino, Perla and Posner, Ingmar.\u003cbr\u003e\n[\u003ca href=\"https://proceedings.mlr.press/v205/zhong23a/zhong23a.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLearning to read braille: Bridging the tactile reality gap with diffusion models\u003c/strong\u003e, ArXiv, 2023.\u003cbr\u003e\nHiguera, Carolina and Boots, Byron and Mukadam, Mustafa.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2304.01182\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGenerating visual scenes from touch\u003c/strong\u003e, CVPR, 2023.\u003cbr\u003e\nYang, Fengyu and Zhang, Jiacheng and Owens, Andrew.\u003cbr\u003e\n[\u003ca href=\"https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Generating_Visual_Scenes_from_Touch_ICCV_2023_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDtact: A vision-based tactile sensor that measures high-resolution 3D geometry directly from darkness\u003c/strong\u003e, ICRA, 2023.\u003cbr\u003e\nLin, Changyi and Lin, Ziqi and Wang, Shaoxiong and Xu, Huazhe.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2209.13916\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eIn-hand pose estimation using hand-mounted RGB cameras and visuotactile sensors\u003c/strong\u003e, IEEE Access, 2023.\u003cbr\u003e\nGao, Yuan and Matsuoka, Shogo and Wan, Weiwei and Kiyokawa, Takuya and Koyama, Keisuke and Harada, Kensuke.\u003cbr\u003e\n[\u003ca href=\"https://ieeexplore.ieee.org/iel7/6287639/6514899/10043666.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eCollision-aware in-hand 6D object pose estimation using multiple vision-based tactile sensors\u003c/strong\u003e, ICRA, 2023.\u003cbr\u003e\nCaddeo, Gabriele M and Piga, Nicola A and Bottarel, Fabrizio and Natale, Lorenzo.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2301.13667\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eImplicit neural representation for 3D shape reconstruction using vision-based tactile sensing\u003c/strong\u003e, ArXiv, 2023.\u003cbr\u003e\nComi, Mauro and Church, Alex and Li, Kejie and Aitchison, Laurence and Lepora, Nathan F.\u003cbr\u003e\n[\u003ca href=\"https://shanluo.github.io/ViTacWorkshops/content/ViTac2023_Paper_06.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSliding touch-based exploration for modeling unknown object shape with multi-fingered hands\u003c/strong\u003e, IROS, 2023.\u003cbr\u003e\nChen, Yiting and Tekden, Ahmet Ercan and Deisenroth, Marc Peter and Bekiroglu, Yasemin.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2308.00576\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGeneral In-hand Object Rotation with Vision and Touch\u003c/strong\u003e, CoRL, 2023.\u003cbr\u003e\nQi, Haozhi and Yi, Brent and Suresh, Sudharshan and Lambeta, Mike and Ma, Yi and Calandra, Roberto and Malik, Jitendra.\u003cbr\u003e\n[\u003ca href=\"https://proceedings.mlr.press/v229/qi23a/qi23a.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSim-to-Real Model-Based and Model-Free Deep Reinforcement Learning for Tactile Pushing\u003c/strong\u003e, IEEE Robotics and Automation Letters, 2023.\u003cbr\u003e\nYang, Max and Lin, Yijiong and Church, Alex and Lloyd, John and Zhang, Dandan and Barton, David AW and Lepora, Nathan F.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2307.14272\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eUnsupervised adversarial domain adaptation for sim-to-real transfer of tactile images\u003c/strong\u003e, IEEE Transactions on Instrumentation and Measurement, 2023.\u003cbr\u003e\nJing, Xingshuo and Qian, Kun and Jianu, Tudor and Luo, Shan.\u003cbr\u003e\n[\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/10106009/\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLearn from incomplete tactile data: Tactile representation learning with masked autoencoders\u003c/strong\u003e, IROS, 2023.\u003cbr\u003e\nCao, Guanqun and Jiang, Jiaqi and Bollegala, Danushka and Luo, Shan.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2307.07358\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDexterity from touch: Self-supervised pre-training of tactile representations with robotic play\u003c/strong\u003e, ArXiv, 2023.\u003cbr\u003e\nGuzey, Irmak and Evans, Ben and Chintala, Soumith and Pinto, Lerrel.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2303.12076\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGelslim 3.0: High-resolution measurement of shape, force and slip in a compact tactile-sensing finger\u003c/strong\u003e, ICRA, 2022.\u003cbr\u003e\nTaylor, Ian H and Dong, Siyuan and Rodriguez, Alberto.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2103.12269\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTacto: A fast, flexible, and open-source simulator for high-resolution vision-based tactile sensors\u003c/strong\u003e, IEEE Robotics and Automation Letters, 2022.\u003cbr\u003e\nWang, Shaoxiong and Lambeta, Mike and Chou, Po-Wei and Calandra, Roberto.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2012.08456\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTaxim: An example-based simulation model for GelSight tactile sensors\u003c/strong\u003e, IEEE Robotics and Automation Letters, 2022.\u003cbr\u003e\nSi, Zilin and Yuan, Wenzhen.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2109.04027\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eObjectfolder 2.0: A multisensory object dataset for sim2real transfer\u003c/strong\u003e, CVPR, 2022.\u003cbr\u003e\nGao, Ruohan and Si, Zilin and Chang, Yen-Yu and Clarke, Samuel and Bohg, Jeannette and Fei-Fei, Li and Yuan, Wenzhen and Wu, Jiajun.\u003cbr\u003e\n[\u003ca href=\"http://openaccess.thecvf.com/content/CVPR2022/papers/Gao_ObjectFolder_2.0_A_Multisensory_Object_Dataset_for_Sim2Real_Transfer_CVPR_2022_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSelf-supervised visuo-tactile pretraining to locate and follow garment features\u003c/strong\u003e, ArXiv, 2022.\u003cbr\u003e\nKerr, Justin and Huang, Huang and Wilcox, Albert and Hoque, Ryan and Ichnowski, Jeffrey and Calandra, Roberto and Goldberg, Ken.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2209.13042\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVisuotactile 6D pose estimation of an in-hand object using vision and tactile sensor data\u003c/strong\u003e, IEEE Robotics and Automation Letters, 2022.\u003cbr\u003e\nDikhale, Snehal and Patel, Karankumar and Dhingra, Daksh and Naramura, Itoshi and Hayashi, Akinobu and Iba, Soshi and Jamali, Nawid.\u003cbr\u003e\n[\u003ca href=\"https://www.researchgate.net/profile/Snehal_Dikhale/publication/357842538_VisuoTactile_6D_Pose_Estimation_of_an_In-Hand_Object_Using_Vision_and_Tactile_Sensor_Data/links/6297b925416ec50bdb022987/VisuoTactile-6D-Pose-Estimation-of-an-In-Hand-Object-Using-Vision-and-Tactile-Sensor-Data.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eShapemap 3-D: Efficient shape mapping through dense touch and vision\u003c/strong\u003e, ICRA, 2022.\u003cbr\u003e\nSuresh, Sudharshan and Si, Zilin and Mangelson, Joshua G and Yuan, Wenzhen and Kaess, Michael.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2109.09884\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVisuotactile-rl: Learning multimodal manipulation policies with deep reinforcement learning\u003c/strong\u003e, ICRA, 2022.\u003cbr\u003e\nHansen, Johanna and Hogan, Francois and Rivkin, Dmitriy and Meger, David and Jenkin, Michael and Dudek, Gregory.\u003cbr\u003e\n[\u003ca href=\"https://johannah.github.io/papers/Visuotactile-RL.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTactile gym 2.0: Sim-to-real deep reinforcement learning for comparing low-cost high-resolution robot touch\u003c/strong\u003e, IEEE Robotics and Automation Letters, 2022.\u003cbr\u003e\nLin, Yijiong and Lloyd, John and Church, Alex and Lepora, Nathan F.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2207.10763\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTouch and go: Learning from human-collected vision and touch\u003c/strong\u003e, ArXiv, 2022.\u003cbr\u003e\nYang, Fengyu and Ma, Chenyang and Zhang, Jiacheng and Zhu, Jing and Yuan, Wenzhen and Owens, Andrew.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2211.12498\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eObjectfolder: A dataset of objects with implicit visual, auditory, and tactile representations\u003c/strong\u003e, arXiv, 2021.\u003cbr\u003e\nGao, Ruohan and Chang, Yen-Yu and Mall, Shivani and Fei-Fei, Li and Wu, Jiajun.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2109.07991\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLearning transferable visual models from natural language supervision\u003c/strong\u003e, International Conference on Machine Learning, 2021.\u003cbr\u003e\nRadford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others.\u003cbr\u003e\n[\u003ca href=\"http://proceedings.mlr.press/v139/radford21a/radford21a.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGelSight wedge: Measuring high-resolution 3D contact geometry with a compact robot finger\u003c/strong\u003e, ICRA, 2021.\u003cbr\u003e\nWang, Shaoxiong and She, Yu and Romero, Branden and Adelson, Edward.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2106.08851\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTactile object pose estimation from the first touch with geometric contact rendering\u003c/strong\u003e, CoRL, 2021.\u003cbr\u003e\nVillalonga, Maria Bauza and Rodriguez, Alberto and Lim, Bryan and Valls, Eric and Sechopoulos, Theo.\u003cbr\u003e\n[\u003ca href=\"https://proceedings.mlr.press/v155/villalonga21a/villalonga21a.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eActive 3D shape reconstruction from vision and touch\u003c/strong\u003e, NeurIPS, 2021.\u003cbr\u003e\nSmith, Edward and Meger, David and Pineda, Luis and Calandra, Roberto and Malik, Jitendra and Romero Soriano, Adriana and Drozdzal, Michal.\u003cbr\u003e\n[\u003ca href=\"https://proceedings.neurips.cc/paper/2021/file/8635b5fd6bc675033fb72e8a3ccc10a0-Paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eInterpreting and predicting tactile signals for the syntouch biotac\u003c/strong\u003e, The International Journal of Robotics Research, 2021.\u003cbr\u003e\nNarang, Yashraj S and Sundaralingam, Balakumar and Van Wyk, Karl and Mousavian, Arsalan and Fox, Dieter.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2101.05452\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGelTip: A finger-shaped optical tactile sensor for robotic manipulation\u003c/strong\u003e, IROS, 2020.\u003cbr\u003e\nGomes, Daniel Fernandes and Lin, Zhonglin and Luo, Shan.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2008.05404\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDIGIT: A Novel Design for a Low-Cost Compact High-Resolution Tactile Sensor With Application to In-Hand Manipulation\u003c/strong\u003e, IEEE Robotics and Automation Letters, 2020.\u003cbr\u003e\nLambeta, Mike and Chou, Po-Wei and Tian, Stephen and Yang, Brian and Maloon, Benjamin and Most, Victoria Rose and Stroud, Dave and Santos, Raymond and Byagowi, Ahmad and Kammerer, Gregg and Jayaraman, Dinesh and Calandra, Roberto.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2005.14679\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDigit: A novel design for a low-cost compact high-resolution tactile sensor with application to in-hand manipulation\u003c/strong\u003e, IEEE Robotics and Automation Letters, 2020.\u003cbr\u003e\nLambeta, Mike and Chou, Po-Wei and Tian, Stephen and Yang, Brian and Maloon, Benjamin and Most, Victoria Rose and Stroud, Dave and Santos, Raymond and Byagowi, Ahmad and Kammerer, Gregg and others.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2005.14679\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDeep tactile experience: Estimating tactile sensor output from depth sensor data\u003c/strong\u003e, IROS, 2020.\u003cbr\u003e\nPatel, Karankumar and Iba, Soshi and Jamali, Nawid.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2110.08946\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003e3D shape reconstruction from vision and touch\u003c/strong\u003e, NeurIPS, 2020.\u003cbr\u003e\nSmith, Edward and Calandra, Roberto and Romero, Adriana and Gkioxari, Georgia and Meger, David and Malik, Jitendra and Drozdzal, Michal.\u003cbr\u003e\n[\u003ca href=\"https://proceedings.neurips.cc/paper/2020/file/a3842ed7b3d0fe3ac263bcabd2999790-Paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSupervised autoencoder joint learning on heterogeneous tactile sensory data: Improving material classification performance\u003c/strong\u003e, IROS, 2020.\u003cbr\u003e\nGao, Ruihan and Taunyazov, Tasbolat and Lin, Zhiping and Wu, Yan.\u003cbr\u003e\n[\u003ca href=\"https://yan-wu.com/wp-content/uploads/2020/08/gao2020supervised.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMaking sense of vision and touch: Learning multimodal representations for contact-rich tasks\u003c/strong\u003e, IEEE Transactions on Robotics, 2020.\u003cbr\u003e\nLee, Michelle A and Zhu, Yuke and Zachares, Peter and Tan, Matthew and Srinivasan, Krishnan and Savarese, Silvio and Fei-Fei, Li and Garg, Animesh and Bohg, Jeannette.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1907.13098\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLearning efficient haptic shape exploration with a rigid tactile sensor array\u003c/strong\u003e, PloS One, 2020.\u003cbr\u003e\nFleer, Sascha and Moringen, Alexandra and Klatzky, Roberta L and Ritter, Helge.\u003cbr\u003e\n[\u003ca href=\"https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0226880\u0026amp;type=printable\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eInterpreting and predicting tactile signals via a physics-based and data-driven framework\u003c/strong\u003e, ArXiv, 2020.\u003cbr\u003e\nNarang, Yashraj S and Van Wyk, Karl and Mousavian, Arsalan and Fox, Dieter.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2006.03777\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eFast texture classification using tactile neural coding and spiking neural network\u003c/strong\u003e, IROS, 2020.\u003cbr\u003e\nTaunyazov, Tasbolat and Chua, Yansong and Gao, Ruihan and Soh, Harold and Wu, Yan.\u003cbr\u003e\n[\u003ca href=\"https://ruihangao.github.io/files/taunyazov2020fast.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSimulation of the SynTouch BioTac sensor\u003c/strong\u003e, Intelligent Autonomous Systems 15: Proceedings of the 15th International Conference IAS-15, 2019.\u003cbr\u003e\nRuppel, Philipp and Jonetzko, Yannick and Grner, Michael and Hendrich, Norman and Zhang, Jianwei.\u003cbr\u003e\n[\u003ca href=\"https://www.researchgate.net/profile/Yannick-Jonetzko/publication/330014756_Simulation_of_the_SynTouch_BioTac_Sensor_Proceedings_of_the_15th_International_Conference_IAS-15/links/5cc7ed694585156cd7bbc519/Simulation-of-the-SynTouch-BioTac-Sensor-Proceedings-of-the-15th-International-Conference-IAS-15.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRobust learning of tactile force estimation through robot interaction\u003c/strong\u003e, ICRA, 2019.\u003cbr\u003e\nSundaralingam, Balakumar and Lambert, Alexander Sasha and Handa, Ankur and Boots, Byron and Hermans, Tucker and Birchfield, Stan and Ratliff, Nathan and Fox, Dieter.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1810.06187\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eFrom pixels to percepts: Highly robust edge perception and contour following using deep learning and an optical biomimetic tactile sensor\u003c/strong\u003e, IEEE Robotics and Automation Letters, 2019.\u003cbr\u003e\nLepora, Nathan F and Church, Alex and De Kerckhove, Conrad and Hadsell, Raia and Lloyd, John.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1812.02941\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTactile mapping and localization from high-resolution tactile imprints\u003c/strong\u003e, ICRA, 2019.\u003cbr\u003e\nBauza, Maria and Canal, Oleguer and Rodriguez, Alberto.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1904.10944\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eConvolutional autoencoder for feature extraction in tactile sensing\u003c/strong\u003e, IEEE Robotics and Automation Letters, 2019.\u003cbr\u003e\nPolic, Marsela and Krajacic, Ivona and Lepora, Nathan and Orsag, Matko.\u003cbr\u003e\n[\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/8758942/\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLearning to identify object instances by touch: Tactile recognition via multimodal matching\u003c/strong\u003e, ICRA, 2019.\u003cbr\u003e\nLin, Justin and Calandra, Roberto and Levine, Sergey.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1903.03591\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eThe tactip family: Soft optical tactile sensors with 3D-printed biomimetic morphologies\u003c/strong\u003e, Soft Robotics, 2018.\u003cbr\u003e\nWard-Cherrier, Benjamin and Pestell, Nicholas and Cramphorn, Luke and Winstone, Benjamin and Giannaccini, Maria Elena and Rossiter, Jonathan and Lepora, Nathan F.\u003cbr\u003e\n[\u003ca href=\"https://www.liebertpub.com/doi/pdf/10.1089/soro.2017.0052\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003e3D shape perception from monocular vision, touch, and shape priors\u003c/strong\u003e, IROS, 2018.\u003cbr\u003e\nWang, Shaoxiong and Wu, Jiajun and Sun, Xingyuan and Yuan, Wenzhen and Freeman, William T and Tenenbaum, Joshua B and Adelson, Edward H.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2209.13916\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGelSight: High-Resolution Robot Tactile Sensors for Estimating Geometry and Force\u003c/strong\u003e, Sensors, 2017.\u003cbr\u003e\nYuan, Wenzhen and Dong, Siyuan and Adelson, Edward H.\u003cbr\u003e\n[\u003ca href=\"https://www.mdpi.com/1424-8220/17/12/2762/pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eThe feeling of success: Does touch sensing help predict grasp outcomes?\u003c/strong\u003e, arXiv, 2017.\u003cbr\u003e\nCalandra, Roberto and Owens, Andrew and Upadhyaya, Manu and Yuan, Wenzhen and Lin, Justin and Adelson, Edward H and Levine, Sergey.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1710.05512\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eImproved GelSight tactile sensor for measuring geometry and slip\u003c/strong\u003e, IROS, 2017.\u003cbr\u003e\nDong, Siyuan and Yuan, Wenzhen and Adelson, Edward H.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/1708.00922\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGelSight: High-resolution robot tactile sensors for estimating geometry and force\u003c/strong\u003e, Sensors, vol. 17, no. 12, pp. 2762, 2017.\u003cbr\u003e\nYuan, Wenzhen and Dong, Siyuan and Adelson, Edward H.\u003cbr\u003e\n[\u003ca href=\"https://www.mdpi.com/1424-8220/17/12/2762/pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eConnecting look and feel: Associating the visual and tactile properties of physical materials\u003c/strong\u003e, CVPR, 2017.\u003cbr\u003e\nYuan, Wenzhen and Wang, Shaoxiong and Dong, Siyuan and Adelson, Edward.\u003cbr\u003e\n[\u003ca href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Yuan_Connecting_Look_and_CVPR_2017_paper.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eStable reinforcement learning with autoencoders for tactile and visual data\u003c/strong\u003e, IROS, 2016.\u003cbr\u003e\nVan Hoof, Herke and Chen, Nutan and Karl, Maximilian and van der Smagt, Patrick and Peters, Jan.\u003cbr\u003e\n[\u003ca href=\"https://www.academia.edu/download/47652433/Hoof2016.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSensing tactile microvibrations with the BioTacComparison with human sensitivity\u003c/strong\u003e, BioRob, 2012.\u003cbr\u003e\nFishel, Jeremy A and Loeb, Gerald E.\u003cbr\u003e\n[\u003ca href=\"https://www.researchgate.net/profile/Gerald-Loeb/publication/256748883_Sensing_tactile_microvibrations_with_the_BioTac_Comparison_with_human_sensitivity/links/5dbcacae299bf1a47b0a3fa6/Sensing-tactile-microvibrations-with-the-BioTac-Comparison-with-human-sensitivity.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e\u003ca id=\"user-content-interaction\"\u003e Embodied Interaction \u003c/a\u003e\u003ca href=\"#table-of-contents\"\u003e\u003c/a\u003e \u003c/h2\u003e\u003ca id=\"user-content--embodied-interaction--\" class=\"anchor\" aria-label=\"Permalink:  Embodied Interaction \" href=\"#-embodied-interaction--\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eBeyond the Destination: A Novel Benchmark for Exploration-Aware Embodied Question Answering\u003c/strong\u003e, arxiv, 2025\u003cbr\u003e\nKaixuan Jiang, Yang Liu, Weixing Chen, Jingzhou Luo, Ziliang Chen, Ling Pan, Guanbin Li, Liang Lin.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2503.11117\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eCross-Embodiment Dexterous Grasping with Reinforcement Learning\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nHaoqi Yuan, Bohan Zhou, Yuhui Fu, Zongqing Lu.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2410.02479\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nGuanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2403.08321\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMANUS: Markerless Grasp Capture using Articulated 3D Gaussians\u003c/strong\u003e, CVPR, 2024\u003cbr\u003e\nChandradeep Pokhariya, Ishaan Nikhil Shah, Angela Xing, Zekun Li, Kefan Chen, Avinash Sharma, Srinath Sridhar.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Pokhariya_MANUS_Markerless_Grasp_Capture_using_Articulated_3D_Gaussians_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLanguage-driven Grasp Detection\u003c/strong\u003e, CVPR, 2024\u003cbr\u003e\nAn Dinh Vuong, Minh Nhat Vu, Baoru Huang, Nghia Nguyen, Hieu Le, Thieu Vo, Anh Nguyen.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Vuong_Language-driven_Grasp_Detection_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGeneralizing 6-DoF Grasp Detection via Domain Prior Knowledge\u003c/strong\u003e, CVPR, 2024\u003cbr\u003e\nHaoxiang Ma, Modi Shi, Boyang Gao, Di Huang.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_Generalizing_6-DoF_Grasp_Detection_via_Domain_Prior_Knowledge_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDexterous Grasp Transformer\u003c/strong\u003e, CVPR, 2024\u003cbr\u003e\nGuo-Hao Xu, Yi-Lin Wei, Dian Zheng, Xiao-Ming Wu, Wei-Shi Zheng.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Dexterous_Grasp_Transformer_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSingle-View Scene Point Cloud Human Grasp Generation\u003c/strong\u003e, CVPR, 2024\u003cbr\u003e\nYan-Kang Wang, Chengyi Xing, Yi-Lin Wei, Xiao-Ming Wu, Wei-Shi Zheng.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Single-View_Scene_Point_Cloud_Human_Grasp_Generation_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eG-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis\u003c/strong\u003e, CVPR, 2024\u003cbr\u003e\nYufei Ye, Abhinav Gupta, Kris Kitani, Shubham Tulsiani.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Ye_G-HOP_Generative_Hand-Object_Prior_for_Interaction_Reconstruction_and_Grasp_Synthesis_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGrasping Diverse Objects with Simulated Humanoids\u003c/strong\u003e ArXiv, 2024.\u003cbr\u003e\nZhengyi Luo, Jinkun Cao, Sammy Christen, Alexander Winkler, Kris Kitani, Weipeng Xu\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2407.11385\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTask-Oriented Dexterous Grasp Synthesis via Differentiable Grasp Wrench Boundary Estimator\u003c/strong\u003e, IROS, 2024.\u003cbr\u003e\nJiayi Chen,Yuxing Chen,Jialiang Zhang, He Wang\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2309.13586\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eOpen6DOR: Benchmarking Open-instruction 6-DoF Object Rearrangement and A VLM-based Approach\u003c/strong\u003e, IROS, 2024.\u003cbr\u003e\nYufei Ding,Haoran Geng , Chaoyi Xu ,Xiaomeng Fang,Jiazhao Zhang,Songlin Wei, Qiyu Dai, Zhizheng Zhang, He Wang\u003cbr\u003e\n\u003ca href=\"https://pku-epic.github.io/Open6DOR/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eASGrasp: Generalizable Transparent Object Reconstruction and 6-DoF Grasp Detection from RGB-D Active Stereo Camera\u003c/strong\u003e, ICRA, 2024.\u003cbr\u003e\nJun Shi, Yong A, Yixiang Jin, Dingzhe Li, Haoyu Niu, Zhezhu Jin, He Wang\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2405.05648\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eOpenEQA: Embodied Question Answering in the Era of Foundation Models\u003c/strong\u003e, CVPR, 2024\u003cbr\u003e\nMajumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and others\u003cbr\u003e\n\u003ca href=\"http://openaccess.thecvf.com/content/CVPR2024/papers/Majumdar_OpenEQA_Embodied_Question_Answering_in_the_Era_of_Foundation_Models_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eExplore until Confident: Efficient Exploration for Embodied Question Answering\u003c/strong\u003e, ICRA Workshop VLMNM, 2024\u003cbr\u003e\nRen, Allen Z and Clark, Jaden and Dixit, Anushri and Itkina, Masha and Majumdar, Anirudha and Sadigh, Dorsa\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2403.15941\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eS-EQA: Tackling Situational Queries in Embodied Question Answering\u003c/strong\u003e, arXix, 2024\u003cbr\u003e\nDorbala, Vishnu Sashank and Goyal, Prasoon and Piramuthu, Robinson and Johnston, Michael and Manocha, Dinesh and Ghanadhan, Reza\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2405.04732\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMap-based Modular Approach for Zero-shot Embodied Question Answering\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nSakamoto, Koya and Azuma, Daichi and Miyanishi, Taiki and Kurita, Shuhei and Kawanabe, Motoaki\u003cbr\u003e\n\u003ca href=\"https://ui.adsabs.harvard.edu/abs/2024arXiv240516559S/abstract\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEmbodied Question Answering via Multi-LLM Systems\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nBhrij Patel and Vishnu Sashank Dorbala and Amrit Singh Bedi\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2406.10918\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMultiGripperGrasp: A Dataset for Robotic Grasping from Parallel Jaw Grippers to Dexterous Hands\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nMurrilo, Luis Felipe Casas and Khargonkar, Ninad and Prabhakaran, Balakrishnan and Xiang, Yu\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2403.09841\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eReasoning Grasping via Multimodal Large Language Model\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nJin, Shiyu and Xu, Jinxuan and Lei, Yutian and Zhang, Liangjun\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2402.06798\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSemGrasp: Semantic Grasp Generation via Language Aligned Discretization\u003c/strong\u003e, CoRR, 2024\u003cbr\u003e\nLi, Kailin and Wang, Jingbo and Yang, Lixin and Lu, Cewu and Dai, Bo\u003cbr\u003e\n\u003ca href=\"https://openreview.net/forum?id=WUbr8NV1G6\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nZheng, Yuhang and Chen, Xiangyu and Zheng, Yupeng and Gu, Songen and Yang, Runyi and Jin, Bu and Li, Pengfei and Zhong, Chengliang and Wang, Zengmao and Liu, Lina and others\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2403.09637\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eKnowledge-based Embodied Question Answering\u003c/strong\u003e, TPAMI, 2023\u003cbr\u003e\nTan, Sinan and Ge, Mengmeng and Guo, Di and Liu, Huaping and Sun, Fuchun\u003cbr\u003e\n\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/37195849/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDeep Learning Approaches to Grasp Synthesis: A Review\u003c/strong\u003e, IEEE Transactions on Robotics, 2023\u003cbr\u003e\nNewbury, Rhys and Gu, Morris and Chumbley, Lachlan and Mousavian, Arsalan and Eppner, Clemens and Leitner, J{\"u}rgen and Bohg, Jeannette and Morales, Antonio and Asfour, Tamim and Kragic, Danica and others\u003cbr\u003e\n\u003ca href=\"https://dl.acm.org/doi/abs/10.1109/TRO.2023.3280597\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLanguage-guided Robot Grasping: CLIP-based Referring Grasp Synthesis in Clutter\u003c/strong\u003e, CoRL, 2023\u003cbr\u003e\nTziafas, Georgios and Xu, Yucheng and Goel, Arushi and Kasaei, Mohammadreza and Li, Zhibin and Kasaei, Hamidreza\u003cbr\u003e\n\u003ca href=\"https://www.research.ed.ac.uk/en/publications/language-guided-robot-grasping-clip-based-referring-grasp-synthes\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eReasoning Tuning Grasp: Adapting Multi-Modal Large Language Models for Robotic Grasping\u003c/strong\u003e, CoRL, 2023\u003cbr\u003e\nXu, Jinxuan and Jin, Shiyu and Lei, Yutian and Zhang, Yuqian and Zhang, Liangjun\u003cbr\u003e\n\u003ca href=\"https://openreview.net/forum?id=3mKb5iyZ2V\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDistilled Feature Fields Enable Few-Shot Language-Guided Manipulation\u003c/strong\u003e, CoRL, 2023\u003cbr\u003e\nShen, William and Yang, Ge and Yu, Alan and Wong, Jansen and Kaelbling, Leslie Pack and Isola, Phillip\u003cbr\u003e\n\u003ca href=\"https://proceedings.mlr.press/v229/shen23a.html\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eAnyGrasp: Robust and Efficient Grasp Perception in Spatial and Temporal Domains\u003c/strong\u003e, IEEE Transactions on Robotics, 2023\nFang, Hao-Shu and Wang, Chenxi and Fang, Hongjie and Gou, Minghao and Liu, Jirong and Yan, Hengxu and Liu, Wenhai and Xie, Yichen and Lu, Cewu\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/10167687\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDexGraspNet: A Large-Scale Robotic Dexterous Grasp Dataset for General Objects Based on Simulation\u003c/strong\u003e, ICRA, 2023.\u003cbr\u003e\nRuicheng Wang, Jialiang Zhang, Jiayi Chen, Yinzhen Xu, Puhao Li, Tengyu Liu, He Wang\u003cbr\u003e\n\u003ca href=\"https://pku-epic.github.io/DexGraspNet/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eUniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy\u003c/strong\u003e, CVPR, 2023.\u003cbr\u003e\nYinzhen Xu, Weikang Wan, Jialiang Zhang, Haoran Liu, Zikang Shan, Hao Shen, Ruicheng Wang, Haoran Geng, Yijia Weng, Jiayi Chen, Tengyu Liu, Li Yi, He Wang\u003cbr\u003e\n\u003ca href=\"https://pku-epic.github.io/UniDexGrasp/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eUniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning\u003c/strong\u003e, ICCV, 2023.\u003cbr\u003e\nWeikang Wan, Haoran Geng, Yun Liu, Zikang Shan, Yaodong Yang, Li Yi, He Wang\u003cbr\u003e\n\u003ca href=\"https://pku-epic.github.io/UniDexGrasp++/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eCLIPort: What and Where Pathways for Robotic Manipulation\u003c/strong\u003e, CoRL, 2022\u003cbr\u003e\nShridhar, Mohit and Manuelli, Lucas and Fox, Dieter\u003cbr\u003e\n\u003ca href=\"https://proceedings.mlr.press/v164/shridhar22a.html\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eACRONYM: A Large-Scale Grasp Dataset Based on Simulation\u003c/strong\u003e, ICRA, 2021\u003cbr\u003e\nEppner, Clemens and Mousavian, Arsalan and Fox, Dieter\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/9560844\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eHabitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI\u003c/strong\u003e, NeurIPS, 2021\u003cbr\u003e\nRamakrishnan, Santhosh K and Gokaslan, Aaron and Wijmans, Erik and Maksymets, Oleksandr and Clegg, Alex and Turner, John and Undersander, Eric and Galuba, Wojciech and Westbury, Andrew and Chang, Angel X and others\u003cbr\u003e\n\u003ca href=\"https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/34173cb38f07f89ddbebc2ac9128303f-Paper-round2.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEnd-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB\u003c/strong\u003e, ICRA, 2021\u003cbr\u003e\nAinetter, Stefan and Fraundorfer, Friedrich\u003cbr\u003e\n\u003ca href=\"https://elib.dlr.de/146134/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRevisiting EmbodiedQA: A Simple Baseline and Beyond\u003c/strong\u003e, IEEE Transactions on Image Processing, 2020\u003cbr\u003e\nWu, Yu and Jiang, Lu and Yang, Yi\u003cbr\u003e\n\u003ca href=\"https://opus.lib.uts.edu.au/rest/bitstreams/ee2d1faf-ce3b-4f63-a133-4217d19e9db1/retrieve\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMulti-agent Embodied Question Answering in Interactive Environments\u003c/strong\u003e, ECCV, 2020\u003cbr\u003e\nTan, Sinan and Xiang, Weilai and Liu, Huaping and Guo, Di and Sun, Fuchun\u003cbr\u003e\n\u003ca href=\"https://dl.acm.org/doi/abs/10.1007/978-3-030-58601-0_39\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLanguage Models are Few-Shot Learners\u003c/strong\u003e, NIPS, 2020\u003cbr\u003e\nBrown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others\u003cbr\u003e\n\u003ca href=\"https://dl.acm.org/doi/pdf/10.5555/3495724.3495883\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGraspNet-1Billion: A Large-Scale Benchmark for General Object Grasping\u003c/strong\u003e, CVPR, 2020\u003cbr\u003e\nFang, Hao-Shu and Wang, Chenxi and Gou, Minghao and Lu, Cewu\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content_CVPR_2020/html/Fang_GraspNet-1Billion_A_Large-Scale_Benchmark_for_General_Object_Grasping_CVPR_2020_paper.html\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMulti-Target Embodied Question Answering\u003c/strong\u003e, CVPR, 2019\u003cbr\u003e\nYu, Licheng and Chen, Xinlei and Gkioxari, Georgia and Bansal, Mohit and Berg, Tamara L and Batra, Dhruv\u003cbr\u003e\n\u003ca href=\"http://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_Multi-Target_Embodied_Question_Answering_CVPR_2019_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEmbodied Question Answering in Photorealistic Environments with Point Cloud Perception\u003c/strong\u003e, CVPR, 2019\u003cbr\u003e\nWijmans, Erik and Datta, Samyak and Maksymets, Oleksandr and Das, Abhishek and Gkioxari, Georgia and Lee, Stefan and Essa, Irfan and Parikh, Devi and Batra, Dhruv\u003cbr\u003e\n\u003ca href=\"http://openaccess.thecvf.com/content_CVPR_2019/papers/Wijmans_Embodied_Question_Answering_in_Photorealistic_Environments_With_Point_Cloud_Perception_CVPR_2019_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVideoNavQA: Bridging the Gap between Visual and Embodied Question Answering\u003c/strong\u003e, BMVC, 2019\u003cbr\u003e\nCangea, C{\\u{a}}t{\\u{a}}lina and Belilovsky, Eugene and Li{`o}, Pietro and Courville, Aaron\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/1908.04950\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003e6-DOF GraspNet: Variational Grasp Generation for Object Manipulation\u003c/strong\u003e, ICCV, 2019\u003cbr\u003e\nMousavian, Arsalan and Eppner, Clemens and Fox, Dieter\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content_ICCV_2019/html/Mousavian_6-DOF_GraspNet_Variational_Grasp_Generation_for_Object_Manipulation_ICCV_2019_paper.html\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEmbodied Question Answering\u003c/strong\u003e, CVPR, 2018\u003cbr\u003e\nDas, Abhishek and Datta, Samyak and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content_cvpr_2018/papers/Das_Embodied_Question_Answering_CVPR_2018_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eIQA: Visual Question Answering in Interactive Environments\u003c/strong\u003e, CVPR, 2018\u003cbr\u003e\nGordon, Daniel and Kembhavi, Aniruddha and Rastegari, Mohammad and Redmon, Joseph and Fox, Dieter and Farhadi, Ali\n\u003ca href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Gordon_IQA_Visual_Question_CVPR_2018_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eBuilding Generalizable Agents with a Realistic and Rich 3D Environment\u003c/strong\u003e, ECCV, 2018\u003cbr\u003e\nWu, Yi and Wu, Yuxin and Gkioxari, Georgia and Tian, Yuandong\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/1801.02209\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMINOS: Multimodal Indoor Simulator for Navigation in Complex Environments\u003c/strong\u003e, ECCV, 2018\u003cbr\u003e\nSavva, Manolis and Chang, Angel X and Dosovitskiy, Alexey and Funkhouser, Thomas and Koltun, Vladlen\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/1712.03931\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eNeural Modular Control for Embodied Question Answering\u003c/strong\u003e, ECCV, 2018\u003cbr\u003e\nDas, Abhishek and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv\u003cbr\u003e\n\u003ca href=\"https://authors.library.caltech.edu/records/ykvm4-2ed40/files/1810.11181.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eJacquard: A Large Scale Dataset for Robotic Grasp Detection\u003c/strong\u003e, IROS, 2018\u003cbr\u003e\nDepierre, Amaury and Dellandr{'e}a, Emmanuel and Chen, Liming\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/8593950\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMatterport3D: Learning from rgb-d data in indoor environments,\u003c/strong\u003e, IEEE International Conference on 3D Vision, 2017\u003cbr\u003e\nChang, Angel and Dai, Angela and Funkhouser, Thomas and Halber, Maciej and Niessner, Matthias and Savva, Manolis and Song, Shuran and Zeng, Andy and Zhang, Yinda\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/1709.06158\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes\u003c/strong\u003e, CVPR, 2017\u003cbr\u003e\nDai, Angela and Chang, Angel X and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\\ss}ner, Matthias\n\u003ca href=\"https://www.computer.org/csdl/proceedings-article/cvpr/2017/0457c432/12OmNyRg4C5\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eShape Completion Enabled Robotic Grasping\u003c/strong\u003e, IROS, 2017\u003cbr\u003e\nVarley, Jacob and DeChant, Chad and Richardson, Adam and Ruales, Joaqu{'\\i}n and Allen, Peter\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/8206060\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEfficient grasping from RGBD images: Learning using a new rectangle representation\u003c/strong\u003e, IEEE International Conference on Robotics and Automation, 2011\u003cbr\u003e\nJiang, Yun and Moseson, Stephen and Saxena, Ashutosh\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/5980145\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eA frontier-based approach for autonomous exploration\u003c/strong\u003e, CIRA, 1997\u003cbr\u003e\nYamauchi, Brian\u003cbr\u003e\n\u003ca href=\"https://dl.acm.org/doi/abs/10.5555/523996.793157\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e\u003ca id=\"user-content-agent\"\u003e Embodied Agent \u003c/a\u003e\u003ca href=\"#table-of-contents\"\u003e\u003c/a\u003e \u003c/h2\u003e\u003ca id=\"user-content--embodied-agent--\" class=\"anchor\" aria-label=\"Permalink:  Embodied Agent \" href=\"#-embodied-agent--\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eEmbodied Multimodal Foundation Models and VLA Methods\u003c/h3\u003e\u003ca id=\"user-content-embodied-multimodal-foundation-models-and-vla-methods\" class=\"anchor\" aria-label=\"Permalink: Embodied Multimodal Foundation Models and VLA Methods\" href=\"#embodied-multimodal-foundation-models-and-vla-methods\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEmbodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks\u003c/strong\u003e, arXiv, 2025.\u003cbr\u003e\nWenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, Weiming Lu, Peng Li, Yueting Zhuang\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/abs/2503.21696\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World\u003c/strong\u003e, arXiv, 2024.\u003cbr\u003e\nWeixin Mao, Weiheng Zhong, Zhou Jiang, Dong Fang, Zhongyue Zhang, Zihan Lan, Fan Jia, Tiancai Wang, Haoqiang Fan, Osamu Yoshie.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2412.00171\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSpatially Visual Perception for End-to-End Robotic Learning\u003c/strong\u003e, arXiv, 2024.\u003cbr\u003e\nTravis Davies, Jiahuan Yan, Xiang Chen, Yu Tian, Yueting Zhuang, Yiqi Huang, Luhui Hu.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2411.17458\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation\u003c/strong\u003e, arXiv, 2024.\u003cbr\u003e\nChi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, Minzhao Zhu.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2410.06158\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eScaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers\u003c/strong\u003e, arXiv, 2024.\u003cbr\u003e\nLirui Wang, Xinlei Chen, Jialiang Zhao, Kaiming He.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2409.20537\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSpatial Reasoning and Planning for Deep Embodied Agents\u003c/strong\u003e, arXiv, 2024.\u003cbr\u003e\nShu Ishida.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2409.19479\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGrounding Large Language Models In Embodied Environment With Imperfect World Models\u003c/strong\u003e, arXiv, 2024.\u003cbr\u003e\nHaolan Liu, Jishen Zhao.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2410.02742\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSELU: Self-Learning Embodied MLLMs in Unknown Environments\u003c/strong\u003e, arXiv, 2024.\u003cbr\u003e\nBoyu Li, Haobin Jiang, Ziluo Ding, Xinrun Xu, Haoran Li, Dongbin Zhao, Zongqing Lu.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2410.03303\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eAutort: Embodied foundation models for large scale orchestration of robotic agents\u003c/strong\u003e, arXiv, 2024.\u003cbr\u003e\nAhn, Michael, Debidatta, Dwibedi, Chelsea, Finn, Montse Gonzalez, Arenas, Keerthana, Gopalakrishnan, Karol, Hausman, Brian, Ichter, Alex, Irpan, Nikhil, Joshi, Ryan, Julian, others.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2401.12963\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDiffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learningn\u003c/strong\u003e, arXiv, 2024.\u003cbr\u003e\nNorman Di Palo, Leonard Hasenclever, Jan Humplik, Arunkumar Byravan.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2407.20798\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRt-h: Action hierarchies using language\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nBelkhale, Suneel, Tianli, Ding, Ted, Xiao, Pierre, Sermanet, Quon, Vuong, Jonathan, Tompson, Yevgen, Chebotar, Debidatta, Dwibedi, Dorsa, Sadigh.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2403.01823\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDo as i can, not as i say: Grounding language in robotic affordances\u003c/strong\u003e, Conference on robot learning. 2023.\u003cbr\u003e\nBrohan, Anthony, Yevgen, Chebotar, Chelsea, Finn, Karol, Hausman, Alexander, Herzog, Daniel, Ho, Julian, Ibarz, Alex, Irpan, Eric, Jang, Ryan, Julian.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2204.01691\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEmbodiedgpt: Vision-language pre-training via embodied chain of thought\u003c/strong\u003e, NeurIPS, 2024.\u003cbr\u003e\nMu, Yao, Qinglong, Zhang, Mengkang, Hu, Wenhai, Wang, Mingyu, Ding, Jun, Jin, Bin, Wang, Jifeng, Dai, Yu, Qiao, Ping, Luo.\u003cbr\u003e\n[\u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2023/file/4ec43957eda1126ad4887995d05fae3b-Paper-Conference.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eQ-transformer: Scalable offline reinforcement learning via autoregressive q-functions\u003c/strong\u003e, Conference on Robot Learning. 2023.\u003cbr\u003e\nChebotar, Yevgen, Quan, Vuong, Karol, Hausman, Fei, Xia, Yao, Lu, Alex, Irpan, Aviral, Kumar, Tianhe, Yu, Alexander, Herzog, Karl, Pertsch, others.\u003cbr\u003e\n[\u003ca href=\"https://proceedings.mlr.press/v229/chebotar23a/chebotar23a.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSara-rt: Scaling up robotics transformers with self-adaptive robust attention\u003c/strong\u003e, arXiv, 2023.\u003cbr\u003e\nLeal, Isabel, Krzysztof, Choromanski, Deepali, Jain, Avinava, Dubey, Jake, Varley, Michael, Ryoo, Yao, Lu, Frederick, Liu, Vikas, Sindhwani, Quan, Vuong, others.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2312.01990\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePalm-e: An embodied multimodal language model\u003c/strong\u003e, ArXiv, 2023.\u003cbr\u003e\nDriess, Danny, Fei, Xia, Mehdi SM, Sajjadi, Corey, Lynch, Aakanksha, Chowdhery, Brian, Ichter, Ayzaan, Wahid, Jonathan, Tompson, Quan, Vuong, Tianhe, Yu, others.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2303.03378\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRt-2: Vision-language-action models transfer web knowledge to robotic control\u003c/strong\u003e, Conference on Robot Learning. 2023.\u003cbr\u003e\nZitkovich, Brianna, Tianhe, Yu, Sichun, Xu, Peng, Xu, Ted, Xiao, Fei, Xia, Jialin, Wu, Paul, Wohlhart, Stefan, Welker, Ayzaan, Wahid, others.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2307.15818\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eOpen x-embodiment: Robotic learning datasets and rt-x models\u003c/strong\u003e, arXiv, 2023.\u003cbr\u003e\nPadalkar, others.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2310.08864\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVision-language foundation models as effective robot imitators\u003c/strong\u003e, arXiv, 2023.\u003cbr\u003e\nLi, Xinghang, Minghuan, Liu, Hanbo, Zhang, Cunjun, Yu, Jie, Xu, Hongtao, Wu, Chilam, Cheang, Ya, Jing, Weinan, Zhang, Huaping, Liu, others.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2311.01378\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRt-1: Robotics transformer for real-world control at scale\u003c/strong\u003e, ArXiv, 2022.\u003cbr\u003e\nBrohan, Anthony, Noah, Brown, Justice, Carbajal, Yevgen, Chebotar, Joseph, Dabis, Chelsea, Finn, Keerthana, Gopalakrishnan, Karol, Hausman, Alex, Herzog, Jasmine, Hsu, others.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2212.06817\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eEmbodied Manipulation \u0026amp; Control\u003c/h3\u003e\u003ca id=\"user-content-embodied-manipulation--control\" class=\"anchor\" aria-label=\"Permalink: Embodied Manipulation \u0026amp; Control\" href=\"#embodied-manipulation--control\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePEAC: Unsupervised Pre-training for Cross-Embodiment Reinforcement Learning\u003c/strong\u003e, NeurIPS, 2024.\u003cbr\u003e\nChengyang Ying, Zhongkai Hao, Xinning Zhou, Xuezhou Xu, Hang Su, Xingxing Zhang, Jun Zhu.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2405.14073\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eFourier Controller Networks for Real-Time Decision-Making in Embodied Learning\u003c/strong\u003e, ICML, 2024.\u003cbr\u003e\nHengkai Tan, Songming Liu, Kai Ma, Chengyang Ying, Xingxing Zhang, Hang Su, Jun Zhu.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2405.19885\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRDT-1B: a Diffusion Foundation Model for Bimanual Manipulation\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nSongming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, Jun Zhu.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2410.07864\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eManiBox: Enhancing Spatial Grasping Generalization via Scalable Simulation Data Generation\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nHengkai Tan, Xuezhou Xu, Chengyang Ying, Xinyi Mao, Songming Liu, Xingxing Zhang, Hang Su, Jun Zhu.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2411.01850\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRoboGSim: A Real2Sim2Real Robotic Gaussian Splatting Simulator\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nXinhai Li, Jialin Li, Ziheng Zhang, Rui Zhang, Fan Jia, Tiancai Wang, Haoqiang Fan, Kuo-Kun Tseng, Ruiping Wang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2411.11839\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for Long-Horizon Manipulation\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nZihan Zhou, Animesh Garg, Dieter Fox, Caelan Garrett, Ajay Mandlekar.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2410.18065\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDiffusion Transformer Policy\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nZhi Hou, Tianyi Zhang, Yuwen Xiong, Hengjun Pu, Chengyang Zhao, Ronglei Tong, Yu Qiao, Jifeng Dai, Yuntao Chen.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2410.15959\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDexcap: Scalable and portable mocap data collection system for dexterous manipulation\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nChen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, C Karen Liu.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2403.07788\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLota-bench: Benchmarking language-oriented task planners for embodied agents\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nChoi, Jae-Woo, Youngwoo, Yoon, Hyobin, Ong, Jaehong, Kim, Minsu, Jang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2402.08178\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSocratic Planner: Inquiry-Based Zero-Shot Planning for Embodied Instruction Following\u003c/strong\u003e, Arxiv, 2024.\u003cbr\u003e\nSuyeon Shin, Sujin jeon, Junghyun Kim, Gi-Cheon Kang, Byoung-Tak Zhang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2404.15190\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLarge Language Models as Commonsense Knowledge for Large-Scale Task Planning\u003c/strong\u003e, NeurIPS, 2024.\u003cbr\u003e\nZhao, Zirui, Wee Sun, Lee, David, Hsu.\u003cbr\u003e\n[\u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2023/file/65a39213d7d0e1eb5d192aa77e77eeb7-Paper-Conference.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGeneralized Planning in PDDL Domains with Pretrained Large Language Models\u003c/strong\u003e, AAAI, 2024.\u003cbr\u003e\nSilver, Tom, Soham, Dan, Kavitha, Srinivas, Joshua B., Tenenbaum, Leslie Pack, Kaelbling, Michael, Katz.\u003cbr\u003e\n[\u003ca href=\"https://ojs.aaai.org/index.php/AAAI/article/download/30006/31766\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTowards Efficient LLM Grounding for Embodied Multi-Agent Collaboration\u003c/strong\u003e arXiv, 2024.\u003cbr\u003e\nZhang, Yang, Shixin, Yang, Chenjia, Bai, Fei, Wu, Xiu, Li, Xuelong, Li, Zhen, Wang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2405.14314\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEmbodied Instruction Following in Unknown Environments\u003c/strong\u003e, arXiv, 2024.\u003cbr\u003e\nZhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2406.11818\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eA Backbone for Long-Horizon Robot Task Understanding\u003c/strong\u003e, arxiv, 2024.\u003cbr\u003e\nXiaoshuai Chen, Wei Chen, Dongmyoung Lee, Yukun Ge, Nicolas Rojas, and Petar Kormushev.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2408.01334\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRoboMamba: Multimodal State Space Model for Efficient Robot Reasoning and Manipulation\u003c/strong\u003e, arXiv, 2024.\u003cbr\u003e\nLiu, Jiaming, Mengzhen, Liu, Zhenyu, Wang, Lily, Lee, Kaichen, Zhou, Pengju, An, Senqiao, Yang, Renrui, Zhang, Yandong, Guo, Shanghang, Zhang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2406.04339\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePlay to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic Manipulation\u003c/strong\u003e, arxiv, 2024.\u003cbr\u003e\nRuoxuan Feng, Di Hu1, Wenke Ma, Xuelong Li.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2408.01366\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEgocentric Vision Language Planning\u003c/strong\u003e, arxiv, 2024.\u003cbr\u003e\nZhirui Fang, Ming Yang, Weishuai Zeng, Boyu Li, Junpeng Yue, Ziluo Ding, Xiu Li, Zongqing Lu.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2408.05802\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePolaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual Grounding and Large Language Models\u003c/strong\u003e, IROS, 2024.\u003cbr\u003e\nTianyu Wang, Haitao Lin, Junqiu Yu, Yanwei Fu.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2408.07975\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLLM-SAP: Large Language Model Situational Awareness Based Planning\u003c/strong\u003e, ICME 2024 Workshop MML4SG.\u003cbr\u003e\nLiman Wang, Hanyang Zhong.\u003cbr\u003e\n\u003ca href=\"https://github.com/HanyangZhong/Situational_Planning_datasets\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eFMB: a Functional Manipulation Benchmark for Generalizable Robotic Learning\u003c/strong\u003e, ArXiv, 2024.\u003cbr\u003e\nJianlan Luo, Charles Xu, Fangchen Liu, Liam Tan, Zipeng Lin, Jeffrey Wu, Pieter Abbeel, and Sergey Levine.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2401.08553\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models\u003c/strong\u003e IROS, 2024.\u003cbr\u003e\nSiyuan Huang, Iaroslav Ponomarenko, Zhengkai Jiang, Xiaoqi Li, Xiaobin Hu, Peng Gao, Hongsheng Li, and Hao Dong.\u003cbr\u003e\n\u003ca href=\"https://github.com/SiyuanHuang95/ManipVQA\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eA3VLM: Actionable Articulation-Aware Vision Language Model\u003c/strong\u003e ArXiv, 2024.\u003cbr\u003e\nSiyuan Huang, Haonan Chang, Yuhan Liu, Yimeng Zhu, Hao Dong, Peng Gao, Abdeslam Boularias, and Hongsheng Li.\u003cbr\u003e\n\u003ca href=\"https://github.com/changhaonan/A3VLM\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEmbodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nYijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li, Li Shen, Xiaodong He, Jing Jiang, Yuhui Shi.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Embodied_Multi-Modal_Agent_trained_by_an_LLM_from_a_Parallel_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRetrieval-Augmented Embodied Agents\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nYichen Zhu, Zhicai Ou, Xiaofeng Mou, Jian Tang.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_Retrieval-Augmented_Embodied_Agents_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMulti-agent Collaborative Perception via Motion-aware Robust Communication Network\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nShixin Hong, Yu Liu, Zhi Li, Shaohui Li, You He.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Hong_Multi-agent_Collaborative_Perception_via_Motion-aware_Robust_Communication_Network_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models\u003c/strong\u003e, ICCV, 2023.\u003cbr\u003e\nChan Hee Song, Jiaman Wu, Clay Washington, Brian M. Sadler, Wei-Lun Chao, Yu Su.\u003cbr\u003e\n[[page](LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models)]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eOpen-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models\u003c/strong\u003e EMNLP, 2023.\u003cbr\u003e\nSarch, Gabriel, Yue, Wu, Michael J., Tarr, Katerina, Fragkiadaki.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2310.15127\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVoyager: An Open-Ended Embodied Agent with Large Language Models\u003c/strong\u003e, TMLR, 2023.\u003cbr\u003e\nWang, Guanzhi, Yuqi, Xie, Yunfan, Jiang, Ajay, Mandlekar, Chaowei, Xiao, Yuke, Zhu, Linxi, Fan, Anima, Anandkumar.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2305.16291\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eReAct: Synergizing Reasoning and Acting in Language Models\u003c/strong\u003e, ICLR, 2023.\u003cbr\u003e\nYao, Shunyu, Jeffrey, Zhao, Dian, Yu, Nan, Du, Izhak, Shafran, Karthik, Narasimhan, Yuan, Cao.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2210.03629\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eProgPrompt: Generating Situated Robot Task Plans Using Large Language Models\u003c/strong\u003e , ICRA, 2023.\u003cbr\u003e\nSingh, Ishika, Valts, Blukis, Arsalan, Mousavian, Ankit, Goyal, Danfei, Xu, Jonathan, Tremblay, Dieter, Fox, Jesse, Thomason, Animesh, Garg.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2209.11302\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eChatGPT for Robotics: Design Principles and Model Abilities\u003c/strong\u003e, IEEE Access 12. (2023): 55682-55696.\u003cbr\u003e\nSai Vemprala, Rogerio Bonatti, Arthur Fender C. Bucker, Ashish Kapoor.\u003cbr\u003e\n[\u003ca href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10500490\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eCode as Policies: Language Model Programs for Embodied Control\u003c/strong\u003e, ICRA, 2023.\u003cbr\u003e\nJacky Liang, , Wenlong Huang, F. Xia, Peng Xu, Karol Hausman, Brian Ichter, Peter R. Florence, Andy Zeng.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2209.07753\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eReasoning with Language Model Is Planning with World Model\u003c/strong\u003e, Arxiv, 2023.\u003cbr\u003e\nHao, Shibo, Yi, Gu, Haodi, Ma, Joshua Jiahua, Hong, Zhen, Wang, Daisy Zhe, Wang, Zhiting, Hu.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2305.14992\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement\u003c/strong\u003e, arXiv, 2023.\u003cbr\u003e\nHaonan Chang, Kai Gao, Kowndinya Boyalakuntla, Alex Lee, Baichuan Huang, Harish Udhaya Kumar, Jinjin Yu, Abdeslam Boularias.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2309.15821\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTranslating Natural Language to Planning Goals with Large-Language Models\u003c/strong\u003e, arXiv, 2023.\u003cbr\u003e\nXie, Yaqi, Chen, Yu, Tongyao, Zhu, Jinbin, Bai, Ze, Gong, Harold, Soh.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2302.05128\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLLM+P: Empowering Large Language Models with Optimal Planning Proficiency\u003c/strong\u003e, arXiv, 2023.\u003cbr\u003e\nLiu, Bo, Yuqian, Jiang, Xiaohan, Zhang, Qiang, Liu, Shiqi, Zhang, Joydeep, Biswas, Peter, Stone.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2304.11477\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDynamic Planning with a LLM\u003c/strong\u003e, arXiv, 2023.\u003cbr\u003e\nDagan, Gautier, Frank, Keller, Alex, Lascarides.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2308.06391\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEmbodied Task Planning with Large Language Models\u003c/strong\u003e, arXiv, 2023.\u003cbr\u003e\nWu, Zhenyu, Ziwei, Wang, Xiuwei, Xu, Jiwen, Lu, Haibin, Yan.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2307.01848\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning\u003c/strong\u003e, Conference on Robot Learning. 2023.\u003cbr\u003e\nKrishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian D. Reid, Niko Sunderhauf.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2307.06135\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning\u003c/strong\u003e, ArXiv, 2023.\u003cbr\u003e\nQiao Gu, Ali Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Ramalingam Chellappa, Chuang Gan, Celso Miguel de Melo, Joshua B Tenenbaum, Antonio Torralba, Florian Shkurti, Liam Paull.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2309.16650\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks\u003c/strong\u003e, arXiv, 2023.\u003cbr\u003e\nYaran Chen, Wenbo Cui, Yuanwen Chen, Mining Tan, Xinyao Zhang, Dong Zhao, He Wang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2311.15649\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eChat with the Environment: Interactive Multimodal Perception Using Large Language Models\u003c/strong\u003e, IROS, 2023.\u003cbr\u003e\nZhao, Xufeng, Mengdi, Li, Cornelius, Weber, Muhammad Burhan, Hafez, Stefan, Wermter.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2303.08268\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVideo Language Planning\u003c/strong\u003e, arxiv, 2023.\u003cbr\u003e\nDu, Yilun, Mengjiao, Yang, Pete, Florence, Fei, Xia, Ayzaan, Wahid, Brian, Ichter, Pierre, Sermanet, Tianhe, Yu, Pieter, Abbeel, Joshua B., Tenenbaum, Leslie, Kaelbling, Andy, Zeng, Jonathan, Tompson.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2310.10625\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eCode as Policies: Language Model Programs for Embodied Control\u003c/strong\u003e, ICRA, 2023,\u003cbr\u003e\nJacky Liang, Wenlong Huang, F. Xia, Peng Xu, Karol Hausman, Brian Ichter, Peter R. Florence, Andy Zeng.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2209.07753\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eReflexion: an autonomous agent with dynamic memory and self-reflection\u003c/strong\u003e, ArXiv, 2023.\u003cbr\u003e\nNoah Shinn, Beck Labash, A. Gopinath.\u003cbr\u003e\n[\u003ca href=\"/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDescribe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents\u003c/strong\u003e, Proceedings of the 37th International Conference on Neural Information Processing Systems, 2023.\u003cbr\u003e\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2302.01560\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eInstruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model\u003c/strong\u003e, ArXiv, 2023.\u003cbr\u003e\nSiyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, and Hongsheng Li.\u003cbr\u003e\n\u003ca href=\"https://github.com/OpenGVLab/Instruct2Act\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eCliport: What and where pathways for robotic manipulation\u003c/strong\u003e, Conference on robot learning, 2022.\u003cbr\u003e\nShridhar, Mohit, Lucas, Manuelli, Dieter, Fox.\u003cbr\u003e\n[\u003ca href=\"https://proceedings.mlr.press/v164/shridhar22a/shridhar22a.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLanguage models as zero-shot planners: Extracting actionable knowledge for embodied agents\u003c/strong\u003e, ICML, 2022.\u003cbr\u003e\nHuang, Wenlong, Pieter, Abbeel, Deepak, Pathak, Igor, Mordatch.\u003cbr\u003e\n[\u003ca href=\"https://proceedings.mlr.press/v162/huang22a/huang22a.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eInner Monologue: Embodied Reasoning through Planning with Language Models\u003c/strong\u003e, Conference on Robot Learning, 2022.\u003cbr\u003e\nHuang, Wenlong, Fei, Xia, Ted, Xiao, Harris, Chan, Jacky, Liang, Pete, Florence, Andy, Zeng, Jonathan, Tompson, Igor, Mordatch, Yevgen, Chebotar, Pierre, Sermanet, Noah, Brown, Tomas, Jackson, Linda, Luu, Sergey, Levine, Karol, Hausman, Brian, Ichter.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2207.05608\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLanguage Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\u003c/strong\u003e, ICML, 2022.\u003cbr\u003e\nHuang, Wenlong, Pieter, Abbeel, Deepak, Pathak, Igor, Mordatch.\u003cbr\u003e\n[\u003ca href=\"https://proceedings.mlr.press/v162/huang22a/huang22a.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSocratic Models: Composing Zero-Shot Multimodal Reasoning with Language\u003c/strong\u003e, ICLR, 2022.\u003cbr\u003e\nZeng, Andy, Maria, Attarian, Brian, Ichter, Krzysztof, Choromanski, Adrian, Wong, Stefan, Welker, Federico, Tombari, Aveek, Purohit, Michael, Ryoo, Vikas, Sindhwani, Johnny, Lee, Vincent, Vanhoucke, Pete, Florence.\u003cbr\u003e\n[[page](Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language)]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSkill Induction and Planning with Latent Language\u003c/strong\u003e, ACL, 2021.\u003cbr\u003e\nPratyusha Sharma, Antonio Torralba, Jacob Andreas.\u003cbr\u003e\n[\u003ca href=\"https://arxiv.org/pdf/2110.01517\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePDDL-the planning domain definition language\u003c/strong\u003e, Technical Report. 1998.\u003cbr\u003e\nDrew McDermott, Malik Ghallab, Adele E. Howe, Craig A. Knoblock, Ashwin Ram, Manuela M. Veloso, Daniel S. Weld, David E. Wilkins.\u003cbr\u003e\n[\u003ca href=\"https://www.researchgate.net/profile/Craig-Knoblock/publication/2278933_PDDL_-_The_Planning_Domain_Definition_Language/links/0912f50c0c99385e19000000/PDDL-The-Planning-Domain-Definition-Language.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eStrips: A new approach to the application of theorem proving to problem solving\u003c/strong\u003e, Artificial Intelligence 2. 3(1971): 189-208.\u003cbr\u003e\nRichard E. Fikes, Nils J. Nilsson.\u003cbr\u003e\n[\u003ca href=\"https://ntrs.nasa.gov/api/citations/19730013831/downloads/19730013831.pdf#page=98\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eA Formal Basis for the Heuristic Determination of Minimum Cost Paths\u003c/strong\u003e, IEEE Trans. Syst. Sci. Cybern. 4. (1968): 100-107.\u003cbr\u003e\nPeter E. Hart, Nils J. Nilsson, Bertram Raphael.\u003cbr\u003e\n[\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/4082128\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eThe Monte Carlo method\u003c/strong\u003e, Journal of the American Statistical Association 44 247. (1949): 335-41.\u003cbr\u003e\nNicholas C. Metropolis, S. M. Ulam.\u003cbr\u003e\n[\u003ca href=\"https://web.williams.edu/Mathematics/sjmiller/public_html/341Fa09/handouts/MetropolisUlam_TheMonteCarloMethod.pdf\" rel=\"nofollow\"\u003epage\u003c/a\u003e]\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e\u003ca id=\"user-content-sim-to-real\"\u003e Sim-to-Real Adaptation \u003c/a\u003e\u003ca href=\"#table-of-contents\"\u003e\u003c/a\u003e \u003c/h2\u003e\u003ca id=\"user-content--sim-to-real-adaptation--\" class=\"anchor\" aria-label=\"Permalink:  Sim-to-Real Adaptation \" href=\"#-sim-to-real-adaptation--\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nShiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, Xipeng Qiu\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2412.18194\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePIVOT-R: Primitive-Driven Waypoint-Aware World Model for Robotic Manipulation\u003c/strong\u003e, NeurIPS, 2024\u003cbr\u003e\nKaidong Zhang, Pengzhen Ren, Bingqian Lin, Junfan Lin, Shikui Ma, Hang Xu, Xiaodan Liang\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2410.10394\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eData Scaling Laws in Imitation Learning for Robotic Manipulation\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nFanqi Lin, Yingdong Hu, Pingyue Sheng, Chuan Wen, Jiacheng You, Yang Gao\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2410.18647\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eEvaluating Real-World Robot Manipulation Policies in Simulation\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nXuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, Ted Xiao\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2405.05941\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eBody Transformer: Leveraging Robot Embodiment for Policy Learning\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nCarmelo Sferrazza, Dun-Ming Huang, Fangchen Liu, Jongmin Lee, Pieter Abbeel\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2408.06316\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eAutonomous Behavior Planning For Humanoid Loco-manipulation Through Grounded Language Model\u003c/strong\u003e, arxiv, 2024\u003cbr\u003e\nJin Wang, Arturo Laurenzi, Nikos Tsagarakis\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2408.082827\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eRobust agents learn causal world models\u003c/strong\u003e, ICLR, 2024\u003cbr\u003e\nRichens, Jonathan, and Tom Everitt\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2402.10877\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eUniversal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots\u003c/strong\u003e, arXiv 2024\u003cbr\u003e\nChi, Cheng and Xu, Zhenjia and Pan, Chuer and Cousineau, Eric and Burchfiel, Benjamin and Feng, Siyuan and Tedrake, Russ and Song, Shuran\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2402.10329\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nFu, Zipeng and Zhao, Tony Z and Finn, Chelsea\u003cbr\u003e\n\u003ca href=\"https://mobile-aloha.github.io/resources/mobile-aloha.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eHuman-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nLuo, Shengcheng and Peng, Quanquan and Lv, Jun and Hong, Kaiwen and Driggs-Campbell, Katherine Rose and Lu, Cewu and Li, Yong-Lu\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2407.00299\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eReconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nTorne, Marcel and Simeonov, Anthony and Li, Zechu and Chan, April and Chen, Tao and Gupta, Abhishek and Agrawal, Pulkit\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2403.03949\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nJiang, Yunfan and Wang, Chen and Zhang, Ruohan and Wu, Jiajun and Fei-Fei, Li\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2405.10315\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eNatural Language Can Help Bridge the Sim2Real Gap\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nYu, Albert and Foote, Adeline and Mooney, Raymond and Mart{'\\i}n-Mart{'\\i}n, Roberto\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2405.10020\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eVisual Whole-Body Control for Legged Loco-Manipulation\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nLiu, Minghuan and Chen, Zixuan and Cheng, Xuxin and Ji, Yandong and Yang, Ruihan and Wang, Xiaolong\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2403.16967\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eExpressive Whole-Body Control for Humanoid Robots\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nCheng, Xuxin and Ji, Yandong and Chen, Junming and Yang, Ruihan and Yang, Ge and Wang, Xiaolong\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2402.16796\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePandora: Towards General World Model with Natural Language Actions and Video States\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nXiang, Jiannan and Liu, Guangyi and Gu, Yi and Gao, Qiyue and Ning, Yuting and Zha, Yuheng and Feng, Zeyu and Tao, Tianhua and Hao, Shibo and Shi, Yemin and others\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2406.09455\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003e3D-VLA: A 3D Vision-Language-Action Generative World Model\u003c/strong\u003e, ICML, 2024\u003cbr\u003e\nZhen, Haoyu and Qiu, Xiaowen and Chen, Peihao and Yang, Jincheng and Yan, Xin and Du, Yilun and Hong, Yining and Gan, Chuang\u003cbr\u003e\n\u003ca href=\"https://openreview.net/forum?id=EZcFK8HupF\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDiffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nDing, Zihan and Zhang, Amy and Tian, Yuandong and Zheng, Qinqing\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2402.03570\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features\u003c/strong\u003e, ICLR, 2024\u003cbr\u003e\nBardes, Adrien and Ponce, Jean and LeCun, Yann\u003cbr\u003e\n\u003ca href=\"https://openreview.net/forum?id=9XdLlbxZCC\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLearning and Leveraging World Models in Visual Representation Learning\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nGarrido, Quentin and Assran, Mahmoud and Ballas, Nicolas and Bardes, Adrien and Najman, Laurent and LeCun, Yann\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2403.00504\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eiVideoGPT: Interactive VideoGPTs are Scalable World Models\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nWu, Jialong and Yin, Shaofeng and Feng, Ningya and He, Xu and Li, Dong and Hao, Jianye and Long, Mingsheng\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2405.15223\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSpatiotemporal Predictive Pre-training for Robotic Motor Control\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nYang, Jiange and Liu, Bei and Fu, Jianlong and Pan, Bocheng and Wu, Gangshan and Wang, Limin\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2403.05304\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLEGENT: Open Platform for Embodied Agents\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nCheng, Zhili and Wang, Zhitong and Hu, Jinyi and Hu, Shengding and Liu, An and Tu, Yuge and Li, Pengkai and Shi, Lei and Liu, Zhiyuan and Sun, Maosong\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2404.18243\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePoint-JEPA: A Joint Embedding Predictive Architecture for Self-Supervised Learning on Point Cloud\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nSaito, Ayumu and Poovvancheri, Jiju\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2404.16432\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMuDreamer: Learning Predictive World Models without Reconstruction\u003c/strong\u003e, ICLR, 2024\u003cbr\u003e\nBurchi, Maxime and Timofte, Radu\u003cbr\u003e\n\u003ca href=\"https://openreview.net/forum?id=9pe38WpsbX\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eFrom Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nWong, Lionel and Grand, Gabriel and Lew, Alexander K and Goodman, Noah D and Mansinghka, Vikash K and Andreas, Jacob and Tenenbaum, Joshua B\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2306.12672\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eElastoGen: 4D Generative Elastodynamics\u003c/strong\u003e, arXiv, 2024\u003cbr\u003e\nFeng, Yutao and Shang, Yintong and Feng, Xiang and Lan, Lei and Zhe, Shandian and Shao, Tianjia and Wu, Hongzhi and Zhou, Kun and Su, Hao and Jiang, Chenfanfu and others\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2405.15056\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLifelike agility and play in quadrupedal robots using reinforcement learning and generative pre-trained models\u003c/strong\u003e, Nature Machine Intelligence, 2024.\u003cbr\u003e\nLei Han, Qingxu Zhu, Jiapeng Sheng, Chong Zhang, Tingguang Li, Yizheng Zhang, He Zhang et al.\u003cbr\u003e\n\u003ca href=\"https://www.nature.com/articles/s42256-024-00861-3\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eModel Adaptation for Time Constrained Embodied Control\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nJaehyun Song, Minjong Yoo, Honguk Woo.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Song_Model_Adaptation_for_Time_Constrained_Embodied_Control_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nXiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, Hao Dong.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Li_ManipLLM_Embodied_Multimodal_Large_Language_Model_for_Object-Centric_Robotic_Manipulation_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nXiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, Hao Dong.\u003cbr\u003e\n\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024/papers/Li_ManipLLM_Embodied_Multimodal_Large_Language_Model_for_Object-Centric_Robotic_Manipulation_CVPR_2024_paper.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGenH2R: Learning Generalizable Human-to-Robot Handover via Scalable Simulation, Demonstration, and Imitation\u003c/strong\u003e, CVPR, 2024.\u003cbr\u003e\nZifan Wang, Junyu Chen, Ziqing Chen, Pengwei Xie, Rui Chen, Li Yi.\u003cbr\u003e\n\u003ca href=\"https://genh2r.github.io/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSAGE: Bridging Semantic and Actionable Parts for Generalizable Manipulation of Articulated Objects\u003c/strong\u003e, RSS, 2024.\u003cbr\u003e\nHaoran Geng, Songlin Wei, Congyue Deng, Bokui Shen, He Wang, Leonidas Guibas.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2312.01307\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGAMMA: Graspability-Aware Mobile MAnipulation Policy Learning based on Online Grasping Pose Fusion\u003c/strong\u003e, ICRA, 2024.\u003cbr\u003e\nJiazhao Zhang, Nandiraju Gireesh, Jilong Wang, Xiaomeng Fang, Chaoyi Xu, Weiguang Chen, Liu Dai, He Wang.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2309.15459\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eReALFRED: An Embodied Instruction Following Benchmark in Photo-Realistic Environments\u003c/strong\u003e, ECCV, 2024.\u003cbr\u003e\nTaewoong Kim, Cheolhong Min, Byeonghwi Kim, Jinyeon Kim, Wonje Jeung, Jonghyun Choi.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2407.18550\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDISCO: Embodied Navigation and Interaction via Differentiable Scene Semantics and Dual-level Control\u003c/strong\u003e, ECCV, 2024.\u003cbr\u003e\nXinyu Xu, Shengcheng Luo, Yanchao Yang, Yong-Lu Li, Cewu Lu.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2407.14758\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDynSyn: Dynamical Synergistic Representation for Efficient Learning and Control in Overactuated Embodied Systems\u003c/strong\u003e, ICML, 2024.\u003cbr\u003e\nKaibo He, Chenhui Zuo, Chengtian Ma, Yanan Sui.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2407.11472\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eA-JEPA: Joint-Embedding Predictive Architecture Can Listen\u003c/strong\u003e, arXiv, 2023\u003cbr\u003e\nFei, Zhengcong and Fan, Mingyuan and Huang, Junshi\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2311.15830\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eOne-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization\u003c/strong\u003e, NeurIPS, 2023\u003cbr\u003e\nLiu, Minghua and Xu, Chao and Jin, Haian and Chen, Linghao and Varma T, Mukund and Xu, Zexiang and Su, Hao\u003cbr\u003e\n\u003ca href=\"https://openreview.net/forum?id=A6X9y8n4sT\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eIntroduction to Latent Variable Energy-Based Models: A Path Towards Autonomous Machine Intelligence\u003c/strong\u003e, arXiv, 2023\u003cbr\u003e\nDawid, Anna and LeCun, Yann\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/2306.02572\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eGAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts\u003c/strong\u003e, CVPR, 2023\u003cbr\u003e\nGeng, Haoran and Xu, Helin and Zhao, Chengyang and Xu, Chao and Yi, Li and Huang, Siyuan and Wang, He\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/10203924\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eReward-Adaptive Reinforcement Learning: Dynamic Policy Gradient Optimization for Bipedal Locomotion\u003c/strong\u003e, IEEE TPAMI, 2023\u003cbr\u003e\nHuang, Changxin and Wang, Guangrun and Zhou, Zhibo and Zhang, Ronghui and Lin, Liang\u003cbr\u003e\n\u003ca href=\"https://www.computer.org/csdl/journal/tp/2023/06/09956746/1Iu2CDAJBcc\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLearning Fine-Grained Bimanual Manipulation with Low-Cost Hardware\u003c/strong\u003e, ICML, 2023\u003cbr\u003e\nZhao, Tony Z and Kumar, Vikash and Levine, Sergey and Finn, Chelsea\u003cbr\u003e\n\u003ca href=\"https://openreview.net/forum?id=e8Eu1lqLaf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSurfer: Progressive Reasoning with World Models for Robotic Manipulation\u003c/strong\u003e, arxiv, 2023.\u003cbr\u003e\nPengzhen Ren, Kaidong Zhang, Hetao Zheng, Zixuan Li, Yuhang Wen, Fengda Zhu, Mas Ma, Xiaodan Liang.\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2306.11335\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePartManip: Learning Cross-Category Generalizable Part Manipulation Policy from Point Cloud Observations\u003c/strong\u003e, CVPR, 2023.\u003cbr\u003e\nHaoran Geng, Ziming Li, Yiran Geng, Jiayi Chen, Hao Dong, He Wang.\u003cbr\u003e\n\u003ca href=\"https://pku-epic.github.io/PartManip/\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eA Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27\u003c/strong\u003e, Open Review, 2022\u003cbr\u003e\nYann LeCun\u003cbr\u003e\n\u003ca href=\"https://openreview.net/pdf?id=BZ5a1r-kVsf\u0026amp;\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eReal2Sim2Real: Self-Supervised Learning of Physical Single-Step Dynamic Actions for Planar Robot Casting\u003c/strong\u003e, ICRA, 2022\u003cbr\u003e\nLim, Vincent and Huang, Huang and Chen, Lawrence Yunliang and Wang, Jonathan and Ichnowski, Jeffrey and Seita, Daniel and Laskey, Michael and Goldberg, Ken\u003cbr\u003e\n\u003ca href=\"https://dl.acm.org/doi/abs/10.1109/ICRA46639.2022.9811651\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eContinuous Jumping for Legged Robots on Stepping Stones via Trajectory Optimization and Model Predictive Control\u003c/strong\u003e, IEEE CDC, 2022\u003cbr\u003e\nNguyen, Chuong and Bao, Lingfan and Nguyen, Quan\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/2204.01147\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eReward-Adaptive Reinforcement Learning: Dynamic Policy Gradient Optimization for Bipedal Locomotion\u003c/strong\u003e, TPAMI, 2022.\u003cbr\u003e\nChangxin Huang, Guangrun Wang, Zhibo Zhou, Ronghui Zhang, Liang Lin.\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/9956746\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eTransporter Networks: Rearranging the Visual World for Robotic Manipulation\u003c/strong\u003e, CoRL, 2021\u003cbr\u003e\nZeng, Andy and Florence, Pete and Tompson, Jonathan and Welker, Stefan and Chien, Jonathan and Attarian, Maria and Armstrong, Travis and Krasin, Ivan and Duong, Dan and Sindhwani, Vikas and others\u003cbr\u003e\n\u003ca href=\"https://proceedings.mlr.press/v155/zeng21a.html\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eThe MIT Humanoid Robot: Design, Motion Planning, and Control for Acrobatic Behaviors\u003c/strong\u003e, IEEE-RAS 20th International Conference on Humanoid Robots (Humanoids), 2021\u003cbr\u003e\nChignoli, Matthew and Kim, Donghyun and Stanger-Jones, Elijah and Kim, Sangbae\u003cbr\u003e\n\u003ca href=\"https://arxiv.longhoe.net/pdf/2104.09025\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSim2Real Transfer for Reinforcement Learning without Dynamics Randomization\u003c/strong\u003e, IROS, 2020\u003cbr\u003e\nKaspar, Manuel and Osorio, Juan D Mu{~n}oz and Bock, Jurgen\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/9341260\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eLearning Dexterous In-Hand Manipulation\u003c/strong\u003e, The International Journal of Robotics Research, 2020\u003cbr\u003e\nAndrychowicz, OpenAI: Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and others\u003cbr\u003e\n\u003ca href=\"https://journals.sagepub.com/doi/full/10.1177/0278364919887447\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDeepGait: Planning and Control of Quadrupedal Gaits using Deep Reinforcement Learning\u003c/strong\u003e, IEEE Robotics and Automation Letters, 2020\u003cbr\u003e\nTsounis, Vassilios and Alge, Mitja and Lee, Joonho and Farshidian, Farbod and Hutter, Marco\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/pdf/1909.08399\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eOptimized Jumping on the MIT Cheetah 3 Robot\u003c/strong\u003e, ICRA, 2019\u003cbr\u003e\nNguyen, Quan and Powell, Matthew J and Katz, Benjamin and Di Carlo, Jared and Kim, Sangbae\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/8794449\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eWorld Models\u003c/strong\u003e, NIPS, 2018\u003cbr\u003e\nHa, David and Schmidhuber, Jurgen\u003cbr\u003e\n\u003ca href=\"https://mx.nthu.edu.tw/~jlliu/teaching/AI17/Auto8.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eMIT Cheetah 3: Design and Control of a Robust, Dynamic Quadruped Robot\u003c/strong\u003e, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2018\u003cbr\u003e\nBledt, Gerardo and Powell, Matthew J and Katz, Benjamin and Di Carlo, Jared and Wensing, Patrick M and Kim, Sangbae\u003cbr\u003e\n\u003ca href=\"https://dspace.mit.edu/bitstream/handle/1721.1/126619/iros.pdf?sequence=2\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eSim-to-Real Reinforcement Learning for Deformable Object Manipulation\u003c/strong\u003e, CoRL, 2018\u003cbr\u003e\nMatas, Jan and James, Stephen and Davison, Andrew J\u003cbr\u003e\n\u003ca href=\"http://proceedings.mlr.press/v87/matas18a/matas18a.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDynamic Walking on Randomly-Varying Discrete Terrain With One-Step Preview\u003c/strong\u003e, Robotics: Science and Systems, 2017\u003cbr\u003e\nNguyen, Quan and Agrawal, Ayush and Da, Xingye and Martin, William C and Geyer, Hartmut and Grizzle, Jessy W and Sreenath, Koushil\u003cbr\u003e\n\u003ca href=\"https://hybrid-robotics.berkeley.edu/publications/RSS2017_DiscreteTerrain_Walking.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDeep Kernels for Optimizing Locomotion Controllers\u003c/strong\u003e, CoRL, 2017\u003cbr\u003e\nAntonova, Rika and Rai, Akshara and Atkeson, Christopher G\u003cbr\u003e\n\u003ca href=\"http://proceedings.mlr.press/v78/antonova17a/antonova17a.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePreparing for the Unknown: Learning a Universal Policy with Online System Identification\u003c/strong\u003e, RSS, 2017\u003cbr\u003e\nYu, Wenhao and Tan, Jie and Liu, C Karen and Turk, Greg\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/1702.02453\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDomain Randomization for Transferring Deep Neural Networks from Simulation to the Real World\u003c/strong\u003e, IROS, 2017\u003cbr\u003e\nTobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter\u003cbr\u003e\n\u003ca href=\"https://ieeexplore.ieee.org/abstract/document/8202133\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003ePractice Makes Perfect: An Optimization-Based Approach to Controlling Agile Motions for a Quadruped Robot\u003c/strong\u003e, IEEE Robotics \u0026amp; Automation Magazine, 2016\u003cbr\u003e\nGehring, Christian and Coros, Stelian and Hutter, Marco and Bellicoso, Carmine Dario and Heijnen, Huub and Diethelm, Remo and Bloesch, Michael and Fankhauser, P{'e}ter and Hwangbo, Jemin and Hoepflinger, Mark and others\u003cbr\u003e\n\u003ca href=\"https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/183161.1/1/eth-49107-01.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eANYmal - a highly mobile and dynamic quadrupedal robot\u003c/strong\u003e, IEEE/RSJ international conference on intelligent robots and systems (IROS), 2016\u003cbr\u003e\nHutter, Marco and Gehring, Christian and Jud, Dominic and Lauber, Andreas and Bellicoso, C Dario and Tsounis, Vassilios and Hwangbo, Jemin and Bodie, Karen and Fankhauser, Peter and Bloesch, Michael and others\u003cbr\u003e\n\u003ca href=\"https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/118642/eth-49454-01.pdf;sequence=1\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eOptimization Based Full Body Control for the Atlas Robot\u003c/strong\u003e, IEEE-RAS International Conference on Humanoid Robots, 2014\u003cbr\u003e\nFeng, Siyuan and Whitman, Eric and Xinjilefu, X and Atkeson, Christopher G\u003cbr\u003e\n\u003ca href=\"http://www.cs.cmu.edu/afs/cs/user/sfeng/www/sf_hum14.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eA Compliant Hybrid Zero Dynamics Controller for Stable, Efficient and Fast Bipedal Walking on MABEL\u003c/strong\u003e, The International Journal of Robotics Research, 2011\u003cbr\u003e\nSreenath, Koushil and Park, Hae-Won and Poulakakis, Ioannis and Grizzle, Jessy W\u003cbr\u003e\n\u003ca href=\"https://sites.udel.edu/poulakas/files/2022/10/J07-A-Compliant-Hybrid-Zero-Dynamics-Controller.pdf\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp dir=\"auto\"\u003e\u003cstrong\u003eDynamic walk of a biped\u003c/strong\u003e, The International Journal of Robotics Research, 1984\u003cbr\u003e\nMiura, Hirofumi and Shimoyama, Isao\u003cbr\u003e\n\u003ca href=\"https://journals.sagepub.com/doi/abs/10.1177/027836498400300206\" rel=\"nofollow\"\u003e[page]\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e\u003ca id=\"user-content-datasets\"\u003e Datasets \u003c/a\u003e\u003ca href=\"#table-of-contents\"\u003e\u003c/a\u003e \u003c/h2\u003e\u003ca id=\"user-content--datasets--\" class=\"anchor\" aria-label=\"Permalink:  Datasets \" href=\"#-datasets--\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eTo be updated...\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003cstrong\u003eRefSpatial\u003c/strong\u003e, 2025. \u003ca href=\"https://huggingface.co/datasets/JingkunAn/RefSpatial\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVisualAgentBench\u003c/strong\u003e, 2023.\u003ca href=\"https://github.com/THUDM/VisualAgentBench\"\u003elink\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOpen X-Embodiment\u003c/strong\u003e, 2023.\u003ca href=\"https://robotics-transformer-x.github.io/\" rel=\"nofollow\"\u003elink\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRH20T-P\u003c/strong\u003e, 2024.\u003ca href=\"https://sites.google.com/view/rh20t-primitive/main\" rel=\"nofollow\"\u003elink\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eALOHA 2\u003c/strong\u003e, 2024.\u003ca href=\"https://aloha-2.github.io/\" rel=\"nofollow\"\u003elink\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGRUtopia\u003c/strong\u003e, 2024.\u003ca href=\"https://github.com/OpenRobotLab/GRUtopia\"\u003elink\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eARIO (All Robots In One)\u003c/strong\u003e, 2024.\u003ca href=\"https://imaei.github.io/project_pages/ario/\" rel=\"nofollow\"\u003elink\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVLABench\u003c/strong\u003e, 2024.\u003ca href=\"https://vlabench.github.io/\" rel=\"nofollow\"\u003elink\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMatterport3D\u003c/strong\u003e, 2017. \u003ca href=\"https://github.com/niessner/Matterport\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRoboMIND\u003c/strong\u003e, 2025. \u003ca href=\"https://x-humanoid-robomind.github.io/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eEmbodied Perception\u003c/h3\u003e\u003ca id=\"user-content-embodied-perception\" class=\"anchor\" aria-label=\"Permalink: Embodied Perception\" href=\"#embodied-perception\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eVision\u003c/h4\u003e\u003ca id=\"user-content-vision\" class=\"anchor\" aria-label=\"Permalink: Vision\" href=\"#vision\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003cstrong\u003eBEHAVIOR Vision Suite\u003c/strong\u003e, 2024. \u003ca href=\"https://behavior-vision-suite.github.io/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSpatialQA\u003c/strong\u003e, 2024.\u003ca href=\"https://github.com/BAAI-DCAI/SpatialBot\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSpatialBench\u003c/strong\u003e, 2024. \u003ca href=\"https://huggingface.co/datasets/RussRobin/SpatialBench\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUni3DScenes\u003c/strong\u003e, 2024. \u003ca href=\"https://huggingface.co/datasets/RussRobinSpatialBench\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eActive Recognition Dataset\u003c/strong\u003e, 2023. \u003ca href=\"https://leifan95.github.io/_pages/AR-dataset/index.html\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBaxter_UR5_95_Objects_Dataset\u003c/strong\u003e, 2023. \u003ca href=\"https://www.eecs.tufts.edu/~gtatiya/pages/2022/Baxter_UR5_95_Objects_Dataset.html\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCaltech-256\u003c/strong\u003e, 2022. \u003ca href=\"https://data.caltech.edu/records/nyy15-4j048\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDIDI Dataset\u003c/strong\u003e, 2020. \u003ca href=\"https://github.com/google-research/google-research/blob/master/didi_dataset/README.md\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReplica\u003c/strong\u003e, 2019. \u003ca href=\"https://github.com/facebookresearch/Replica-Dataset\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eScanObjectNN\u003c/strong\u003e, 2019. \u003ca href=\"https://hkust-vgd.github.io/scanobjectnn/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOCID Dataset\u003c/strong\u003e, 2019. \u003ca href=\"https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/object-clutter-indoor-dataset/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eL3RScan\u003c/strong\u003e, 2019. \u003ca href=\"https://github.com/WaldJohannaU/3RScan\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEmbodiedScan\u003c/strong\u003e, 2019. \u003ca href=\"https://docs.google.com/forms/d/e/1FAIpQLScUXEDTksGiqHZp31j7Zp7zlCNV7p_08uViwP_Nbzfn3g6hhw/viewform\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUZH-FPV Dataset\u003c/strong\u003e, 2019. \u003ca href=\"https://fpv.ifi.uzh.ch/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLM Data\u003c/strong\u003e, 2019. \u003ca href=\"https://peringlab.org/lmdata/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTUM Visual-Inertial Dataset\u003c/strong\u003e, 2018. \u003ca href=\"https://cvg.cit.tum.de/data/datasets/visual-inertial-dataset\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eScanNet\u003c/strong\u003e, 2017. \u003ca href=\"https://github.com/ScanNet/ScanNet\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSUNCG\u003c/strong\u003e, 2017. \u003ca href=\"http://suncg.cs.princeton.edu/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSemantic 3D\u003c/strong\u003e, 2017. \u003ca href=\"http://www.semantic3d.net/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eScanNet v2\u003c/strong\u003e, 2017. \u003ca href=\"https://github.com/ScanNet/ScanNet\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eS3DIS\u003c/strong\u003e, 2016. \u003ca href=\"http://buildingparser.stanford.edu/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSynthia\u003c/strong\u003e, 2016. \u003ca href=\"https://synthia-dataset.net/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModelNet\u003c/strong\u003e, 2015. \u003ca href=\"https://modelnet.cs.princeton.edu/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eORBvoc\u003c/strong\u003e, 2015. \u003ca href=\"https://github.com/raulmur/ORB_SLAM\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSketch dataset\u003c/strong\u003e, 2015. \u003ca href=\"https://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSUN RGBD\u003c/strong\u003e, 2015. \u003ca href=\"https://rgbd.cs.princeton.edu/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eShapeNet\u003c/strong\u003e, 2015. \u003ca href=\"https://shapenet.org/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMVS Dataset\u003c/strong\u003e, 2014. \u003ca href=\"http://roboimagedata.compute.dtu.dk/?page_id=36\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSUOD\u003c/strong\u003e, 2013. \u003ca href=\"https://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSUN360\u003c/strong\u003e, 2012. \u003ca href=\"https://vision.cs.princeton.edu/projects/2012/SUN360/data/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNYU Depth Dataset V2\u003c/strong\u003e, 2012. \u003ca href=\"https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTUM-RGBD\u003c/strong\u003e, 2012. \u003ca href=\"https://cvg.cit.tum.de/data/datasets/rgbd-dataset/download\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEuRoC MAV Dataset\u003c/strong\u003e, 2012. \u003ca href=\"https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSemantic KITTI\u003c/strong\u003e, 2012. \u003ca href=\"https://www.semantic-kitti.org/dataset.html#download\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eKITTI Object Recognition\u003c/strong\u003e, 2012. \u003ca href=\"http://www.cvlibs.net/datasets/kitti/eval_object.php\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStanford Track Collection\u003c/strong\u003e, 2011. \u003ca href=\"http://cs.stanford.edu/people/teichman/stc/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eTactile\u003c/h4\u003e\u003ca id=\"user-content-tactile\" class=\"anchor\" aria-label=\"Permalink: Tactile\" href=\"#tactile\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003cstrong\u003eTouch100k\u003c/strong\u003e, 2024. \u003ca href=\"https://cocacola-lab.github.io/Touch100k/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eARIO (All Robots In One)\u003c/strong\u003e, 2024. \u003ca href=\"https://imaei.github.io/project_pages/ario/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTaRF\u003c/strong\u003e, 2024. \u003ca href=\"https://dou-yiming.github.io/TaRF/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTVL\u003c/strong\u003e, 2024. \u003ca href=\"https://huggingface.co/datasets/mlfu7/Touch-Vision-Language-Dataset\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eYCB-Slide\u003c/strong\u003e, 2022. \u003ca href=\"https://github.com/rpl-cmu/YCB-Slide\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTouch and Go\u003c/strong\u003e, 2022. \u003ca href=\"https://drive.google.com/drive/folders/1NDasyshDCL9aaQzxjn_-Q5MBURRT360B\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSSVTP\u003c/strong\u003e, 2022. \u003ca href=\"https://drive.google.com/file/d/1H0B-jJ4l3tJu2zuqf-HbZy2bjEl-vL3f/view?usp=sharing\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eObjectFolder\u003c/strong\u003e, 2021-2023. \u003ca href=\"https://github.com/rhgao/ObjectFolder\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDecoding the BioTac\u003c/strong\u003e, 2020. \u003ca href=\"https://drive.google.com/drive/folders/1-BkqiFN9q6cz9Dk74oDlfmDs2m7ZvbWC\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSynTouch\u003c/strong\u003e, 2019. \u003ca href=\"https://tams.informatik.uni-hamburg.de/research/datasets/index.php#biotac_single_contact_response\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThe Feeling of Success\u003c/strong\u003e, 2017. \u003ca href=\"https://sites.google.com/view/the-feeling-of-success/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eEmbodied Navigation\u003c/h3\u003e\u003ca id=\"user-content-embodied-navigation\" class=\"anchor\" aria-label=\"Permalink: Embodied Navigation\" href=\"#embodied-navigation\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003cstrong\u003eALFRED\u003c/strong\u003e, 2020. \u003ca href=\"https://askforalfred.com/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eREVERIE\u003c/strong\u003e, 2020. \u003ca href=\"https://github.com/YuankaiQi/REVERIE\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCVDN\u003c/strong\u003e, 2019. \u003ca href=\"https://github.com/mmurray/cvdn/\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRoom to Room (R2R)\u003c/strong\u003e, 2017. \u003ca href=\"https://paperswithcode.com/dataset/room-to-room\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDivScene\u003c/strong\u003e, 2024.\u003ca href=\"https://github.com/zhaowei-wang-nlp/DivScene\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLH-VLN\u003c/strong\u003e, 2025. \u003ca href=\"https://hcplab-sysu.github.io/LH-VLN/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eEmbodied Question Answering\u003c/h3\u003e\u003ca id=\"user-content-embodied-question-answering\" class=\"anchor\" aria-label=\"Permalink: Embodied Question Answering\" href=\"#embodied-question-answering\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003cstrong\u003eSpatialQA\u003c/strong\u003e, 2024. \u003ca href=\"https://github.com/BAAI-DCAI/SpatialBot\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eS-EQA\u003c/strong\u003e, 2024. \u003ca href=\"https://gamma.umd.edu/researchdirections/embodied/seqa/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHM-EQA\u003c/strong\u003e, 2024. \u003ca href=\"https://github.com/Stanford-ILIAD/explore-eqa\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eK-EQA\u003c/strong\u003e, 2023. \u003ca href=\"https://arxiv.org/abs/2109.07872\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSQA3D\u003c/strong\u003e, 2023. \u003ca href=\"https://sqa3d.github.io/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVideoNavQA\u003c/strong\u003e, 2019. \u003ca href=\"https://github.com/catalina17/VideoNavQA\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMP3D-EQA\u003c/strong\u003e, 2019. \u003ca href=\"https://askforalfred.com/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMT-EQA\u003c/strong\u003e, 2019. \u003ca href=\"https://github.com/facebookresearch/MT-EQA\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIQUAD V1\u003c/strong\u003e, 2018. \u003ca href=\"https://github.com/danielgordon10/thor-iqa-cvpr-2018\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEQA\u003c/strong\u003e, 2018. \u003ca href=\"https://embodiedqa.org/data\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eEmbodied Manipulation\u003c/h3\u003e\u003ca id=\"user-content-embodied-manipulation\" class=\"anchor\" aria-label=\"Permalink: Embodied Manipulation\" href=\"#embodied-manipulation\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003cstrong\u003eOAKINK2\u003c/strong\u003e, 2024. \u003ca href=\"https://oakink.net/v2/\" rel=\"nofollow\"\u003e[link]\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eOther Useful Embodied Projects \u0026amp; Tools\u003c/h2\u003e\u003ca id=\"user-content-other-useful-embodied-projects--tools\" class=\"anchor\" aria-label=\"Permalink: Other Useful Embodied Projects \u0026amp; Tools\" href=\"#other-useful-embodied-projects--tools\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eResources\u003c/h3\u003e\u003ca id=\"user-content-resources\" class=\"anchor\" aria-label=\"Permalink: Resources\" href=\"#resources\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca href=\"https://github.com/zchoi/Awesome-Embodied-Agent-with-LLMs\"\u003eAwesome-Embodied-Agent-with-LLMs\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://github.com/ChanganVR/awesome-embodied-vision\"\u003eAwesome Embodied Vision\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://github.com/linchangyi1/Awesome-Touch\"\u003eAwesome Touch\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eSimulate Platforms \u0026amp; Enviroments\u003c/h3\u003e\u003ca id=\"user-content-simulate-platforms--enviroments\" class=\"anchor\" aria-label=\"Permalink: Simulate Platforms \u0026amp; Enviroments\" href=\"#simulate-platforms--enviroments\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca href=\"https://github.com/facebookresearch/habitat-lab\"\u003eHabitat-Lab\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://github.com/facebookresearch/habitat-sim\"\u003eHabitat-Sim\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://github.com/StanfordVL/GibsonEnv\"\u003eGibsonEnv\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://github.com/thunlp/LEGENT\"\u003eLEGENT\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://metadriverse.github.io/metaurban/\" rel=\"nofollow\"\u003eMetaUrban\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://github.com/OpenRobotLab/GRUtopia\"\u003eGRUtopia\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://genh2r.github.io/\" rel=\"nofollow\"\u003eGenH2R\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://sites.google.com/view/humanthor/\" rel=\"nofollow\"\u003eDemonstrating HumanTHOR\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://github.com/AutonoBot-Lab/BestMan_Pybullet\"\u003eBestMan\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://github.com/pzhren/InfiniteWorld\"\u003eInfiniteWorld\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://genesis-embodied-ai.github.io/\" rel=\"nofollow\"\u003eGenesis\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://www.nvidia.com/en-us/ai/cosmos/\" rel=\"nofollow\"\u003eCosmos\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eProjects\u003c/h3\u003e\u003ca id=\"user-content-projects\" class=\"anchor\" aria-label=\"Permalink: Projects\" href=\"#projects\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eManipulation\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ca href=\"https://sites.google.com/view/robomamba-web\" rel=\"nofollow\"\u003eRoboMamba\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://robot-ma.github.io/\" rel=\"nofollow\"\u003eMANIPULATE-ANYTHING\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://pku-epic.github.io/DexGraspNet/\" rel=\"nofollow\"\u003eDexGraspNet\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://pku-epic.github.io/UniDexGrasp/\" rel=\"nofollow\"\u003eUniDexGrasp\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://pku-epic.github.io/UniDexGrasp++/\" rel=\"nofollow\"\u003eUniDexGrasp++\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://oakink.net/v2\" rel=\"nofollow\"\u003eOAKINK2\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://github.com/OpenDriveLab/agibot-world\"\u003eAgiBot-World\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eEmbodied Interaction\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ca href=\"https://github.com/facebookresearch/EmbodiedQA\"\u003eEmbodiedQA\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eEmbodied Perception\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ca href=\"https://github.com/OpenRobotLab/EmbodiedScan\"\u003eEmbodiedScan\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eModels \u0026amp; Tools\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ca href=\"https://github.com/dongyh20/Octopus\"\u003eOctopus\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://github.com/allenai/Holodeck\"\u003eHolodeck\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://github.com/allenai/allenact\"\u003eAllenAct\u003c/a\u003e\u003c/p\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003eAgents\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"auto\"\u003e\u003ca href=\"https://github.com/embodied-generalist/embodied-generalist\"\u003eLEO\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://github.com/MineDojo/Voyager\"\u003eVoyager\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e Citation\u003c/h2\u003e\u003ca id=\"user-content-newspaper-citation\" class=\"anchor\" aria-label=\"Permalink: :newspaper: Citation\" href=\"#newspaper-citation\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eIf you think this survey is helpful, please feel free to leave a star  and cite our paper:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{liu2024aligning,\n  title={Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI},\n  author={Liu, Yang and Chen, Weixing and Bai, Yongjie and Liang, Xiaodan and Li, Guanbin and Gao, Wen and Lin, Liang},\n  journal={arXiv preprint arXiv:2407.06886},\n  year={2024}\n}\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e@article\u003c/span\u003e{\u003cspan class=\"pl-en\"\u003eliu2024aligning\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003etitle\u003c/span\u003e=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003eAligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003eauthor\u003c/span\u003e=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003eLiu, Yang and Chen, Weixing and Bai, Yongjie and Liang, Xiaodan and Li, Guanbin and Gao, Wen and Lin, Liang\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003ejournal\u003c/span\u003e=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003earXiv preprint arXiv:2407.06886\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003eyear\u003c/span\u003e=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003e2024\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e\n}\u003c/pre\u003e\u003c/div\u003e\n\u003cdiv class=\"highlight highlight-text-bibtex notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"@article{liu2025aligning,\n  title={Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI},\n  author={Liu, Yang and Chen, Weixing and Bai, Yongjie and Liang, Xiaodan and Li, Guanbin and Gao, Wen and Lin, Liang},\n  journal={IEEE/ASME Transactions on Mechatronics},\n  year={2025}\n}\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e@article\u003c/span\u003e{\u003cspan class=\"pl-en\"\u003eliu2025aligning\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003etitle\u003c/span\u003e=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003eAligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003eauthor\u003c/span\u003e=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003eLiu, Yang and Chen, Weixing and Bai, Yongjie and Liang, Xiaodan and Li, Guanbin and Gao, Wen and Lin, Liang\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003ejournal\u003c/span\u003e=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003eIEEE/ASME Transactions on Mechatronics\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003eyear\u003c/span\u003e=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003e2025\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e\n}\u003c/pre\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003e Acknowledgements\u003c/h2\u003e\u003ca id=\"user-content--acknowledgements\" class=\"anchor\" aria-label=\"Permalink:  Acknowledgements\" href=\"#-acknowledgements\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eWe sincerely thank Jingzhou Luo, Xinshuai Song, Kaixuan Jiang, Junyi Lin, Zhida Li, and Ganlong Zhao for their contributions.\u003c/p\u003e\n\u003c/article\u003e","loaded":true,"timedOut":false,"errorMessage":null,"headerInfo":{"toc":[{"level":1,"text":"Paper List  and Resource Repository for Embodied AI","anchor":"paper-list--and-resource-repository-for-embodied-ai","htmlText":"Paper List  and Resource Repository for Embodied AI"},{"level":4,"text":"We appreciate any useful suggestions for improvement of this paper list or survey from peers. Please raise issues or send an email to liuy856@mail.sysu.edu.cn and chen867820261@gmail.com. Thanks for your cooperation! We also welcome your pull requests for this project!","anchor":"we-appreciate-any-useful-suggestions-for-improvement-of-this-paper-list-or-survey-from-peers-please-raise-issues-or-send-an-email-to-liuy856mailsysueducn-and-chen867820261gmailcom-thanks-for-your-cooperation-we-also-welcome-your-pull-requests-for-this-project","htmlText":"We appreciate any useful suggestions for improvement of this paper list or survey from peers. Please raise issues or send an email to liuy856@mail.sysu.edu.cn and chen867820261@gmail.com. Thanks for your cooperation! We also welcome your pull requests for this project!"},{"level":2,"text":" About","anchor":"-about","htmlText":" About"},{"level":2,"text":":collision: Update Log","anchor":"collision-update-log","htmlText":" Update Log"},{"level":2,"text":" Table of Contents ","anchor":"-table-of-contents-","htmlText":" Table of Contents "},{"level":2,"text":" Books \u0026 Surveys ","anchor":"-books--surveys--","htmlText":" Books \u0026amp; Surveys "},{"level":2,"text":" Embodied Simulators ","anchor":"-embodied-simulators--","htmlText":" Embodied Simulators "},{"level":3,"text":"General Simulator","anchor":"general-simulator","htmlText":"General Simulator"},{"level":3,"text":"Real-Scene Based Simulators","anchor":"real-scene-based-simulators","htmlText":"Real-Scene Based Simulators"},{"level":2,"text":"  Embodied Perception ","anchor":"--embodied-perception--","htmlText":"  Embodied Perception "},{"level":3,"text":"Active Visual Exploration","anchor":"active-visual-exploration","htmlText":"Active Visual Exploration"},{"level":3,"text":"3D Visual Perception and Grounding","anchor":"3d-visual-perception-and-grounding","htmlText":"3D Visual Perception and Grounding"},{"level":3,"text":"Visual Language Navigation","anchor":"visual-language-navigation","htmlText":"Visual Language Navigation"},{"level":3,"text":"Non-Visual Perception: Tactile","anchor":"non-visual-perception-tactile","htmlText":"Non-Visual Perception: Tactile"},{"level":2,"text":" Embodied Interaction ","anchor":"-embodied-interaction--","htmlText":" Embodied Interaction "},{"level":2,"text":" Embodied Agent ","anchor":"-embodied-agent--","htmlText":" Embodied Agent "},{"level":3,"text":"Embodied Multimodal Foundation Models and VLA Methods","anchor":"embodied-multimodal-foundation-models-and-vla-methods","htmlText":"Embodied Multimodal Foundation Models and VLA Methods"},{"level":3,"text":"Embodied Manipulation \u0026 Control","anchor":"embodied-manipulation--control","htmlText":"Embodied Manipulation \u0026amp; Control"},{"level":2,"text":" Sim-to-Real Adaptation ","anchor":"-sim-to-real-adaptation--","htmlText":" Sim-to-Real Adaptation "},{"level":2,"text":" Datasets ","anchor":"-datasets--","htmlText":" Datasets "},{"level":3,"text":"Embodied Perception","anchor":"embodied-perception","htmlText":"Embodied Perception"},{"level":4,"text":"Vision","anchor":"vision","htmlText":"Vision"},{"level":4,"text":"Tactile","anchor":"tactile","htmlText":"Tactile"},{"level":3,"text":"Embodied Navigation","anchor":"embodied-navigation","htmlText":"Embodied Navigation"},{"level":3,"text":"Embodied Question Answering","anchor":"embodied-question-answering","htmlText":"Embodied Question Answering"},{"level":3,"text":"Embodied Manipulation","anchor":"embodied-manipulation","htmlText":"Embodied Manipulation"},{"level":2,"text":"Other Useful Embodied Projects \u0026 Tools","anchor":"other-useful-embodied-projects--tools","htmlText":"Other Useful Embodied Projects \u0026amp; Tools"},{"level":3,"text":"Resources","anchor":"resources","htmlText":"Resources"},{"level":3,"text":"Simulate Platforms \u0026 Enviroments","anchor":"simulate-platforms--enviroments","htmlText":"Simulate Platforms \u0026amp; Enviroments"},{"level":3,"text":"Projects","anchor":"projects","htmlText":"Projects"},{"level":2,"text":":newspaper: Citation","anchor":"newspaper-citation","htmlText":" Citation"},{"level":2,"text":" Acknowledgements","anchor":"-acknowledgements","htmlText":" Acknowledgements"}],"siteNavLoginPath":"/login?return_to=https%3A%2F%2Fgithub.com%2FHCPLab-SYSU%2FEmbodied_AI_Paper_List"}}],"overviewFilesProcessingTime":0,"copilotSWEAgentEnabled":false}},"appPayload":{"helpUrl":"https://docs.github.com","findFileWorkerPath":"/assets-cdn/worker/find-file-worker-0cea8c6113ab.js","findInFileWorkerPath":"/assets-cdn/worker/find-in-file-worker-105a4a160ddd.js","githubDevUrl":null,"enabled_features":{"copilot_workspace":null,"code_nav_ui_events":false,"react_blob_overlay":false,"accessible_code_button":true}}}}</script>
  <div data-target="react-partial.reactRoot"> <!-- --> <!-- --> <div class="OverviewContent-module__Box--uNd1J"><div class="OverviewHeader-module__Box--fFKf5"></div><div class="OverviewContent-module__Box_1--RhaEy"><div class="OverviewContent-module__Box_2--uHewD"><div class="OverviewContent-module__Box_3--NEYWl"><button type="button" aria-haspopup="true" aria-expanded="false" tabindex="0" style="min-width:0" aria-label="main branch" data-testid="anchor-button" class="prc-Button-ButtonBase-c50BI overview-ref-selector width-full RefSelectorAnchoredOverlay-module__RefSelectorOverlayBtn--D34zl" data-loading="false" data-size="medium" data-variant="default" aria-describedby="ref-picker-repos-header-ref-selector-loading-announcement" id="ref-picker-repos-header-ref-selector"><span data-component="buttonContent" data-align="center" class="prc-Button-ButtonContent-HKbr-"><span data-component="text" class="prc-Button-Label-pTQ3x"><div class="RefSelectorAnchoredOverlay-module__RefSelectorOverlayContainer--mCbv8"><div class="RefSelectorAnchoredOverlay-module__RefSelectorOverlayHeader--D4cnZ"><svg aria-hidden="true" focusable="false" class="octicon octicon-git-branch" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z"></path></svg></div><div class="ref-selector-button-text-container RefSelectorAnchoredOverlay-module__RefSelectorBtnTextContainer--yO402"><span class="RefSelectorAnchoredOverlay-module__RefSelectorText--bxVhQ"><!-- -->main</span></div></div></span><span data-component="trailingVisual" class="prc-Button-Visual-2epfX prc-Button-VisualWrap-Db-eB"><svg aria-hidden="true" focusable="false" class="octicon octicon-triangle-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z"></path></svg></span></span></button><button hidden="" data-testid="ref-selector-hotkey-button" data-hotkey-scope="read-only-cursor-text-area"></button></div><div class="OverviewContent-module__Box_4--rOz8J"><a type="button" href="/HCPLab-SYSU/Embodied_AI_Paper_List/branches" class="prc-Button-ButtonBase-c50BI OverviewContent-module__Button--MDoYP" data-loading="false" data-size="medium" data-variant="invisible" aria-describedby=":Rclab:-loading-announcement"><span data-component="buttonContent" data-align="center" class="prc-Button-ButtonContent-HKbr-"><span data-component="leadingVisual" class="prc-Button-Visual-2epfX prc-Button-VisualWrap-Db-eB"><svg aria-hidden="true" focusable="false" class="octicon octicon-git-branch" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z"></path></svg></span><span data-component="text" class="prc-Button-Label-pTQ3x">Branches</span></span></a><a type="button" href="/HCPLab-SYSU/Embodied_AI_Paper_List/tags" class="prc-Button-ButtonBase-c50BI OverviewContent-module__Button--MDoYP" data-loading="false" data-size="medium" data-variant="invisible" aria-describedby=":Rklab:-loading-announcement"><span data-component="buttonContent" data-align="center" class="prc-Button-ButtonContent-HKbr-"><span data-component="leadingVisual" class="prc-Button-Visual-2epfX prc-Button-VisualWrap-Db-eB"><svg aria-hidden="true" focusable="false" class="octicon octicon-tag" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z"></path></svg></span><span data-component="text" class="prc-Button-Label-pTQ3x">Tags</span></span></a></div><div class="OverviewContent-module__Box_5--PPbL1"><a type="button" aria-label="Go to Branches page" href="/HCPLab-SYSU/Embodied_AI_Paper_List/branches" class="prc-Button-ButtonBase-c50BI OverviewContent-module__Button_1--_1Ng2" data-loading="false" data-no-visuals="true" data-size="medium" data-variant="invisible" aria-describedby=":Relab:-loading-announcement"><svg aria-hidden="true" focusable="false" class="octicon octicon-git-branch" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z"></path></svg></a><a type="button" aria-label="Go to Tags page" href="/HCPLab-SYSU/Embodied_AI_Paper_List/tags" class="prc-Button-ButtonBase-c50BI OverviewContent-module__Button_1--_1Ng2" data-loading="false" data-no-visuals="true" data-size="medium" data-variant="invisible" aria-describedby=":Rmlab:-loading-announcement"><svg aria-hidden="true" focusable="false" class="octicon octicon-tag" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.752 1.752 0 0 1 1 7.775Zm1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2Z"></path></svg></a></div></div><div class="OverviewContent-module__Box_6--wV7Tw"><div class="OverviewContent-module__Box_7--SbxdI"><div class="OverviewContent-module__Box_8--oumpR"><!--$--><div class="Box-sc-62in7e-0 OverviewContent-module__FileResultsList--irMg6"><span class="TextInput__StyledTextInput-sc-ttxlvl-0 d-flex FileResultsList-module__FilesSearchBox--fSAh3 TextInput-wrapper prc-components-TextInputWrapper-i1ofR prc-components-TextInputBaseWrapper-ueK9q" data-leading-visual="true" data-trailing-visual="true" aria-busy="false"><span class="TextInput-icon" id=":R535ab:" aria-hidden="true"><svg aria-hidden="true" focusable="false" class="octicon octicon-search" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z"></path></svg></span><input type="text" aria-label="Go to file" role="combobox" aria-controls="file-results-list" aria-expanded="false" aria-haspopup="dialog" autoCorrect="off" spellcheck="false" placeholder="Go to file" aria-describedby=":R535ab: :R535abH1:" data-component="input" class="prc-components-Input-Ic-y8" value=""/><span class="TextInput-icon" id=":R535abH1:" aria-hidden="true"></span></span></div><!--/$--></div><div class="OverviewContent-module__Box_9--mQYON"><button type="button" class="prc-Button-ButtonBase-c50BI" data-loading="false" data-no-visuals="true" data-size="medium" data-variant="default" aria-describedby=":R1j5ab:-loading-announcement"><span data-component="buttonContent" data-align="center" class="prc-Button-ButtonContent-HKbr-"><span data-component="text" class="prc-Button-Label-pTQ3x">Go to file</span></span></button></div></div><button type="button" aria-haspopup="true" aria-expanded="false" tabindex="0" class="prc-Button-ButtonBase-c50BI" data-loading="false" data-size="medium" data-variant="primary" aria-describedby=":R75ab:-loading-announcement" id=":R75ab:"><span data-component="buttonContent" data-align="center" class="prc-Button-ButtonContent-HKbr-"><span data-component="leadingVisual" class="prc-Button-Visual-2epfX prc-Button-VisualWrap-Db-eB"><svg aria-hidden="true" focusable="false" class="octicon octicon-code hide-sm" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path></svg></span><span data-component="text" class="prc-Button-Label-pTQ3x">Code</span><span data-component="trailingVisual" class="prc-Button-Visual-2epfX prc-Button-VisualWrap-Db-eB"><svg aria-hidden="true" focusable="false" class="octicon octicon-triangle-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z"></path></svg></span></span></button><div class="OverviewContent-module__Box_10--ULKAG"><button data-component="IconButton" type="button" aria-haspopup="true" aria-expanded="false" tabindex="0" class="prc-Button-ButtonBase-c50BI prc-Button-IconButton-szpyj" data-loading="false" data-no-visuals="true" data-size="medium" data-variant="default" aria-describedby=":R95ab:-loading-announcement" aria-labelledby=":R7p5ab:" id=":R95ab:"><svg aria-hidden="true" focusable="false" class="octicon octicon-kebab-horizontal" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path></svg></button><span class="prc-TooltipV2-Tooltip-cYMVY" data-direction="n" aria-hidden="true" id=":R7p5ab:">Open more actions menu</span></div></div></div><div class="OverviewContent-module__Box_11--Tqhu2"><div data-hpc="true"><button hidden="" data-testid="focus-next-element-button" data-hotkey="j"></button><button hidden="" data-testid="focus-previous-element-button" data-hotkey="k"></button><h2 class="sr-only ScreenReaderHeading-module__userSelectNone--vlUbc prc-Heading-Heading-6CmGO" data-testid="screen-reader-heading" id="folders-and-files">Folders and files</h2><table class="Table-module__Box--KyMHK" aria-labelledby="folders-and-files"><thead class="DirectoryContent-module__OverviewHeaderRow--FlrUZ Table-module__Box_1--DkRqs"><tr class="Table-module__Box_2--l1wjV"><th colSpan="2" class="DirectoryContent-module__Box--y3Nvf"><span class="text-bold">Name</span></th><th colSpan="1" class="DirectoryContent-module__Box_1--xeAhp"><span class="text-bold">Name</span></th><th class="hide-sm"><div class="width-fit prc-Truncate-Truncate-A9Wn6" data-inline="true" title="Last commit message" style="--truncate-max-width:125px"><span class="text-bold">Last commit message</span></div></th><th colSpan="1" class="DirectoryContent-module__Box_2--h912w"><div class="width-fit prc-Truncate-Truncate-A9Wn6" data-inline="true" title="Last commit date" style="--truncate-max-width:125px"><span class="text-bold">Last commit date</span></div></th></tr></thead><tbody><tr class="DirectoryContent-module__Box_3--zI0N1"><td colSpan="3" class="bgColor-muted p-1 rounded-top-2"><div class="LatestCommit-module__Box--Fimpo"><h2 class="sr-only ScreenReaderHeading-module__userSelectNone--vlUbc prc-Heading-Heading-6CmGO" data-testid="screen-reader-heading">Latest commit</h2><div style="width:120px" class="Skeleton Skeleton--text" data-testid="loading"></div><div class="d-flex flex-shrink-0 gap-2"><div data-testid="latest-commit-details" class="d-none d-sm-flex flex-items-center"></div><div class="d-flex gap-2"><h2 class="sr-only ScreenReaderHeading-module__userSelectNone--vlUbc prc-Heading-Heading-6CmGO" data-testid="screen-reader-heading">History</h2><a href="/HCPLab-SYSU/Embodied_AI_Paper_List/commits/main/" class="prc-Button-ButtonBase-c50BI d-none d-lg-flex LinkButton-module__code-view-link-button--thtqc flex-items-center fgColor-default" data-loading="false" data-size="small" data-variant="invisible" aria-describedby=":Raqj8pab:-loading-announcement"><span data-component="buttonContent" data-align="center" class="prc-Button-ButtonContent-HKbr-"><span data-component="leadingVisual" class="prc-Button-Visual-2epfX prc-Button-VisualWrap-Db-eB"><svg aria-hidden="true" focusable="false" class="octicon octicon-history" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z"></path></svg></span><span data-component="text" class="prc-Button-Label-pTQ3x"><span class="fgColor-default">211 Commits</span></span></span></a><div class="d-sm-none"></div><div class="d-flex d-lg-none"><span role="tooltip" aria-label="211 Commits" id="history-icon-button-tooltip" class="prc-Tooltip-Tooltip--1XZX prc-Tooltip-Tooltip--n-BOOzB tooltipped-n"><a aria-label="View commit history for this file." href="/HCPLab-SYSU/Embodied_AI_Paper_List/commits/main/" class="prc-Button-ButtonBase-c50BI LinkButton-module__code-view-link-button--thtqc flex-items-center fgColor-default" data-loading="false" data-size="small" data-variant="invisible" aria-describedby=":R1iqj8pab:-loading-announcement history-icon-button-tooltip"><span data-component="buttonContent" data-align="center" class="prc-Button-ButtonContent-HKbr-"><span data-component="leadingVisual" class="prc-Button-Visual-2epfX prc-Button-VisualWrap-Db-eB"><svg aria-hidden="true" focusable="false" class="octicon octicon-history" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z"></path></svg></span></span></a></span></div></div></div></div></td></tr><tr class="react-directory-row undefined" id="folder-row-0"><td class="react-directory-row-name-cell-small-screen" colSpan="2"><div class="react-directory-filename-column"><svg aria-hidden="true" focusable="false" class="octicon octicon-file color-fg-muted" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z"></path></svg><div class="overflow-hidden"><div class="react-directory-filename-cell"><div class="react-directory-truncate"><a title="EmbodiedAI.jpg" aria-label="EmbodiedAI.jpg, (File)" class="Link--primary" href="/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/EmbodiedAI.jpg" data-discover="true">EmbodiedAI.jpg</a></div></div></div></div></td><td class="react-directory-row-name-cell-large-screen" colSpan="1"><div class="react-directory-filename-column"><svg aria-hidden="true" focusable="false" class="octicon octicon-file color-fg-muted" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z"></path></svg><div class="overflow-hidden"><div class="react-directory-filename-cell"><div class="react-directory-truncate"><a title="EmbodiedAI.jpg" aria-label="EmbodiedAI.jpg, (File)" class="Link--primary" href="/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/EmbodiedAI.jpg" data-discover="true">EmbodiedAI.jpg</a></div></div></div></div></td><td class="react-directory-row-commit-cell"><div class="Skeleton Skeleton--text"></div></td><td><div class="Skeleton Skeleton--text"></div></td></tr><tr class="react-directory-row undefined" id="folder-row-1"><td class="react-directory-row-name-cell-small-screen" colSpan="2"><div class="react-directory-filename-column"><svg aria-hidden="true" focusable="false" class="octicon octicon-file color-fg-muted" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z"></path></svg><div class="overflow-hidden"><div class="react-directory-filename-cell"><div class="react-directory-truncate"><a title="EmbodiedAI_Review.pdf" aria-label="EmbodiedAI_Review.pdf, (File)" class="Link--primary" href="/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/EmbodiedAI_Review.pdf" data-discover="true">EmbodiedAI_Review.pdf</a></div></div></div></div></td><td class="react-directory-row-name-cell-large-screen" colSpan="1"><div class="react-directory-filename-column"><svg aria-hidden="true" focusable="false" class="octicon octicon-file color-fg-muted" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z"></path></svg><div class="overflow-hidden"><div class="react-directory-filename-cell"><div class="react-directory-truncate"><a title="EmbodiedAI_Review.pdf" aria-label="EmbodiedAI_Review.pdf, (File)" class="Link--primary" href="/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/EmbodiedAI_Review.pdf" data-discover="true">EmbodiedAI_Review.pdf</a></div></div></div></div></td><td class="react-directory-row-commit-cell"><div class="Skeleton Skeleton--text"></div></td><td><div class="Skeleton Skeleton--text"></div></td></tr><tr class="react-directory-row undefined" id="folder-row-2"><td class="react-directory-row-name-cell-small-screen" colSpan="2"><div class="react-directory-filename-column"><svg aria-hidden="true" focusable="false" class="octicon octicon-file color-fg-muted" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z"></path></svg><div class="overflow-hidden"><div class="react-directory-filename-cell"><div class="react-directory-truncate"><a title="README.md" aria-label="README.md, (File)" class="Link--primary" href="/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/README.md" data-discover="true">README.md</a></div></div></div></div></td><td class="react-directory-row-name-cell-large-screen" colSpan="1"><div class="react-directory-filename-column"><svg aria-hidden="true" focusable="false" class="octicon octicon-file color-fg-muted" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z"></path></svg><div class="overflow-hidden"><div class="react-directory-filename-cell"><div class="react-directory-truncate"><a title="README.md" aria-label="README.md, (File)" class="Link--primary" href="/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/README.md" data-discover="true">README.md</a></div></div></div></div></td><td class="react-directory-row-commit-cell"><div class="Skeleton Skeleton--text"></div></td><td><div class="Skeleton Skeleton--text"></div></td></tr><tr class="react-directory-row undefined" id="folder-row-3"><td class="react-directory-row-name-cell-small-screen" colSpan="2"><div class="react-directory-filename-column"><svg aria-hidden="true" focusable="false" class="octicon octicon-file color-fg-muted" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z"></path></svg><div class="overflow-hidden"><div class="react-directory-filename-cell"><div class="react-directory-truncate"><a title="Survey.png" aria-label="Survey.png, (File)" class="Link--primary" href="/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/Survey.png" data-discover="true">Survey.png</a></div></div></div></div></td><td class="react-directory-row-name-cell-large-screen" colSpan="1"><div class="react-directory-filename-column"><svg aria-hidden="true" focusable="false" class="octicon octicon-file color-fg-muted" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z"></path></svg><div class="overflow-hidden"><div class="react-directory-filename-cell"><div class="react-directory-truncate"><a title="Survey.png" aria-label="Survey.png, (File)" class="Link--primary" href="/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/Survey.png" data-discover="true">Survey.png</a></div></div></div></div></td><td class="react-directory-row-commit-cell"><div class="Skeleton Skeleton--text"></div></td><td><div class="Skeleton Skeleton--text"></div></td></tr><tr class="react-directory-row undefined" id="folder-row-4"><td class="react-directory-row-name-cell-small-screen" colSpan="2"><div class="react-directory-filename-column"><svg aria-hidden="true" focusable="false" class="octicon octicon-file color-fg-muted" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z"></path></svg><div class="overflow-hidden"><div class="react-directory-filename-cell"><div class="react-directory-truncate"><a title="teaser.png" aria-label="teaser.png, (File)" class="Link--primary" href="/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/teaser.png" data-discover="true">teaser.png</a></div></div></div></div></td><td class="react-directory-row-name-cell-large-screen" colSpan="1"><div class="react-directory-filename-column"><svg aria-hidden="true" focusable="false" class="octicon octicon-file color-fg-muted" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z"></path></svg><div class="overflow-hidden"><div class="react-directory-filename-cell"><div class="react-directory-truncate"><a title="teaser.png" aria-label="teaser.png, (File)" class="Link--primary" href="/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/teaser.png" data-discover="true">teaser.png</a></div></div></div></div></td><td class="react-directory-row-commit-cell"><div class="Skeleton Skeleton--text"></div></td><td><div class="Skeleton Skeleton--text"></div></td></tr><tr class="d-none DirectoryContent-module__Box_4--QyUbd" data-testid="view-all-files-row"><td colSpan="3" class="DirectoryContent-module__Box_5--OJZQU"><div><button class="prc-Link-Link-85e08">View all files</button></div></td></tr></tbody></table></div><div class="OverviewRepoFiles-module__Box_1--xSt0T"><div class="OverviewRepoFiles-module__Box_2--yIjMp"><div itemscope="" itemType="https://schema.org/abstract" class="OverviewRepoFiles-module__Box_3--Bi2jM"><h2 class="prc-src-InternalVisuallyHidden-nlR9R">Repository files navigation</h2><nav class="prc-components-UnderlineWrapper-oOh5J OverviewRepoFiles-module__UnderlineNav--BHfFi" aria-label="Repository files" data-variant="inset"><ul class="prc-components-UnderlineItemList-b23Hf" role="list"><li class="prc-UnderlineNav-UnderlineNavItem--xDk1"><a href="#" aria-current="page" class="prc-components-UnderlineItem-lJsg-"><span data-component="icon"><svg aria-hidden="true" focusable="false" class="octicon octicon-book" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z"></path></svg></span><span data-component="text" data-content="README">README</span></a></li></ul></nav><button type="button" aria-label="Outline" aria-haspopup="true" aria-expanded="false" tabindex="0" class="prc-Button-ButtonBase-c50BI OverviewRepoFiles-module__ActionMenu_Button--xB9DS" data-loading="false" data-size="medium" data-variant="invisible" aria-describedby=":Rr9ab:-loading-announcement" id=":Rr9ab:"><svg aria-hidden="true" focusable="false" class="octicon octicon-list-unordered" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg></button></div><div class="Box-sc-62in7e-0 js-snippet-clipboard-copy-unpositioned DirectoryRichtextContent-module__SharedMarkdownContent--BTKsc" data-hpc="true"><article class="markdown-body entry-content container-lg" itemprop="text"><br>
<p align="center" dir="auto">
</p><div class="markdown-heading" dir="auto"><h1 align="center" tabindex="-1" class="heading-element" dir="auto"><strong>Paper List  and Resource Repository for Embodied AI</strong></h1><a id="user-content-paper-list--and-resource-repository-for-embodied-ai" class="anchor" aria-label="Permalink: Paper List  and Resource Repository for Embodied AI" href="#paper-list--and-resource-repository-for-embodied-ai"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
  <p align="center" dir="auto">
    <a href="https://www.sysu-hcp.net/" rel="nofollow">HCPLab</a>
    <br>
    SYSU HCP Lab and Pengcheng Laboratory
    <br>
  </p>
<p dir="auto"></p>
<p align="center" dir="auto">
<a target="_blank" rel="noopener noreferrer" href="/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/EmbodiedAI.jpg"><img src="/HCPLab-SYSU/Embodied_AI_Paper_List/raw/main/EmbodiedAI.jpg" width="250" style="max-width: 100%;"></a>
</p>
<p dir="auto"><a href="https://arxiv.org/abs/2407.06886" rel="nofollow"><img src="https://camo.githubusercontent.com/c0553ae02c65b5a63d6c1151b3b8daa66e616c1507fa48020913fd1c7c4b1b85/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323430372e30363838362d6f72616e6765" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2407.06886-orange" style="max-width: 100%;"></a>
<a href="https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/EmbodiedAI_Review.pdf"><img src="https://camo.githubusercontent.com/7a912aa4be033314e25afdcb5d53a629ec68b831a6025150f0a285348f902cdb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617065722d2546302539462539332539362d79656c6c6f77" alt="" data-canonical-src="https://img.shields.io/badge/Paper-%F0%9F%93%96-yellow" style="max-width: 100%;"></a>
<a href="https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List"><img src="https://camo.githubusercontent.com/df7e91dc57b9025906b2ae1bd1f72640281a0dc4c38e500268d3427776e5553c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50726f6a6563742d2546302539462539412538302d70696e6b" alt="" data-canonical-src="https://img.shields.io/badge/Project-%F0%9F%9A%80-pink" style="max-width: 100%;"></a></p>
<div class="markdown-heading" dir="auto"><h4 tabindex="-1" class="heading-element" dir="auto">We appreciate any useful suggestions for improvement of this paper list or survey from peers. Please raise issues or send an email to <strong><a href="mailto:liuy856@mail.sysu.edu.cn">liuy856@mail.sysu.edu.cn</a></strong> and <strong><a href="mailto:chen867820261@gmail.com">chen867820261@gmail.com</a></strong>. Thanks for your cooperation! We also welcome your pull requests for this project!</h4><a id="user-content-we-appreciate-any-useful-suggestions-for-improvement-of-this-paper-list-or-survey-from-peers-please-raise-issues-or-send-an-email-to-liuy856mailsysueducn-and-chen867820261gmailcom-thanks-for-your-cooperation-we-also-welcome-your-pull-requests-for-this-project" class="anchor" aria-label="Permalink: We appreciate any useful suggestions for improvement of this paper list or survey from peers. Please raise issues or send an email to liuy856@mail.sysu.edu.cn and chen867820261@gmail.com. Thanks for your cooperation! We also welcome your pull requests for this project!" href="#we-appreciate-any-useful-suggestions-for-improvement-of-this-paper-list-or-survey-from-peers-please-raise-issues-or-send-an-email-to-liuy856mailsysueducn-and-chen867820261gmailcom-thanks-for-your-cooperation-we-also-welcome-your-pull-requests-for-this-project"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/teaser.png"><img src="/HCPLab-SYSU/Embodied_AI_Paper_List/raw/main/teaser.png" alt="Teaser" title="demo" style="max-width: 100%;"></a></p>
<p dir="auto"><a href="https://arxiv.org/pdf/2407.06886" rel="nofollow"><strong>Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI, IEEE/ASME Transactions on Mechatronics 2025</strong></a><br>
<a href="https://yangliu9208.github.io" rel="nofollow">Yang Liu</a>, Weixing Chen, Yongjie Bai, <a href="https://lemondan.github.io" rel="nofollow">Xiaodan Liang</a>, <a href="http://guanbinli.com/" rel="nofollow">Guanbin Li</a>, <a href="https://idm.pku.edu.cn/info/1017/1041.htm" rel="nofollow">Wen Gao</a>, <a href="http://www.linliang.net/" rel="nofollow">Liang Lin</a></p>
<p align="center" dir="auto">
<a target="_blank" rel="noopener noreferrer" href="/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/Survey.png"><img src="/HCPLab-SYSU/Embodied_AI_Paper_List/raw/main/Survey.png" width="800" style="max-width: 100%;"></a>
</p>  
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"> About</h2><a id="user-content--about" class="anchor" aria-label="Permalink:  About" href="#-about"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications (e.g., intelligent mechatronics systems, smart manufacturing) that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for embodied agents. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in digital and physical environments.  Finally, we summarize the challenges and limitations of embodied AI and discuss potential future directions. We hope this survey will serve as a foundational reference for the research community.</p>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"> Update Log</h2><a id="user-content-collision-update-log" class="anchor" aria-label="Permalink: :collision: Update Log" href="#collision-update-log"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>[2025.05.27] Our Embodied AI Survey paper is accepted by IEEE/ASME Transactions on Mechatronics!</li>
<li>[2024.09.08] We are constantly updating the Dataset section!</li>
<li>[2024.08.31] We added the Datasets section and classified the useful projects!</li>
<li>[2024.08.19] To make readers focus on newest works, we have arranged papers in chronological order!</li>
<li>[2024.08.02] We regularly update the project weekly!</li>
<li>[2024.07.29] We have updated the project!</li>
<li>[2024.07.22] We have updated the paper list and other useful embodied projects!</li>
<li>[2024.07.10] We release the first version of the survey on Embodied AI <a href="https://arxiv.org/pdf/2407.06886" rel="nofollow">PDF</a>!</li>
<li>[2024.07.10] We release the first version of the paper list for Embodied AI. This page is continually updating!</li>
</ul>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><a id="user-content-table-of-contents"> Table of Contents </a></h2><a id="user-content--table-of-contents-" class="anchor" aria-label="Permalink:  Table of Contents " href="#-table-of-contents-"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><a href="#books-surveys">Books &amp; Surveys</a></li>
<li><a href="#simulators">Embodied Simulators</a></li>
<li><a href="#perception">Embodied Perception</a></li>
<li><a href="#interaction">Embodied Interaction</a></li>
<li><a href="#agent">Embodied Agent</a></li>
<li><a href="#sim-to-real">Sim-to-Real Adaptation</a></li>
<li><a href="#datasets">Datasets</a></li>
</ul>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><a id="user-content-books-surveys"> Books &amp; Surveys </a><a href="#table-of-contents"></a> </h2><a id="user-content--books--surveys--" class="anchor" aria-label="Permalink:  Books &amp; Surveys " href="#-books--surveys--"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>
<p dir="auto"><strong>A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI</strong>, arXiv:2505.01458, 2025<br>
Lik Hang Kenny Wong, Xueyang Kang, Kaixin Bai, Jianwei Zhang.<br>
[<a href="https://arxiv.org/pdf/2505.01458" rel="nofollow">Paper</a>]</p>
</li>
<li>
<p dir="auto"><strong>Multimodal Large Models: The New Paradigm of Artificial General Intelligence</strong>, Publishing House of Electronics Industry (PHE), 2024<br>
Yang Liu, Liang Lin<br>
[<a href="https://hcplab-sysu.github.io/Book-of-MLM/" rel="nofollow">Page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI</strong>, arXiv:2407.06886, 2024<br>
Yang Liu, Weixing Chen, Yongjie Bai, Guanbin Li, Wen Gao, Liang Lin.<br>
[<a href="https://arxiv.org/pdf/2407.06886" rel="nofollow">Paper</a>]</p>
</li>
<li>
<p dir="auto"><strong>All Robots in One: A New Standard and Unified Dataset for Versatile, General-Purpose Embodied Agents</strong>, arXiv:2408.10899, 2024<br>
Zhiqiang Wang, Hao Zheng, Yunshuang Nie, Wenjun Xu, Qingwei Wang, Hua Ye, Zhe Li, Kaidong Zhang, Xuewen Cheng, Wanxi Dong, Chang Cai, Liang Lin, Feng Zheng, Xiaodan Liang<br>
[<a href="https://arxiv.org/pdf/2408.10899" rel="nofollow">Paper</a>][<a href="https://imaei.github.io/project_pages/ario/" rel="nofollow">Project</a>]</p>
</li>
<li>
<p dir="auto"><strong>Embodied intelligence toward future smart manufacturing in the era of AI foundation model</strong>, IEEE/ASME Transactions on Mechatronics, 2024<br>
Lei Ren, Jiabao Dong, Shuai Liu, Lin Zhang, and Lihui Wang.<br>
[<a href="https://ieeexplore.ieee.org/abstract/document/10697107" rel="nofollow">Paper</a>]</p>
</li>
<li>
<p dir="auto"><strong>A Survey of Embodied Learning for Object-Centric Robotic Manipulation</strong>, arXiv:2408.11537, 2024<br>
Ying Zheng, Lei Yao, Yuejiao Su, Yi Zhang, Yi Wang, Sicheng Zhao, Yiyi Zhang, Lap-Pui Chau<br>
[<a href="https://arxiv.org/pdf/2408.11537" rel="nofollow">Paper</a>]</p>
</li>
<li>
<p dir="auto"><strong>Teleoperation of Humanoid Robots: A Survey</strong>, IEEE Transactions on Robotics, 2024<br>
Kourosh Darvish, Luigi Penco, Joao Ramos, Rafael Cisneros, Jerry Pratt, Eiichi Yoshida, Serena Ivaldi, Daniele Pucci.<br>
[<a href="https://arxiv.org/pdf/2301.04317" rel="nofollow">Paper</a>]</p>
</li>
<li>
<p dir="auto"><strong>A Survey on Vision-Language-Action Models for Embodied AI</strong>, arXiv:2405.14093, 2024<br>
Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King<br>
[<a href="https://arxiv.org/pdf/2405.14093" rel="nofollow">Paper</a>]</p>
</li>
<li>
<p dir="auto"><strong>Towards Generalist Robot Learning from Internet Video: A Survey</strong>, arXiv:2404.19664, 2024<br>
McCarthy, Robert, Daniel CH Tan, Dominik Schmidt, Fernando Acero, Nathan Herr, Yilun Du, Thomas G. Thuruthel, and Zhibin Li.<br>
[<a href="https://arxiv.org/pdf/2404.19664" rel="nofollow">Paper</a>]</p>
</li>
<li>
<p dir="auto"><strong>A Survey on Robotics with Foundation Models: toward Embodied AI</strong>, arXiv:2402.02385, 2024<br>
Zhiyuan Xu, Kun Wu, Junjie Wen, Jinming Li, Ning Liu, Zhengping Che, and Jian Tang.<br>
[<a href="https://arxiv.org/pdf/2402.02385" rel="nofollow">Paper</a>]</p>
</li>
<li>
<p dir="auto"><strong>Toward general-purpose robots via foundation models: A survey and meta-analysis</strong>, Machines, 2023<br>
Yafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, Jay Patrikar, Nikhil Keetha, Seungchan Kim, Yaqi Xie, Tianyi Zhang, Shibo Zhao, Yu Quan Chong, Chen Wang, Katia Sycara, Matthew Johnson-Roberson, Dhruv Batra, Xiaolong Wang, Sebastian Scherer, Zsolt Kira, Fei Xia, Yonatan Bisk.<br>
[<a href="https://arxiv.org/pdf/2312.08782" rel="nofollow">Paper</a>]</p>
</li>
<li>
<p dir="auto"><strong>Deformable Object Manipulation in Caregiving Scenarios: A Review</strong>, Machines, 2023<br>
Liman Wang, Jihong Zhu.<br>
[[Paper]<a href="https://www.mdpi.com/2075-1702/11/11/1013" rel="nofollow">https://www.mdpi.com/2075-1702/11/11/1013</a>]</p>
</li>
<li>
<p dir="auto"><strong>A survey of embodied ai: From simulators to research tasks</strong>, IEEE Transactions on Emerging Topics in Computational Intelligence, 2022<br>
Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, Cheston Tan<br>
[<a href="https://arxiv.org/pdf/2103.04918" rel="nofollow">Paper</a>]</p>
</li>
<li>
<p dir="auto"><strong>The development of embodied cognition: Six lessons from babies</strong>, Artificial life, 2005<br>
Linda Smith, Michael Gasser<br>
[<a href="https://cogdev.sitehost.iu.edu/labwork/6_lessons.pdf" rel="nofollow">Paper</a>]</p>
</li>
<li>
<p dir="auto"><strong>Embodied artificial intelligence: Trends and challenges</strong>, Lecture notes in computer science, 2004<br>
Rolf Pfeifer, Fumiya Iida<br>
[<a href="https://people.csail.mit.edu/iida/papers/PfeiferIidaEAIDags.pdf" rel="nofollow">Paper</a>]</p>
</li>
</ul>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><a id="user-content-simulators"> Embodied Simulators </a><a href="#table-of-contents"></a> </h2><a id="user-content--embodied-simulators--" class="anchor" aria-label="Permalink:  Embodied Simulators " href="#-embodied-simulators--"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">General Simulator</h3><a id="user-content-general-simulator" class="anchor" aria-label="Permalink: General Simulator" href="#general-simulator"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>
<p dir="auto"><strong>Design and use paradigms for gazebo, an open-source multi-robot simulator</strong>, IROS, 2004<br>
Koenig, Nathan, Andrew, Howard.<br>
[<a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=79f91c1c95271a075b91e9fdca43d6c31e4cbe17" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Nvidia isaac sim: Robotics simulation and synthetic data</strong>, NVIDIA, 2023<br>
[<a href="https://developer.nvidia.com/isaac/sim" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Aerial Gym -- Isaac Gym Simulator for Aerial Robots</strong>, ArXiv, 2023<br>
Mihir Kulkarni and Theodor J. L. Forgaard and Kostas Alexis.<br>
[<a href="https://arxiv.org/abs/2305.16510" rel="nofollow">paper</a>]</p>
</li>
<li>
<p dir="auto"><strong>Webots: open-source robot simulator</strong>, 2018<br>
Cyberbotics<br>
[<a href="https://cyberbotics.com/doc/reference/index" rel="nofollow">page</a>, <a href="https://github.com/cyberbotics/webots">code</a>]</p>
</li>
<li>
<p dir="auto"><strong>Unity: A general platform for intelligent agents</strong>, ArXiv, 2020<br>
Juliani, Arthur, Vincent-Pierre, Berges, Ervin, Teng, Andrew, Cohen, Jonathan, Harper, Chris, Elion, Chris, Goy, Yuan, Gao, Hunter, Henry, Marwan, Mattar, Danny, Lange.<br>
[<a href="https://arxiv.org/pdf/1809.02627" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles</strong>, Field and Service Robotics, 2017<br>
Shital Shah, , Debadeepta Dey, Chris Lovett, Ashish Kapoor.<br>
[<a href="https://arxiv.org/pdf/1705.05065.pdf%20http://arxiv.org/abs/1705.05065" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Pybullet, a python module for physics simulation for games, robotics and machine learning</strong>, 2016<br>
Coumans, Erwin, Yunfei, Bai.<br>
[<a href="https://github.com/bulletphysics/bullet3">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>V-REP: A versatile and scalable robot simulation framework</strong>, IROS, 2013<br>
Rohmer, Eric, Surya PN, Singh, Marc, Freese.<br>
[<a href="https://coppeliarobotics.com/coppeliaSim_v-rep_iros2013.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>MuJoCo: A physics engine for model-based control</strong>, IROS, 2012<br>
Todorov, Emanuel, Tom, Erez, Yuval, Tassa.<br>
[<a href="https://ieeexplore.ieee.org/abstract/document/6386109/" rel="nofollow">page</a>, <a href="https://github.com/google-deepmind/mujoco">code</a>]</p>
</li>
<li>
<p dir="auto"><strong>Modular open robots simulation engine: Morse</strong>, ICRA, 2011<br>
Echeverria, Gilberto and Lassabe, Nicolas and Degroote, Arnaud and Lemaignan, S{'e}verin<br>
[<a href="https://www.openrobots.org/morse/material/media/pdf/paper-icra.pdf" rel="nofollow">page</a>]</p>
</li>
</ul>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">Real-Scene Based Simulators</h3><a id="user-content-real-scene-based-simulators" class="anchor" aria-label="Permalink: Real-Scene Based Simulators" href="#real-scene-based-simulators"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>
<p dir="auto"><strong>InfiniteWorld: A Unified Scalable Simulation Framework for General Visual-Language Robot Interaction</strong>, arxiv, 2024<br>
Pengzhen Ren, Min Li, Zhen Luo, Xinshuai Song, Ziwei Chen, Weijia Liufu, Yixuan Yang, Hao Zheng, Rongtao Xu, Zitong Huang, Tongsheng Ding, Luyang Xie, Kaidong Zhang, Changfei Fu, Yang Liu, Liang Lin, Feng Zheng, Xiaodan Liang.<br>
[<a href="https://arxiv.org/pdf/2412.05789" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>ManiSkill3: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI</strong>, arxiv, 2024<br>
Stone Tao, Fanbo Xiang, Arth Shukla, Yuzhe Qin, Xander Hinrichsen, Xiaodi Yuan, Chen Bao, Xinsong Lin, Yulin Liu, Tse-kai Chan, Yuan Gao, Xuanlin Li, Tongzhou Mu, Nan Xiao, Arnav Gurha, Zhiao Huang, Roberto Calandra, Rui Chen, Shan Luo, Hao Su.<br>
[<a href="https://arxiv.org/pdf/2410.00425" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI</strong>, arxiv, 2024<br>
Yang, Yandan, Baoxiong, Jia, Peiyuan, Zhi, Siyuan, Huang.<br>
[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_PhyScene_Physically_Interactable_3D_Scene_Synthesis_for_Embodied_AI_CVPR_2024_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Holodeck: Language Guided Generation of 3D Embodied AI Environments</strong>, CVPR, 2024<br>
Yue Yang, , Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Mark Yatskar, Aniruddha Kembhavi, Christopher Clark.<br>
[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Holodeck_Language_Guided_Generation_of_3D_Embodied_AI_Environments_CVPR_2024_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation</strong>, arXiv, 2023<br>
Wang, Yufei, Zhou, Xian, Feng, Chen, Tsun-Hsuan, Wang, Yian, Wang, Katerina, Fragkiadaki, Zackory, Erickson, David, Held, Chuang, Gan.<br>
[<a href="https://arxiv.org/pdf/2311.01455" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>ProcTHOR: Large-Scale Embodied AI Using Procedural Generation</strong>, NeurIPS, 2022<br>
Deitke, VanderBilt, Herrasti, Weihs, Salvador, Ehsani, Han, Kolve, Farhadi, Kembhavi, Mottaghi<br>
[<a href="https://arxiv.org/pdf/2206.06994" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation</strong>, NeurIPS, 2021<br>
Gan, Chuang, J., Schwartz, Seth, Alter, Martin, Schrimpf, James, Traer, JulianDe, Freitas, Jonas, Kubilius, Abhishek, Bhandwaldar, Nick, Haber, Megumi, Sano, Kuno, Kim, Elias, Wang, Damian, Mrowca, Michael, Lingelbach, Aidan, Curtis, KevinT., Feigelis, DavidM., Bear, Dan, Gutfreund, DavidD., Cox, JamesJ., DiCarlo, JoshH., McDermott, JoshuaB., Tenenbaum, Daniel, Yamins.<br>
[<a href="https://arxiv.org/pdf/2007.04954" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes</strong>, IROS, 2021<br>
Shen, Bokui, Fei, Xia, Chengshu, Li, Roberto, Martn-Martn, Linxi, Fan, Guanzhi, Wang, Claudia, Prez-DArpino, Shyamal, Buch, Sanjana, Srivastava, Lyne, Tchapmi, Micael, Tchapmi, Kent, Vainio, Josiah, Wong, Li, Fei-Fei, Silvio, Savarese.<br>
[<a href="https://arxiv.org/pdf/2012.02924" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>SAPIEN: A SimulAted Part-Based Interactive ENvironment</strong>, CVPR, 2020<br>
Xiang, Fanbo, Yuzhe, Qin, Kaichun, Mo, Yikuan, Xia, Hao, Zhu, Fangchen, Liu, Minghua, Liu, Hanxiao, Jiang, Yifu, Yuan, He, Wang, Li, Yi, Angel X., Chang, Leonidas J., Guibas, Hao, Su.<br>
[<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Xiang_SAPIEN_A_SimulAted_Part-Based_Interactive_ENvironment_CVPR_2020_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Habitat: A Platform for Embodied AI Research</strong>, ICCV, 2019<br>
Savva, Manolis, Abhishek, Kadian, Oleksandr, Maksymets, Yili, Zhao, Erik, Wmans, Bhavana, Jain, Julian, Straub, Jia, Liu, Vladlen, Koltun, Jitendra, Malik, Devi, Parikh, Dhruv, Batra.<br>
[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Savva_Habitat_A_Platform_for_Embodied_AI_Research_ICCV_2019_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>VirtualHome: Simulating Household Activities Via Programs</strong>, CVPR, 2018<br>
Puig, Xavier, Kevin, Ra, Marko, Boben, Jiaman, Li, Tingwu, Wang, Sanja, Fidler, Antonio, Torralba.<br>
[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Matterport3D: Learning from RGB-D Data in Indoor Environments</strong>, 3DV, 2017<br>
Chang, Angel, Angela, Dai, Thomas, Funkhouser, Maciej, Halber, Matthias, Niebner, Manolis, Savva, Shuran, Song, Andy, Zeng, Yinda, Zhang.<br>
[<a href="https://arxiv.org/pdf/1709.06158" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>AI2-THOR: An Interactive 3D Environment for Visual AI</strong>. arXiv, 2017<br>
Kolve, Eric, Roozbeh, Mottaghi, Daniel, Gordon, Yuke, Zhu, Abhinav, Gupta, Ali, Farhadi.<br>
[<a href="https://arxiv.org/pdf/1712.05474" rel="nofollow">page</a>]</p>
</li>
</ul>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><a id="user-content-perception">  Embodied Perception </a><a href="#table-of-contents"></a> </h2><a id="user-content---embodied-perception--" class="anchor" aria-label="Permalink:   Embodied Perception " href="#--embodied-perception--"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">Active Visual Exploration</h3><a id="user-content-active-visual-exploration" class="anchor" aria-label="Permalink: Active Visual Exploration" href="#active-visual-exploration"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>
<p dir="auto"><strong>RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics</strong>, arxiv, 2025.<br>
Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, Shanghang Zhang.<br>
[<a href="https://arxiv.org/abs/2506.04308" rel="nofollow">Paper</a>] [<a href="https://zhoues.github.io/RoboRefer/" rel="nofollow">Project</a>]</p>
</li>
<li>
<p dir="auto"><strong>3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians</strong>, arxiv, 2025.<br>
Zeming Wei, Junyi Lin, Yang Liu, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin.<br>
[<a href="https://arxiv.org/pdf/2504.11218" rel="nofollow">Paper</a>] [<a href="https://github.com/HCPLab-SYSU/3DAffordSplat">Project</a>]</p>
</li>
<li>
<p dir="auto"><strong>Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection</strong>, CVPR, 2025.<br>
Enshen Zhou, Qi Su, Cheng Chi, Zhizheng Zhang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, He Wang.<br>
[<a href="https://arxiv.org/abs/2412.04455" rel="nofollow">Paper</a>] [<a href="https://zhoues.github.io/Code-as-Monitor/" rel="nofollow">Project</a>]</p>
</li>
<li>
<p dir="auto"><strong>SnapMem: Snapshot-based 3D Scene Memory for Embodied Exploration and Reasoning</strong>, arxiv, 2024.<br>
Yuncong Yang, Han Yang, Jiachen Zhou, Peihao Chen, Hongxin Zhang, Yilun Du, Chuang Gan.<br>
[<a href="https://arxiv.org/pdf/2411.17735" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>AIR-Embodied: An Efficient Active 3DGS-based Interaction and Reconstruction Framework with Embodied Large Language Model</strong>, arxiv, 2024.<br>
Zhenghao Qi, Shenghai Yuan, Fen Liu, Haozhi Cao, Tianchen Deng, Jianfei Yang, Lihua Xie.<br>
[<a href="https://arxiv.org/pdf/2409.16019" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation</strong>, CVPR, 2024.<br>
Yunhao Ge, Yihe Tang, Jiashu Xu, Cem Gokmen, Chengshu Li, Wensi Ai, Benjamin Jose Martinez, Arman Aydin, Mona Anvari, Ayush K Chakravarthy, Hong-Xing Yu, Josiah Wong, Sanjana Srivastava, Sharon Lee, Shengxin Zha, Laurent Itti, Yunzhu Li, Roberto Martn-Martn, Miao Liu, Pengchuan Zhang, Ruohan Zhang, Li Fei-Fei, Jiajun Wu.<br>
[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ge_BEHAVIOR_Vision_Suite_Customizable_Dataset_Generation_via_Simulation_CVPR_2024_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Coarse-to-Fine Detection of Multiple Seams for Robotic Welding</strong>, arxiv, 2024.<br>
Pengkun Wei, Shuo Cheng, Dayou Li, Ran Song, Yipeng Zhang, Wei Zhang.<br>
[<a href="https://arxiv.org/pdf/2408.10710" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Evidential Active Recognition: Intelligent and Prudent Open-World Embodied Perception</strong>, CVPR, 2024.<br>
Fan, Lei, Mingfu, Liang, Yunxuan, Li, Gang, Hua, Ying, Wu.<br>
[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Fan_Evidential_Active_Recognition_Intelligent_and_Prudent_Open-World_Embodied_Perception_CVPR_2024_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>SpatialBot: Precise Spatial Understanding with Vision Language Models</strong>, arxiv, 2024.<br>
Wenxiao Cai, Yaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, Bo Zhao.<br>
[<a href="https://arxiv.org/pdf/2406.13642" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Embodied Uncertainty-Aware Object Segmentations</strong>, IROS, 2024.<br>
Xiaolin Fang, Leslie Pack Kaelbling, Tom as Lozano-P erez.<br>
[<a href="https://arxiv.org/pdf/2408.04760" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Point Transformer V3: Simpler Faster Stronger</strong>, CVPR, 2024.
Wu, Xiaoyang, Li, Jiang, Peng-Shuai, Wang, Zhijian, Liu, Xihui, Liu, Yu, Qiao, Wanli, Ouyang, Tong, He, Hengshuang, Zhao.<br>
[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Point_Transformer_V3_Simpler_Faster_Stronger_CVPR_2024_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>PointMamba: A Simple State Space Model for Point Cloud Analysis</strong>, arXiv, 2024.<br>
Liang, Dingkang, Xin, Zhou, Xinyu, Wang, Xingkui, Zhu, Wei, Xu, Zhikang, Zou, Xiaoqing, Ye, Xiang, Bai.<br>
[<a href="https://arxiv.org/pdf/2402.10739" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Point Could Mamba: Point Cloud Learning via State Space Model</strong>, arXiv, 2024.<br>
Zhang, Tao, Xiangtai, Li, Haobo, Yuan, Shunping, Ji, Shuicheng, Yan.<br>
[<a href="https://arxiv.org/pdf/2403.00762" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Mamba3d: Enhancing local features for 3d point cloud analysis via state space model</strong>, arXiv, 2024.<br>
Han, Xu, Yuan, Tang, Zhaoxuan, Wang, Xianzhi, Li.<br>
[<a href="https://arxiv.org/pdf/2404.14966" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Gs-slam: Dense visual slam with 3d gaussian splatting</strong>, CVPR, 2024.<br>
Yan, Chi, Delin, Qu, Dan, Xu, Bin, Zhao, Zhigang, Wang, Dong, Wang, Xuelong, Li.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_GS-SLAM_Dense_Visual_SLAM_with_3D_Gaussian_Splatting_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>GOReloc: Graph-based Object-Level Relocalization for Visual SLAM</strong>, IEEE RAL, 2024.<br>
Yutong Wang, Chaoyang Jiang, Xieyuanli Chen.<br>
<a href="https://arxiv.org/pdf/2408.07917" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai</strong> CVPR, 2024.<br>
Wang, Tai, Xiaohan, Mao, Chenming, Zhu, Runsen, Xu, Ruiyuan, Lyu, Peisen, Li, Xiao, Chen, Wenwei, Zhang, Kai, Chen, Tianfan, Xue, others.<br>
[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_EmbodiedScan_A_Holistic_Multi-Modal_3D_Perception_Suite_Towards_Embodied_AI_CVPR_2024_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Neu-nbv: Next best view planning using uncertainty estimation in image-based neural rendering</strong>, IROS, 2023.<br>
Jin, Liren, Xieyuanli, Chen, Julius, Rckin, Marija, Popovi'c.<br>
[<a href="https://arxiv.org/pdf/2303.01284" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Off-policy evaluation with online adaptation for robot exploration in challenging environments</strong>, IEEE Robotics and Automation Letters, 2023.<br>
Hu, Yafei, Junyi, Geng, Chen, Wang, John, Keller, Sebastian, Scherer.<br>
[<a href="https://arxiv.org/pdf/2204.03140" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>OVD-SLAM: An online visual SLAM for dynamic environments</strong>, IEEE Sensors Journal, 2023.<br>
He, Jiaming, Mingrui, Li, Yangyang, Wang, Hongyu, Wang.<br>
<a href="https://ieeexplore.ieee.org/abstract/document/10113832" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Transferring implicit knowledge of non-visual object properties across heterogeneous robot morphologies</strong>, ICRA, 2023.<br>
Tatiya, Gyan, Jonathan, Francis, Jivko, Sinapov.<br>
[<a href="https://arxiv.org/pdf/2209.06890" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Swin3d: A pretrained transformer backbone for 3d indoor scene understanding</strong>, arXiv, 2023.<br>
Yang, Yu-Qi, Yu-Xiao, Guo, Jian-Yu, Xiong, Yang, Liu, Hao, Pan, Peng-Shuai, Wang, Xin, Tong, Baining, Guo.<br>
[<a href="https://arxiv.org/pdf/2304.06906" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Point transformer v2: Grouped vector attention and partition-based pooling</strong>, NeurIPS, 2022.<br>
Wu, Xiaoyang, Yixing, Lao, Li, Jiang, Xihui, Liu, Hengshuang, Zhao.<br>
[<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/d78ece6613953f46501b958b7bb4582f-Paper-Conference.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Rethinking network design and local geometry in point cloud: A simple residual MLP framework</strong>, arXiv, 2022.
Ma, Xu, Can, Qin, Haoxuan, You, Haoxi, Ran, Yun, Fu.
[<a href="https://arxiv.org/pdf/2202.07123" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>So-slam: Semantic object slam with scale proportional and symmetrical texture constraints</strong>. IEEE Robotics and Automation Letters 7. 2(2022): 40084015.<br>
Liao, Ziwei, Yutong, Hu, Jiadong, Zhang, Xianyu, Qi, Xiaoyu, Zhang, Wei, Wang.<br>
<a href="https://ieeexplore.ieee.org/abstract/document/9705562" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>SG-SLAM: A real-time RGB-D visual SLAM toward dynamic scenes with semantic and geometric information</strong>, IEEE Transactions on Instrumentation and Measurement 72. (2022): 112.<br>
Cheng, Shuhong, Changhe, Sun, Shun, Zhang, Dianfan, Zhang.<br>
<a href="https://ieeexplore.ieee.org/abstract/document/9978699" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Point transformer</strong>, ICCV, 2021.
Zhao, Hengshuang, Li, Jiang, Jiaya, Jia, Philip HS, Torr, Vladlen, Koltun.<br>
[<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Point_Transformer_ICCV_2021_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Pointpillars: Fast encoders for object detection from point clouds</strong>, CVPR, 2019.<br>
Lang, Alex H, Sourabh, Vora, Holger, Caesar, Lubing, Zhou, Jiong, Yang, Oscar, Beijbom.<br>
<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Lang_PointPillars_Fast_Encoders_for_Object_Detection_From_Point_Clouds_CVPR_2019_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>4d spatio-temporal convnets: Minkowski convolutional neural networks</strong>, CVPR, 2019.<br>
Choy, Christopher, JunYoung, Gwak, Silvio, Savarese.<br>
<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Choy_4D_Spatio-Temporal_ConvNets_Minkowski_Convolutional_Neural_Networks_CVPR_2019_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Cubeslam: Monocular 3-d object slam</strong>, IEEE T-RO 35. 4(2019): 925938<br>
Yang, Shichao, Sebastian, Scherer.<br>
<a href="https://ieeexplore.ieee.org/abstract/document/8708251" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Hierarchical topic model based object association for semantic SLAM</strong>, IEEE T-VCG 25. 11(2019): 30523062<br>
Zhang, Jianhua, Mengping, Gui, Qichao, Wang, Ruyu, Liu, Junzhe, Xu, Shengyong, Chen.<br>
<a href="https://ieeexplore.ieee.org/abstract/document/8794595" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>DS-SLAM: A semantic visual SLAM towards dynamic environments</strong>, IROS, 2018<br>
Yu, Chao, Zuxin, Liu, Xin-Jun, Liu, Fugui, Xie, Yi, Yang, Qi, Wei, Qiao, Fei.<br>
<a href="https://ieeexplore.ieee.org/abstract/document/8593691" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>DynaSLAM: Tracking, mapping, and inpainting in dynamic scenes</strong>, IEEE Robotics and Automation Letters 3. 4(2018): 40764083<br>
Bescos, Berta, Jos M, Facil, Javier, Civera, Jos, Neira.<br>
<a href="https://ieeexplore.ieee.org/abstract/document/8421015" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Quadricslam: Dual quadrics from object detections as landmarks in object-oriented slam</strong>, IEEE Robotics and Automation Letters 4. 1(2018): 18.<br>
Nicholson, Lachlan, Michael, Milford, Niko, Snderhauf.<br>
<a href="https://ieeexplore.ieee.org/abstract/document/8440105" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>3d semantic segmentation with submanifold sparse convolutional networks</strong>, CVPR, 2018.<br>
Graham, Benjamin, Martin, Engelcke, Laurens, Van Der Maaten.<br>
[<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Graham_3D_Semantic_Segmentation_CVPR_2018_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Learning to look around: Intelligently exploring unseen environments for unknown tasks</strong>, CVPR, 2018.<br>
Jayaraman, Dinesh, Kristen, Grauman.<br>
[<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Jayaraman_Learning_to_Look_CVPR_2018_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Multi-view 3d object detection network for autonomous driving</strong>, CVPR, 2017.<br>
Chen, Xiaozhi, Huimin, Ma, Ji, Wan, Bo, Li, Tian, Xia.<br>
<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Multi-View_3D_Object_CVPR_2017_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Semantic scene completion from a single depth image</strong>, CVPR, 2017.<br>
Song, Shuran, Fisher, Yu, Andy, Zeng, Angel X, Chang, Manolis, Savva, Thomas, Funkhouser.<br>
<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Song_Semantic_Scene_Completion_CVPR_2017_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Pointnet: Deep learning on point sets for 3d classification and segmentation</strong>, CVPR, 2017.<br>
Qi, Charles R, Hao, Su, Kaichun, Mo, Leonidas J, Guibas.<br>
[[page](Pointnet: Deep learning on point sets for 3d classification and segmentation)]</p>
</li>
<li>
<p dir="auto"><strong>Pointnet++: Deep hierarchical feature learning on point sets in a metric space</strong>, NeurIPS, 2017.<br>
Qi, Charles Ruizhongtai, Li, Yi, Hao, Su, Leonidas J, Guibas.<br>
[<a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>The curious robot: Learning visual representations via physical interactions</strong>, ECCV, 2016.<br>
Pinto, Lerrel, Dhiraj, Gandhi, Yuanfeng, Han, Yong-Lae, Park, Abhinav, Gupta.<br>
[<a href="https://arxiv.org/pdf/1604.01360" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Multi-view convolutional neural networks for 3d shape recognition</strong>, ICCV, 2015.<br>
Su, Hang, Subhransu, Maji, Evangelos, Kalogerakis, Erik, Learned-Miller.<br>
<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Su_Multi-View_Convolutional_Neural_ICCV_2015_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Voxnet: A 3d convolutional neural network for real-time object recognition</strong>, IROS, 2015.<br>
Maturana, Daniel, Sebastian, Scherer.<br>
<a href="https://ieeexplore.ieee.org/abstract/document/7353481" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>ORB-SLAM: a versatile and accurate monocular SLAM system</strong> IEEE T-RO 31. 5(2015): 11471163<br>
Mur-Artal, Raul, Jose Maria Martinez, Montiel, Juan D, Tardos.<br>
<a href="https://ieeexplore.ieee.org/abstract/document/7219438/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>LSD-SLAM: Large-scale direct monocular SLAM</strong>, ECCV, 2014<br>
Engel, Jakob, Thomas, Schops, Daniel, Cremers.<br>
<a href="https://link.springer.com/chapter/10.1007/978-3-319-10605-2_54" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Slam++: Simultaneous localisation and mapping at the level of objects</strong>, CVPR, 2013<br>
Salas-Moreno, Renato F, Richard A, Newcombe, Hauke, Strasdat, Paul HJ, Kelly, Andrew J, Davison.<br>
<a href="https://openaccess.thecvf.com/content_cvpr_2013/papers/Salas-Moreno_SLAM_Simultaneous_Localisation_2013_CVPR_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>DTAM: Dense tracking and mapping in real-time</strong>, ICCV, 2011<br>
Newcombe, Richard A, Steven J, Lovegrove, Andrew J, Davison.<br>
<a href="https://ieeexplore.ieee.org/abstract/document/6126513/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>MonoSLAM: Real-time single camera SLAM</strong>, IEEE T-PAMI, 2007.<br>
Davison, Andrew J, Ian D, Reid, Nicholas D, Molton, Olivier, Stasse.<br>
<a href="http://www.doc.ic.ac.uk/~ajd/Publications/davison_etal_pami2007.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>A multi-state constraint Kalman filter for vision-aided inertial navigation</strong>, IROS, 2007<br>
Mourikis, Anastasios I, Stergios I, Roumeliotis.<br>
<a href="https://intra.engr.ucr.edu/~mourikis/tech_reports/TR_MSCKF.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Parallel tracking and mapping for small AR workspaces</strong>, ISMAR, 2007<br>
Klein, Georg, David, Murray.<br>
<a href="https://ieeexplore.ieee.org/abstract/document/4538852/" rel="nofollow">[page]</a></p>
</li>
</ul>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">3D Visual Perception and Grounding</h3><a id="user-content-3d-visual-perception-and-grounding" class="anchor" aria-label="Permalink: 3D Visual Perception and Grounding" href="#3d-visual-perception-and-grounding"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>
<p dir="auto"><strong>UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation</strong>, ICRA, 2025
Yihe Tang, Wenlong Huang, Yingke Wang, Chengshu Li, Roy Yuan, Ruohan Zhang, Jiajun Wu, Li Fei-Fei<br>
<a href="https://openreview.net/pdf?id=an953WOpo2" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions</strong>, arxiv, 2025<br>
He Zhu, Quyu Kong, Kechun Xu, Xunlong Xia, Bing Deng, Jieping Ye, Rong Xiong, Yue Wang<br>
<a href="https://arxiv.org/pdf/2504.04744" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds</strong>, arxiv, 2025<br>
Hengshuo Chu, Xiang Deng, Qi Lv, Xiaoyang Chen, Yinchuan Li, Jianye Hao, Liqiang Nie<br>
<a href="https://arxiv.org/pdf/2502.20041" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>SeqAfford: Sequential 3D affordance reasoning via Multimodal Large Language Model</strong>, CVPR, 2025<br>
Hanqing Wang, Chunlin Yu, Haoyang Luo, Jingyi Yu, Ye Shi, Jingya Wang<br>
<a href="https://arxiv.org/pdf/2412.01550" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency</strong>, CVPR, 2025<br>
Dongyue Lu, Lingdong Kong, Tianxin Huang, Gim Hee Lee<br>
<a href="https://arxiv.org/pdf/2412.09511" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding</strong>, arxiv, 2024<br>
Yawen Shao, Wei Zhai, Yuhang Yang, Hongchen Luo, Yang Cao, Zheng-Jun Zha, CVPR, 2025
<a href="https://arxiv.org/pdf/2411.19626" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>LASO: Language-guided affordance segmentation on 3d object</strong>, CVPR, 2024<br>
Yicong Li, Na Zhao, Junbin Xiao, Chun Feng, Xiang Wang, Tat-seng Chua<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_LASO_Language-guided_Affordance_Segmentation_on_3D_Object_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>SceneFun3D: fine-grained functionality and affordance understanding in 3D scenes</strong>, CVPR, 2024<br>
Alexandros Delitzas, Ayca Takmaz, Federico Tombari, Robert Sumner, Marc Pollefeys, Francis Engelmann<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Delitzas_SceneFun3D_Fine-Grained_Functionality_and_Affordance_Understanding_in_3D_Scenes_CVPR_2024_paper.html" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Language-conditioned affordance-pose detection in 3d point clouds</strong>, ICRA, 2024<br>
Toan Nguyen, Minh Nhat Vu, Baoru Huang, Tuan Van Vo, Vy Truong, Ngan Le, Thieu Vo, Bac Le, Anh Nguyen<br>
<a href="https://arxiv.org/pdf/2309.10911" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering</strong>, CVPR, 2025<br>
Jingzhou Luo, Yang Liu, Weixing Chen, Zhen Li, Yaowei Wang, Guanbin Li, Liang Lin<br>
<a href="https://arxiv.org/pdf/2503.03190" rel="nofollow">[page]</a><a href="https://github.com/LZ-CH/DSPNet">project</a></p>
</li>
<li>
<p dir="auto"><strong>Learning 2D Invariant Affordance Knowledge for 3D Affordance Grounding</strong>, arxiv, 2024<br>
Xianqiang Gao, Pingrui Zhang, Delin Qu, Dong Wang, Zhigang Wang, Yan Ding, Bin Zhao, Xuelong Li<br>
<a href="https://arxiv.org/pdf/2408.13024" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>EmbodiedSAM: Online Segment Any 3D Thing in Real Time</strong>, arxiv, 2024<br>
Xiuwei Xu, Huangxing Chen, Linqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu<br>
<a href="https://arxiv.org/pdf/2408.11811" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>OpenScan: A Benchmark for Generalized Open-Vocabulary 3D Scene Understanding</strong>, arxiv, 2024<br>
Youjun Zhao, Jiaying Lin, Shuquan Ye, Qianshi Pang, Rynson W.H. Lau<br>
<a href="https://arxiv.org/pdf/2408.11030" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>LLMI3D: Empowering LLM with 3D Perception from a Single 2D Image</strong>, arxiv, 2024<br>
Fan Yang, Sicheng Zhao, Yanhao Zhang, Haoxiang Chen, Hui Chen, Wenbo Tang, Haonan Lu, Pengfei Xu, Zhenyu Yang, Jungong Han, Guiguang Ding<br>
<a href="https://arxiv.org/pdf/2408.07422" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations</strong>, arxiv, 2024<br>
Ruiyuan Lyu, Tai Wang, Jingli Lin, Shuai Yang, Xiaohan Mao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu, Dahua Lin, Jiangmiao Pang<br>
<a href="https://arxiv.org/pdf/2406.09401" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>ShapeLLM: Universal 3D Object Understanding for Embodied Interaction</strong>, arxiv, 2024<br>
Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, He Wang, Li Yi, Kaisheng Ma<br>
<a href="https://qizekun.github.io/shapellm/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>LEO: An Embodied Generalist Agent in 3D World</strong>, ICML, 2024<br>
Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang<br>
<a href="https://embodied-generalist.github.io/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding</strong>, ECCV, 2024<br>
Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, and Siyuan Huang<br>
<a href="https://scene-verse.github.io/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>PQ3D: Unifying 3D Vision-Language Understanding via Promptable Queries</strong>, ECCV, 2024<br>
Ziyu Zhu, Zhuofan Zhang, Xiaojian Ma, Xuesong Niu, Yixin Chen, Baoxiong Jia, Zhidong Deng, Siyuan Huang, and Qing Li<br>
<a href="https://3d-vista.github.io/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World</strong>, CVPR, 2024<br>
Yining Hong, Zishuo Zheng, Peihao Chen, Yian Wang, Junyan Li, Chuang Gan<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hong_MultiPLY_A_Multisensory_Object-Centric_Embodied_Large_Language_Model_in_3D_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception</strong>, CVPR, 2024<br>
Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, Jing Shao<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Qin_MP5_A_Multi-modal_Open-ended_Embodied_System_in_Minecraft_via_Active_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>MaskClustering: View Consensus based Mask Graph Clustering for Open-Vocabulary 3D Instance Segmentation</strong>, CVPR, 2024<br>
Mi Yan, Jiazhao Zhang, Yan Zhu, He Wang<br>
<a href="https://arxiv.org/pdf/2401.07745" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding</strong>, CVPR, 2024<br>
Yun Liu, Haolin Yang, Xu Si, Ling Liu, Zipeng Li, Yuxiang Zhang, Yebin Liu, Li Yi<br>
<a href="https://taco2024.github.io/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding</strong>, CVPR, 2023<br>
Wu, Yanmin and Cheng, Xinhua and Zhang, Renrui and Cheng, Zesen and Zhang, Jian<br>
<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_EDA_Explicit_Text-Decoupling_and_Dense_Alignment_for_3D_Visual_Grounding_CVPR_2023_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Affordpose: A large-scale dataset of hand-object interactions with affordance-driven hand pose</strong>, ICCV, 2023<br>
Juntao Jian, Xiuping Liu, Manyi Li, Ruizhen Hu, Jian Liu<br>
<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jian_AffordPose_A_Large-Scale_Dataset_of_Hand-Object_Interactions_with_Affordance-Driven_Hand_ICCV_2023_paper.html" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Grounding 3d object affordance from 2d interactions in images</strong>, ICCV, 2023<br>
Yuhang Yang, Wei Zhai, Hongchen Luo, Yang Cao, Jiebo Luo, Zheng-Jun Zha<br>
<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Grounding_3D_Object_Affordance_from_2D_Interactions_in_Images_ICCV_2023_paper.html" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>3d-vista: Pre-trained transformer for 3d vision and text alignment</strong>, ICCV, 2023<br>
Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li<br>
<a href="https://3d-vista.github.io/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>LeaF: Learning Frames for 4D Point Cloud Sequence Understanding</strong>, ICCV, 2023<br>
Yunze Liu, Junyu Chen, Zekai Zhang, Li Yi<br>
<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_LeaF_Learning_Frames_for_4D_Point_Cloud_Sequence_Understanding_ICCV_2023_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>SQA3D: Situated Question Answering in 3D Scenes</strong>, ICLR, 2023<br>
Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang<br>
[page]</p>
</li>
<li>
<p dir="auto"><strong>LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent</strong>, arXix, 2023<br>
Yang, Jianing and Chen, Xuweiyi and Qian, Shengyi and Madaan, Nikhil and Iyengar, Madhavan and Fouhey, David F and Chai, Joyce<br>
<a href="https://arxiv.org/pdf/2309.12311" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding</strong>, arXix, 2023<br>
Yuan, Zhihao and Ren, Jinke and Feng, Chun-Mei and Zhao, Hengshuang and Cui, Shuguang and Li, Zhen<br>
<a href="https://arxiv.org/pdf/2311.15383" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Multi-view transformer for 3D visual grounding</strong>, CVPR, 2022<br>
Huang, Shijia and Chen, Yilun and Jia, Jiaya and Wang, Liwei<br>
<a href="https://arxiv.org/pdf/2204.02174" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Look Around and Refer: 2D Synthetic Semantics Knowledge Distillation for 3D Visual Grounding</strong>, CVPR, 2022<br>
Bakr, Eslam and Alsaedy, Yasmeen and Elhoseiny, Mohamed<br>
<a href="https://arxiv.org/pdf/2211.14241" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection</strong>, CVPR, 2022<br>
Luo, Junyu and Fu, Jiahui and Kong, Xianghao and Gao, Chen and Ren, Haibing and Shen, Hao and Xia, Huaxia and Liu, Si<br>
<a href="https://arxiv.org/pdf/2204.06272" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds</strong>, ECCV, 2022<br>
Jain, Ayush and Gkanatsios, Nikolaos and Mediratta, Ishita and Fragkiadaki, Katerina<br>
<a href="https://arxiv.org/pdf/2112.08879" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>3d affordancenet: A benchmark for visual object affordance understanding</strong>, CVPR, 2021<br>
Shengheng Deng, Xun Xu, Chaozheng Wu, Ke Chen, Kui Jia<br>
<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Deng_3D_AffordanceNet_A_Benchmark_for_Visual_Object_Affordance_Understanding_CVPR_2021_paper.html" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Text-guided graph neural networks for referring 3D instance segmentation</strong>, AAAI, 2021<br>
Huang, Pin-Hao and Lee, Han-Hung and Chen, Hwann-Tzong and Liu, Tyng-Luh<br>
<a href="https://ojs.aaai.org/index.php/AAAI/article/view/16253/16060" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring</strong>, ICCV, 2021<br>
Yuan, Zhihao and Yan, Xu and Liao, Yinghong and Zhang, Ruimao and Wang, Sheng and Li, Zhen and Cui, Shuguang<br>
<a href="https://arxiv.org/pdf/2103.01128" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Free-form Description Guided 3D Visual Graph Network for Object Grounding in Point Cloud</strong>, CVPR, 2021<br>
Feng, Mingtao and Li, Zhen and Li, Qi and Zhang, Liang and Zhang, XiangDong and Zhu, Guangming and Zhang, Hui and Wang, Yaonan and Mian, Ajmal<br>
<a href="https://arxiv.org/pdf/2103.16381" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>SAT: 2D Semantics Assisted Training for 3D Visual Grounding</strong>, CVPR, 2021<br>
Yang, Zhengyuan and Zhang, Songyang and Wang, Liwei and Luo, Jiebo<br>
<a href="https://arxiv.org/pdf/2105.11450" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>LanguageRefer: Spatiallanguage model for 3D visual grounding</strong>, CVPR, 2021<br>
Roh, Junha and Desingh, Karthik and Farhadi, Ali and Fox, Dieter<br>
<a href="https://arxiv.org/pdf/2107.03438" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>3DVG-Transformer: Relation Modeling for Visual Grounding on Point Clouds</strong>, ICCV, 2021<br>
Zhao, Lichen and Cai, Daigang and Sheng, Lu and Xu, Dong<br>
<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_3DVG-Transformer_Relation_Modeling_for_Visual_Grounding_on_Point_Clouds_ICCV_2021_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>TransRefer3D: Entity-and-relation aware transformer for fine-grained 3D visual grounding</strong>, CVPR, 2021<br>
He, Dailan and Zhao, Yusheng and Luo, Junyu and Hui, Tianrui and Huang, Shaofei and Zhang, Aixi and Liu, Si
<a href="https://arxiv.org/pdf/2108.02388" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language</strong>, ECCV, 2020<br>
Chen, Dave Zhenyu and Chang, Angel X and Nie{\ss}ner, Matthias<br>
<a href="https://arxiv.org/pdf/1912.08830" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes</strong>, ECCV, 2020<br>
Achlioptas, Panos and Abdelreheem, Ahmed and Xia, Fei and Elhoseiny, Mohamed and Guibas, Leonidas<br>
<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460409.pdf" rel="nofollow">[page]</a></p>
</li>
</ul>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">Visual Language Navigation</h3><a id="user-content-visual-language-navigation" class="anchor" aria-label="Permalink: Visual Language Navigation" href="#visual-language-navigation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>
<p dir="auto"><strong>Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method</strong>, CVPR, 2025.<br>
Xinshuai Song, Weixing Chen, Yang Liu, Weikai Chen, Guanbin Li, Liang Lin.<br>
[<a href="https://arxiv.org/pdf/2412.09082" rel="nofollow">page</a>]<a href="https://hcplab-sysu.github.io/LH-VLN/" rel="nofollow">project</a></p>
</li>
<li>
<p dir="auto"><strong>DivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes and Objects</strong>, arxiv, 2024.<br>
Zhaowei Wang, Hongming Zhang, Tianqing Fang, Ye Tian, Yue Yang, Kaixin Ma, Xiaoman Pan, Yangqiu Song, Dong Yu.<br>
[<a href="https://arxiv.org/abs/2410.02730" rel="nofollow">Paper</a>] [<a href="https://zhaowei-wang-nlp.github.io/divscene-project-page/" rel="nofollow">Project</a>]</p>
</li>
<li>
<p dir="auto"><strong>MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation</strong>, ACL, 2024.<br>
Jiaqi Chen, Bingqian Lin, Ran Xu, Zhenhua Chai, Xiaodan Liang, Kwan-Yee K. Wong.<br>
[<a href="https://chen-judge.github.io/MapGPT/" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning</strong>, ArXiv, 2024.<br>
Bingqian Lin, Yunshuang Nie, Ziming Wei, Jiaqi Chen, Shikui Ma, Jianhua Han, Hang Xu, Xiaojun Chang, Xiaodan Liang.<br>
[<a href="https://arxiv.org/abs/2403.07376" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>OMEGA: Efficient Occlusion-Aware Navigation for Air-Ground Robot in Dynamic Environments via State Space Model</strong>, ArXiv, 2024.<br>
Junming Wang, Dong Huang, Xiuxian Guan, Zekai Sun, Tianxiang Shen, Fangming Liu, Heming Cui.<br>
[<a href="https://arxiv.org/pdf/2408.10618" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving</strong>, ArXiv, 2024.<br>
Hidehisa Arai, Keita Miwa, Kento Sasaki, Yu Yamaguchi, Kohei Watanabe, Shunsuke Aoki, Issei Yamamoto.<br>
[<a href="https://arxiv.org/pdf/2408.10845" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>FLAME: Learning to Navigate with Multimodal LLM in Urban Environments</strong>, ArXiv, 2024.<br>
Yunzhe Xu, Yiyuan Pan, Zhe Liu, Hesheng Wang.<br>
[<a href="https://arxiv.org/pdf/2408.11051" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation</strong>, ArXiv, 2024.<br>
Jiaqi Chen, Bingqian Lin, Xinmin Liu, Xiaodan Liang, Kwan-Yee K Wong.<br>
[<a href="https://arxiv.org/pdf/2407.05890" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Embodied Instruction Following in Unknown Environments</strong>, ArXiv, 2024.<br>
Wu, Wang, Xu, Lu, Yan.<br>
[<a href="https://arxiv.org/pdf/2406.11818" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>DISCO: Embodied Navigation and Interaction via Differentiable Scene Semantics and Dual-level Control</strong>, arxiv, 2024.<br>
Xinyu Xu, Shengcheng Luo, Yanchao Yang, Yong-Lu Li, Cewu Lu.<br>
<a href="https://arxiv.org/abs/2407.14758" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>NOLO: Navigate Only Look Once</strong>, arxiv, 2024.<br>
Bohan Zhou, Jiangxing Wang, Zongqing Lu.<br>
<a href="https://arxiv.org/pdf/2408.01384" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Towards Learning a Generalist Model for Embodied Navigation</strong>, CVPR, 2024.<br>
Duo Zheng, , Shijia Huang, Lin Zhao, Yiwu Zhong, Liwei Wang.<br>
[<a href="https://arxiv.org/pdf/2312.02010" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Fast-Slow Test-time Adaptation for Online Vision-and-Language Navigation</strong> ICML, 2024.<br>
Junyu Gao, , Xuan Yao, Changsheng Xu.<br>
[<a href="https://arxiv.org/pdf/2311.13209" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Discuss before moving: Visual language navigation via multi-expert discussions</strong>, ICRA, 2024.<br>
Long, Yuxing, Xiaoqi, Li, Wenzhe, Cai, Hao, Dong.<br>
[<a href="https://arxiv.org/pdf/2309.11382" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Vision-and-Language Navigation via Causal Learning</strong>, CVPR, 2024.<br>
Liuyi Wang, Qijun Chen.<br>
[<a href="https://arxiv.org/pdf/2404.10241" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Volumetric Environment Representation for Vision-Language Navigation</strong>, CVPR, 2024.<br>
Rui Liu, Yi Yang.<br>
[<a href="https://arxiv.org/pdf/2403.14158" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation</strong>, CVPR 2024.<br>
Wang, Zihan, Xiangyang, Li, Jiahao, Yang, Yeqi, Liu, Junjie, Hu, Ming, Jiang, Shuqiang, Jiang.
[<a href="https://arxiv.org/pdf/2404.01943" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Bridging zero-shot object navigation and foundation models through pixel-guided navigation skill</strong> ICRA, 2024.<br>
Wenzhe Cai, Siyuan Huang, Guangran Cheng, Yuxing Long, Peng Gao, Changyin Sun, and Hao Dong.<br>
<a href="https://github.com/wzcai99/Pixel-Navigator">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>OVER-NAV: Elevating Iterative Vision-and-Language Navigation with Open-Vocabulary Detection and StructurEd Representation</strong>, CVPR, 2024.<br>
Ganlong Zhao, Guanbin Li, Weikai Chen, Yizhou Yu.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_OVER-NAV_Elevating_Iterative_Vision-and-Language_Navigation_with_Open-Vocabulary_Detection_and_StructurEd_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>RILA: Reflective and Imaginative Language Agent for Zero-Shot Semantic Audio-Visual Navigation</strong>, CVPR, 2024.<br>
Zeyuan Yang, Jiageng Liu, Peihao Chen, Anoop Cherian, Tim K. Marks, Jonathan Le Roux, Chuang Gan.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_RILA_Reflective_and_Imaginative_Language_Agent_for_Zero-Shot_Semantic_Audio-Visual_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Towards Learning a Generalist Model for Embodied Navigation</strong>, CVPR, 2024.<br>
Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, Liwei Wang.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_Towards_Learning_a_Generalist_Model_for_Embodied_Navigation_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Vision-and-Language Navigation via Causal Learning</strong>, CVPR, 2024.<br>
Liuyi Wang, Zongtao He, Ronghao Dang, Mengjiao Shen, Chengju Liu, Qijun Chen.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Vision-and-Language_Navigation_via_Causal_Learning_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Instance-aware Exploration-Verification-Exploitation for Instance ImageGoal Navigation</strong>, CVPR, 2024.<br>
Xiaohan Lei, Min Wang, Wengang Zhou, Li Li, Houqiang Li.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lei_Instance-aware_Exploration-Verification-Exploitation_for_Instance_ImageGoal_Navigation_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation</strong>, CVPR, 2024.<br>
Mukul Khanna, Yongsen Mao, Hanxiao Jiang, Sanjay Haresh, Brennan Shacklett, Dhruv Batra, Alexander Clegg, Eric Undersander, Angel X. Chang, Manolis Savva.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Khanna_Habitat_Synthetic_Scenes_Dataset_HSSD-200_An_Analysis_of_3D_Scene_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>SchurVINS: Schur Complement-Based Lightweight Visual Inertial Navigation System</strong>, CVPR, 2024.<br>
Yunfei Fan, Tianyu Zhao, Guidong Wang.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Fan_SchurVINS_Schur_Complement-Based_Lightweight_Visual_Inertial_Navigation_System_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>SPOC: Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World</strong>, CVPR, 2024.<br>
Kiana Ehsani, Tanmay Gupta, Rose Hendrix, Jordi Salvador, Luca Weihs, Kuo-Hao Zeng, Kunal Pratap Singh, Yejin Kim, Winson Han, Alvaro Herrasti, Ranjay Krishna, Dustin Schwenk, Eli VanderBilt, Aniruddha Kembhavi.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ehsani_SPOC_Imitating_Shortest_Paths_in_Simulation_Enables_Effective_Navigation_and_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Volumetric Environment Representation for Vision-Language Navigation</strong>, CVPR, 2024.<br>
Rui Liu, Wenguan Wang, Yi Yang.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Volumetric_Environment_Representation_for_Vision-Language_Navigation_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation</strong>, CVPR, 2024.<br>
Xiaohan Wang, Yuehu Liu, Xinhang Song, Yuyi Liu, Sixian Zhang, Shuqiang Jiang.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Khanna_GOAT-Bench_A_Benchmark_for_Multi-Modal_Lifelong_Navigation_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>An Interactive Navigation Method with Effect-oriented Affordance</strong>, CVPR, 2024.<br>
Xiaohan Wang, Yuehu Liu, Xinhang Song, Yuyi Liu, Sixian Zhang, Shuqiang Jiang.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_An_Interactive_Navigation_Method_with_Effect-oriented_Affordance_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Imagine Before Go: Self-Supervised Generative Map for Object Goal Navigation</strong>, CVPR, 2024.<br>
Sixian Zhang, Xinyao Yu, Xinhang Song, Xiaohan Wang, Shuqiang Jiang.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Imagine_Before_Go_Self-Supervised_Generative_Map_for_Object_Goal_Navigation_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>MemoNav: Working Memory Model for Visual Navigation</strong>, CVPR, 2024.<br>
Hongxin Li, Zeyu Wang, Xu Yang, Yuran Yang, Shuqi Mei, Zhaoxiang Zhang.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_MemoNav_Working_Memory_Model_for_Visual_Navigation_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Versatile Navigation Under Partial Observability via Value-guided Diffusion Policy</strong>, CVPR, 2024.<br>
Gengyu Zhang, Hao Tang, Yan Yan.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Versatile_Navigation_Under_Partial_Observability_via_Value-guided_Diffusion_Policy_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation</strong>, CVPR, 2024.<br>
Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, Junjie Hu, Ming Jiang, Shuqiang Jiang.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Lookahead_Exploration_with_Neural_Radiance_Representation_for_Continuous_Vision-Language_Navigation_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>SPIN: Simultaneous Perception Interaction and Navigation</strong>, CVPR, 2024.<br>
Shagun Uppal, Ananye Agarwal, Haoyu Xiong, Kenneth Shaw, Deepak Pathak.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Uppal_SPIN_Simultaneous_Perception_Interaction_and_Navigation_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Correctable Landmark Discovery via Large Models for Vision-Language Navigation</strong>, TPAMI, 2024.<br>
Bingqian Lin, Yunshuang Nie, Ziming Wei, Yi Zhu, Hang Xu, Shikui Ma, Jianzhuang Liu, Xiaodan Liang.<br>
<a href="https://ieeexplore.ieee.org/abstract/document/10543121" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments</strong>, IEEE T-PAMI, 2024.<br>
An, Dong, Hanqing, Wang, Wenguan, Wang, Zun, Wang, Yan, Huang, Keji, He, Liang, Wang.
[<a href="https://arxiv.org/pdf/2304.03047" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation</strong>, RSS, 2024.<br>
Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, He Wang.<br>
[<a href="https://arxiv.org/pdf/2402.15852" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>March in Chat: Interactive Prompting for Remote Embodied Referring Expression</strong>, ICCV, 2023.<br>
Qiao, Yanyuan, Yuankai, Qi, Zheng, Yu, Jing, Liu, Qi, Wu.<br>
[<a href="https://arxiv.org/pdf/2308.10141" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Multi-level compositional reasoning for interactive instruction following</strong>, AAAI, 2023.<br>
Bhambri, Suvaansh, Byeonghwi, Kim, Jonghyun, Choi.<br>
[<a href="https://arxiv.org/pdf/2308.09387" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Vision and Language Navigation in the Real World via Online Visual Language Mapping</strong>, ArXiv, 2023.<br>
Chengguang Xu, , Hieu T. Nguyen, Christopher Amato, Lawson L.S. Wong.
[<a href="https://arxiv.org/pdf/2310.10822" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Towards Deviation-robust Agent Navigation via Perturbation-aware Contrastive Learning</strong>, TPAMI, 2023.<br>
Bingqian Lin, Yanxin Long, Yi Zhu, Fengda Zhu, Xiaodan Liang , Qixiang Ye, Liang Lin.<br>
<a href="https://ieeexplore.ieee.org/abstract/document/10120966/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation</strong>, NIPS, 2023.<br>
Wang, Chen, Li, Wu, Dong.<br>
[<a href="https://arxiv.org/pdf/2309.08138" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>HomeRobot: Open-Vocabulary Mobile Manipulation</strong>, NIPS, 2023.<br>
Yenamandra, Sriram, Arun, Ramachandran, Karmesh, Yadav, Austin, Wang, Mukul, Khanna, Theophile, Gervet, Tsung-Yen, Yang, Vidhi, Jain, AlexanderWilliam, Clegg, John, Turner, Zsolt, Kira, Manolis, Savva, Angel, Chang, DevendraSingh, Chaplot, Dhruv, Batra, Roozbeh, Mottaghi, Yonatan, Bisk, Chris, Paxton.<br>
[<a href="https://arxiv.org/pdf/2306.11565" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation</strong>, Conference on Robot Learning. 2023.<br>
Li, Chengshu, Ruohan, Zhang, Josiah, Wong, Cem, Gokmen, Sanjana, Srivastava, Roberto, Mart\in-Mart'\in, Chen, Wang, Gabrael, Levine, Michael, Lingelbach, Jiankai, Sun, others.<br>
[<a href="https://arxiv.org/pdf/2403.09227" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following</strong>, arXiv, 2022.<br>
Gao, Xiaofeng, Qiaozi, Gao, Ran, Gong, Kaixiang, Lin, Govind, Thattai, GauravS., Sukhatme.<br>
[<a href="https://arxiv.org/pdf/2202.13330" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>HOP: History-and-Order Aware Pretraining for Vision-and-Language Navigation</strong>, CVPR, 2022.<br>
Qiao, Yanyuan, Yuankai, Qi, Yicong, Hong, Zheng, Yu, Peng, Wang, Qi, Wu.<br>
[<a href="https://arxiv.org/pdf/2203.11591" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation</strong>, CVPR, 2022.<br>
Hong, Yicong, Zun, Wang, Qi, Wu, Stephen, Gould.<br>
[<a href="https://arxiv.org/pdf/2203.02764" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>FILM: Following Instructions in Language with Modular Methods</strong>, ICLR, 2022.<br>
So Yeon Min, , Devendra Singh Chaplot, Pradeep Kumar Ravikumar, Yonatan Bisk, Ruslan Salakhutdinov.<br>
[<a href="https://arxiv.org/pdf/2110.07342" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action</strong>, Conference on Robot Learning. 2022.<br>
Dhruv Shah, , Blazej Osinski, Brian Ichter, Sergey Levine.<br>
[<a href="https://arxiv.org/pdf/2207.04429" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>SOON: Scenario Oriented Object Navigation with Graph-based Exploration</strong>, CVPR, 2021.<br>
Zhu, Fengda, Xiwen, Liang, Yi, Zhu, Qizhi, Yu, Xiaojun, Chang, Xiaodan, Liang.<br>
[<a href="https://arxiv.org/pdf/2103.17138" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Vision-Language Navigation Policy Learning and Adaptation</strong>, IEEE T-PAMI 43. 12(2021): 4205-4216.<br>
Wang, Xin, Qiuyuan, Huang, Asli, Celikyilmaz, Jianfeng, Gao, Dinghan, Shen, Yuan-Fang, Wang, William Yang, Wang, Lei, Zhang.<br>
[<a href="https://arxiv.org/pdf/https://ieeexplore.ieee.org/document/8986691" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Neighbor-view enhanced model for vision and language navigation</strong>, MM, 2021.<br>
An, Dong, Yuankai, Qi, Yan, Huang, Qi, Wu, Liang, Wang, Tieniu, Tan.<br>
[<a href="https://arxiv.org/pdf/2107.07201" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments</strong>, ECCV, 2020.<br>
Krantz, Jacob and Wijmans, Erik and Majumdar, Arjun and Batra, Dhruv and Lee, Stefan.<br>
[<a href="https://arxiv.org/pdf/2004.02857" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments</strong>, CVPR, 2020.<br>
Qi, Yuankai, Qi, Wu, Peter, Anderson, Xin, Wang, William Yang, Wang, Chunhua, Shen, Anton, Hengel.<br>
[<a href="https://arxiv.org/pdf/1904.10151" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks</strong>, CVPR, 2020.<br>
Shridhar, Mohit, Jesse, Thomason, Daniel, Gordon, Yonatan, Bisk, Winson, Han, Roozbeh, Mottaghi, Luke, Zettlemoyer, Dieter, Fox.<br>
[<a href="https://arxiv.org/pdf/1912.01734" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Vision-and-dialog navigation</strong>, Conference on Robot Learning. 2020.<br>
Thomason, Jesse, Michael, Murray, Maya, Cakmak, Luke, Zettlemoyer.<br>
[<a href="https://arxiv.org/pdf/1907.04957" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Language and visual entity relationship graph for agent navigation</strong>, NeurIPS, 2020.<br>
Hong, Yicong, Cristian, Rodriguez, Yuankai, Qi, Qi, Wu, Stephen, Gould.<br>
[<a href="https://arxiv.org/pdf/2010.09304" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Language-Guided Navigation via Cross-Modal Grounding and Alternate Adversarial Learning</strong>, IEEE T-CSVT 31. (2020): 3469-3481.<br>
Weixia Zhang, , Chao Ma, Qi Wu, Xiaokang Yang.<br>
[<a href="https://arxiv.org/pdf/2011.10972" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation</strong>, ACL, 2019.<br>
Jain, Vihan, Gabriel, Magalhaes, Alexander, Ku, Ashish, Vaswani, Eugene, Ie, Jason, Baldridge.<br>
[<a href="https://arxiv.org/pdf/1905.12255" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments</strong>, CVPR, 2019.<br>
Chen, Howard, Alane, Suhr, Dipendra, Misra, Noah, Snavely, Yoav, Artzi.<br>
[<a href="https://arxiv.org/pdf/1811.12354" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments</strong>, CVPR, 2018.<br>
Anderson, Peter, Qi, Wu, Damien, Teney, Jake, Bruce, Mark, Johnson, Niko, Sunderhauf, Ian, Reid, Stephen, Gould, Anton, Hengel.<br>
[<a href="https://arxiv.org/pdf/1711.07280" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation</strong>, ECCV, 2018.<br>
Xin Eric Wang, , Wenhan Xiong, Hongmin Wang, William Yang Wang.<br>
[<a href="https://arxiv.org/pdf/1803.07729" rel="nofollow">page</a>]</p>
</li>
</ul>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">Non-Visual Perception: Tactile</h3><a id="user-content-non-visual-perception-tactile" class="anchor" aria-label="Permalink: Non-Visual Perception: Tactile" href="#non-visual-perception-tactile"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>
<p dir="auto"><strong>When Vision Meets Touch: A Contemporary Review for Visuotactile Sensors from the Signal Processing Perspective</strong>, Arxiv, 2024.<br>
Li, Shoujie and Wang, Zihan and Wu, Changsheng and Li, Xiang and Luo, Shan and Fang, Bin and Sun, Fuchun and Zhang, Xiao-Ping and Ding, Wenbo.<br>
[<a href="https://arxiv.org/pdf/2406.12226" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Enhancing Generalizable 6D Pose Tracking of an In-Hand Object with Tactile Sensing</strong>, RA-L, 2024.<br>
Yun Liu, Xiaomeng Xu, Weihang Chen, Haocheng Yuan, He Wang, Jing Xu, Rui Chen, Li Yi.<br>
[<a href="https://arxiv.org/abs/2210.04026" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Learning visuotactile skills with two multifingered hands</strong>, ArXiv, 2024.<br>
Lin, Toru and Zhang, Yu and Li, Qiyang and Qi, Haozhi and Yi, Brent and Levine, Sergey and Malik, Jitendra.<br>
[<a href="https://arxiv.org/pdf/2404.16823" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Binding touch to everything: Learning unified multimodal tactile representations</strong>, CVPR, 2024.<br>
Yang, Fengyu and Feng, Chao and Chen, Ziyang and Park, Hyoungseob and Wang, Daniel and Dou, Yiming and Zeng, Ziyao and Chen, Xien and Gangopadhyay, Rit and Owens, Andrew and others.<br>
[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Binding_Touch_to_Everything_Learning_Unified_Multimodal_Tactile_Representations_CVPR_2024_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Bioinspired sensors and applications in intelligent robots: a review</strong>, Robotic Intelligence and Automation, 2024.<br>
Zhou, Yanmin and Yan, Zheng and Yang, Ye and Wang, Zhipeng and Lu, Ping and Yuan, Philip F and He, Bin.<br>
[<a href="https://www.emerald.com/insight/content/doi/10.1108/RIA-07-2023-0088/full/pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Give Me a Sign: Using Data Gloves for Static Hand-Shape Recognition</strong>, Sensors, 2023.<br>
Achenbach, Philipp and Laux, Sebastian and Purdack, Dennis and Mller, Philipp Niklas and Gbel, Stefan.<br>
[<a href="https://www.mdpi.com/1424-8220/23/24/9847/pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Semantics-aware adaptive knowledge distillation for sensor-to-vision action recognition</strong>, IEEE Transactions on Image Processing, 2021.<br>
Liu, Yang and Wang, Keze and Li, Guanbin and Lin, Liang.<br>
[<a href="https://arxiv.org/pdf/2009.00210" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Hand movements: A window into haptic object recognition</strong>, Cognitive psychology, 1987.<br>
Lederman, Susan J and Klatzky, Roberta L.<br>
[<a href="https://www.sciencedirect.com/science/article/pii/0010028587900089" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Force and tactile sensing</strong>, Springer Handbook of Robotics, 2016.<br>
Cutkosky, Mark R and Howe, Robert D and Provancher, William R.<br>
[<a href="https://link.springer.com/chapter/10.1007/978-3-319-32552-1_28" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Haptic perception: A tutorial</strong>, Attention, Perception, &amp; Psychophysics, 2009.<br>
Lederman, Susan J and Klatzky, Roberta L.<br>
[<a href="https://link.springer.com/content/pdf/10.3758/APP.71.7.1439.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Flexible tactile sensing based on piezoresistive composites: A review</strong>, Sensors, 2014.<br>
Stassi, Stefano and Cauda, Valentina and Canavese, Giancarlo and Pirri, Candido Fabrizio.<br>
[<a href="https://www.mdpi.com/1424-8220/14/3/5296." rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Tactile sensing in dexterous robot hands</strong>, Robotics and Autonomous Systems, 2015.<br>
Kappassov, Zhanat and Corrales, Juan-Antonio and Perdereau, Vronique.<br>
[<a href="https://uca.hal.science/hal-01680649/document" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>GelLink: A Compact Multi-phalanx Finger with Vision-based Tactile Sensing and Proprioception</strong>, arXiv, 2024.<br>
Ma, Yuxiang and Adelson, Edward.<br>
[<a href="https://arxiv.org/pdf/2403.14887" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>A Touch, Vision, and Language Dataset for Multimodal Alignment</strong>, ArXiv, 2024.<br>
Fu, Letian and Datta, Gaurav and Huang, Huang and Panitch, William Chung-Ho and Drake, Jaimyn and Ortiz, Joseph and Mukadam, Mustafa and Lambeta, Mike and Calandra, Roberto and Goldberg, Ken.<br>
[<a href="https://arxiv.org/pdf/2402.13232" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Large-scale actionless video pre-training via discrete diffusion for efficient policy learning</strong>, ArXiv, 2024.<br>
He, Haoran and Bai, Chenjia and Pan, Ling and Zhang, Weinan and Zhao, Bin and Li, Xuelong.<br>
[<a href="https://arxiv.org/pdf/2402.14407" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces</strong>, ArXiv, 2024.<br>
Comi, Mauro and Tonioni, Alessio and Yang, Max and Tremblay, Jonathan and Blukis, Valts and Lin, Yijiong and Lepora, Nathan F and Aitchison, Laurence.<br>
[<a href="https://arxiv.org/pdf/2403.20275" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Tactile-augmented radiance fields</strong>, CVPR, 2024.<br>
Dou, Yiming and Yang, Fengyu and Liu, Yi and Loquercio, Antonio and Owens, Andrew.<br>
[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Dou_Tactile-Augmented_Radiance_Fields_CVPR_2024_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>AnyRotate: Gravity-Invariant In-Hand Object Rotation with Sim-to-Real Touch</strong>, ArXiv, 2024.<br>
Yang, Max and Lu, Chenghua and Church, Alex and Lin, Yijiong and Ford, Chris and Li, Haoran and Psomopoulou, Efi and Barton, David AW and Lepora, Nathan F.<br>
[<a href="https://arxiv.org/pdf/2405.07391" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Feature-level Sim2Real Regression of Tactile Images for Robot Manipulation</strong>, ICRA ViTac, 2024.<br>
Duan, Boyi and Qian, Kun and Zhao, Yongqiang and Zhang, Dongyuan and Luo, Shan.<br>
[<a href="https://shanluo.github.io/ViTacWorkshops/content/ViTac2024_Paper_09.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>MAE4GM: Visuo-Tactile Learning for Property Estimation of Granular Material using Multimodal Autoencoder</strong>,ICRA ViTac, 2024.<br>
Zhang, Zeqing and Zheng, Guangze and Ji, Xuebo and Chen, Guanqi and Jia, Ruixing and Chen, Wentao and Chen, Guanhua and Zhang, Liangjun and Pan, Jia.<br>
[<a href="https://shanluo.github.io/ViTacWorkshops/content/ViTac2024_Paper_01.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Octopi: Object Property Reasoning with Large Tactile-Language Models</strong>, arXiv preprint arXiv:2405.02794, 2024.<br>
Yu, Samson and Lin, Kelvin and Xiao, Anxing and Duan, Jiafei and Soh, Harold.<br>
[<a href="https://arxiv.org/pdf/2405.02794" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>9dtact: A compact vision-based tactile sensor for accurate 3D shape reconstruction and generalizable 6D force estimation</strong>, IEEE Robotics and Automation Letters, 2023.<br>
Lin, Changyi and Zhang, Han and Xu, Jikai and Wu, Lei and Xu, Huazhe.<br>
[<a href="https://arxiv.org/pdf/2308.14277" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Allsight: A low-cost and high-resolution round tactile sensor with zero-shot learning capability</strong>, IEEE Robotics and Automation Letters, 2023.<br>
Azulay, Osher and Curtis, Nimrod and Sokolovsky, Rotem and Levitski, Guy and Slomovik, Daniel and Lilling, Guy and Sintov, Avishai.<br>
[<a href="https://arxiv.org/pdf/2307.02928" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Vistac towards a unified multi-modal sensing finger for robotic manipulation</strong>, IEEE Sensors Journal, 2023.<br>
Athar, Sheeraz and Patel, Gaurav and Xu, Zhengtong and Qiu, Qiang and She, Yu.<br>
[<a href="https://ieeexplore.ieee.org/abstract/document/10242327/" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Midastouch: Monte-carlo inference over distributions across sliding touch</strong>, CoRL, 2023.<br>
Suresh, Sudharshan and Si, Zilin and Anderson, Stuart and Kaess, Michael and Mukadam, Mustafa.<br>
[<a href="https://proceedings.mlr.press/v205/suresh23a/suresh23a.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>The objectfolder benchmark: Multisensory learning with neural and real objects</strong>, CVPR, 2023.<br>
Gao, Ruohan and Dou, Yiming and Li, Hao and Agarwal, Tanmay and Bohg, Jeannette and Li, Yunzhu and Fei-Fei, Li and Wu, Jiajun.
[<a href="http://openaccess.thecvf.com/content/CVPR2023/papers/Gao_The_ObjectFolder_Benchmark_Multisensory_Learning_With_Neural_and_Real_Objects_CVPR_2023_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Imagebind: One embedding space to bind them all</strong>, CVPR, 2023.<br>
Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan.<br>
[<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Touching a nerf: Leveraging neural radiance fields for tactile sensory data generation</strong>, Conference on Robot Learning, pp. 1618-1628, 2023.<br>
Zhong, Shaohong and Albini, Alessandro and Jones, Oiwi Parker and Maiolino, Perla and Posner, Ingmar.<br>
[<a href="https://proceedings.mlr.press/v205/zhong23a/zhong23a.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Learning to read braille: Bridging the tactile reality gap with diffusion models</strong>, ArXiv, 2023.<br>
Higuera, Carolina and Boots, Byron and Mukadam, Mustafa.<br>
[<a href="https://arxiv.org/pdf/2304.01182" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Generating visual scenes from touch</strong>, CVPR, 2023.<br>
Yang, Fengyu and Zhang, Jiacheng and Owens, Andrew.<br>
[<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Generating_Visual_Scenes_from_Touch_ICCV_2023_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Dtact: A vision-based tactile sensor that measures high-resolution 3D geometry directly from darkness</strong>, ICRA, 2023.<br>
Lin, Changyi and Lin, Ziqi and Wang, Shaoxiong and Xu, Huazhe.<br>
[<a href="https://arxiv.org/pdf/2209.13916" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>In-hand pose estimation using hand-mounted RGB cameras and visuotactile sensors</strong>, IEEE Access, 2023.<br>
Gao, Yuan and Matsuoka, Shogo and Wan, Weiwei and Kiyokawa, Takuya and Koyama, Keisuke and Harada, Kensuke.<br>
[<a href="https://ieeexplore.ieee.org/iel7/6287639/6514899/10043666.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Collision-aware in-hand 6D object pose estimation using multiple vision-based tactile sensors</strong>, ICRA, 2023.<br>
Caddeo, Gabriele M and Piga, Nicola A and Bottarel, Fabrizio and Natale, Lorenzo.<br>
[<a href="https://arxiv.org/pdf/2301.13667" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Implicit neural representation for 3D shape reconstruction using vision-based tactile sensing</strong>, ArXiv, 2023.<br>
Comi, Mauro and Church, Alex and Li, Kejie and Aitchison, Laurence and Lepora, Nathan F.<br>
[<a href="https://shanluo.github.io/ViTacWorkshops/content/ViTac2023_Paper_06.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Sliding touch-based exploration for modeling unknown object shape with multi-fingered hands</strong>, IROS, 2023.<br>
Chen, Yiting and Tekden, Ahmet Ercan and Deisenroth, Marc Peter and Bekiroglu, Yasemin.<br>
[<a href="https://arxiv.org/pdf/2308.00576" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>General In-hand Object Rotation with Vision and Touch</strong>, CoRL, 2023.<br>
Qi, Haozhi and Yi, Brent and Suresh, Sudharshan and Lambeta, Mike and Ma, Yi and Calandra, Roberto and Malik, Jitendra.<br>
[<a href="https://proceedings.mlr.press/v229/qi23a/qi23a.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Sim-to-Real Model-Based and Model-Free Deep Reinforcement Learning for Tactile Pushing</strong>, IEEE Robotics and Automation Letters, 2023.<br>
Yang, Max and Lin, Yijiong and Church, Alex and Lloyd, John and Zhang, Dandan and Barton, David AW and Lepora, Nathan F.<br>
[<a href="https://arxiv.org/pdf/2307.14272" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Unsupervised adversarial domain adaptation for sim-to-real transfer of tactile images</strong>, IEEE Transactions on Instrumentation and Measurement, 2023.<br>
Jing, Xingshuo and Qian, Kun and Jianu, Tudor and Luo, Shan.<br>
[<a href="https://ieeexplore.ieee.org/abstract/document/10106009/" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Learn from incomplete tactile data: Tactile representation learning with masked autoencoders</strong>, IROS, 2023.<br>
Cao, Guanqun and Jiang, Jiaqi and Bollegala, Danushka and Luo, Shan.<br>
[<a href="https://arxiv.org/pdf/2307.07358" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Dexterity from touch: Self-supervised pre-training of tactile representations with robotic play</strong>, ArXiv, 2023.<br>
Guzey, Irmak and Evans, Ben and Chintala, Soumith and Pinto, Lerrel.<br>
[<a href="https://arxiv.org/pdf/2303.12076" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Gelslim 3.0: High-resolution measurement of shape, force and slip in a compact tactile-sensing finger</strong>, ICRA, 2022.<br>
Taylor, Ian H and Dong, Siyuan and Rodriguez, Alberto.<br>
[<a href="https://arxiv.org/pdf/2103.12269" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Tacto: A fast, flexible, and open-source simulator for high-resolution vision-based tactile sensors</strong>, IEEE Robotics and Automation Letters, 2022.<br>
Wang, Shaoxiong and Lambeta, Mike and Chou, Po-Wei and Calandra, Roberto.<br>
[<a href="https://arxiv.org/pdf/2012.08456" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Taxim: An example-based simulation model for GelSight tactile sensors</strong>, IEEE Robotics and Automation Letters, 2022.<br>
Si, Zilin and Yuan, Wenzhen.<br>
[<a href="https://arxiv.org/pdf/2109.04027" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Objectfolder 2.0: A multisensory object dataset for sim2real transfer</strong>, CVPR, 2022.<br>
Gao, Ruohan and Si, Zilin and Chang, Yen-Yu and Clarke, Samuel and Bohg, Jeannette and Fei-Fei, Li and Yuan, Wenzhen and Wu, Jiajun.<br>
[<a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Gao_ObjectFolder_2.0_A_Multisensory_Object_Dataset_for_Sim2Real_Transfer_CVPR_2022_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Self-supervised visuo-tactile pretraining to locate and follow garment features</strong>, ArXiv, 2022.<br>
Kerr, Justin and Huang, Huang and Wilcox, Albert and Hoque, Ryan and Ichnowski, Jeffrey and Calandra, Roberto and Goldberg, Ken.<br>
[<a href="https://arxiv.org/pdf/2209.13042" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Visuotactile 6D pose estimation of an in-hand object using vision and tactile sensor data</strong>, IEEE Robotics and Automation Letters, 2022.<br>
Dikhale, Snehal and Patel, Karankumar and Dhingra, Daksh and Naramura, Itoshi and Hayashi, Akinobu and Iba, Soshi and Jamali, Nawid.<br>
[<a href="https://www.researchgate.net/profile/Snehal_Dikhale/publication/357842538_VisuoTactile_6D_Pose_Estimation_of_an_In-Hand_Object_Using_Vision_and_Tactile_Sensor_Data/links/6297b925416ec50bdb022987/VisuoTactile-6D-Pose-Estimation-of-an-In-Hand-Object-Using-Vision-and-Tactile-Sensor-Data.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Shapemap 3-D: Efficient shape mapping through dense touch and vision</strong>, ICRA, 2022.<br>
Suresh, Sudharshan and Si, Zilin and Mangelson, Joshua G and Yuan, Wenzhen and Kaess, Michael.<br>
[<a href="https://arxiv.org/pdf/2109.09884" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Visuotactile-rl: Learning multimodal manipulation policies with deep reinforcement learning</strong>, ICRA, 2022.<br>
Hansen, Johanna and Hogan, Francois and Rivkin, Dmitriy and Meger, David and Jenkin, Michael and Dudek, Gregory.<br>
[<a href="https://johannah.github.io/papers/Visuotactile-RL.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Tactile gym 2.0: Sim-to-real deep reinforcement learning for comparing low-cost high-resolution robot touch</strong>, IEEE Robotics and Automation Letters, 2022.<br>
Lin, Yijiong and Lloyd, John and Church, Alex and Lepora, Nathan F.<br>
[<a href="https://arxiv.org/pdf/2207.10763" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Touch and go: Learning from human-collected vision and touch</strong>, ArXiv, 2022.<br>
Yang, Fengyu and Ma, Chenyang and Zhang, Jiacheng and Zhu, Jing and Yuan, Wenzhen and Owens, Andrew.<br>
[<a href="https://arxiv.org/pdf/2211.12498" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Objectfolder: A dataset of objects with implicit visual, auditory, and tactile representations</strong>, arXiv, 2021.<br>
Gao, Ruohan and Chang, Yen-Yu and Mall, Shivani and Fei-Fei, Li and Wu, Jiajun.<br>
[<a href="https://arxiv.org/pdf/2109.07991" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Learning transferable visual models from natural language supervision</strong>, International Conference on Machine Learning, 2021.<br>
Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others.<br>
[<a href="http://proceedings.mlr.press/v139/radford21a/radford21a.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>GelSight wedge: Measuring high-resolution 3D contact geometry with a compact robot finger</strong>, ICRA, 2021.<br>
Wang, Shaoxiong and She, Yu and Romero, Branden and Adelson, Edward.<br>
[<a href="https://arxiv.org/pdf/2106.08851" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Tactile object pose estimation from the first touch with geometric contact rendering</strong>, CoRL, 2021.<br>
Villalonga, Maria Bauza and Rodriguez, Alberto and Lim, Bryan and Valls, Eric and Sechopoulos, Theo.<br>
[<a href="https://proceedings.mlr.press/v155/villalonga21a/villalonga21a.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Active 3D shape reconstruction from vision and touch</strong>, NeurIPS, 2021.<br>
Smith, Edward and Meger, David and Pineda, Luis and Calandra, Roberto and Malik, Jitendra and Romero Soriano, Adriana and Drozdzal, Michal.<br>
[<a href="https://proceedings.neurips.cc/paper/2021/file/8635b5fd6bc675033fb72e8a3ccc10a0-Paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Interpreting and predicting tactile signals for the syntouch biotac</strong>, The International Journal of Robotics Research, 2021.<br>
Narang, Yashraj S and Sundaralingam, Balakumar and Van Wyk, Karl and Mousavian, Arsalan and Fox, Dieter.<br>
[<a href="https://arxiv.org/pdf/2101.05452" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>GelTip: A finger-shaped optical tactile sensor for robotic manipulation</strong>, IROS, 2020.<br>
Gomes, Daniel Fernandes and Lin, Zhonglin and Luo, Shan.<br>
[<a href="https://arxiv.org/pdf/2008.05404" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>DIGIT: A Novel Design for a Low-Cost Compact High-Resolution Tactile Sensor With Application to In-Hand Manipulation</strong>, IEEE Robotics and Automation Letters, 2020.<br>
Lambeta, Mike and Chou, Po-Wei and Tian, Stephen and Yang, Brian and Maloon, Benjamin and Most, Victoria Rose and Stroud, Dave and Santos, Raymond and Byagowi, Ahmad and Kammerer, Gregg and Jayaraman, Dinesh and Calandra, Roberto.<br>
[<a href="https://arxiv.org/pdf/2005.14679" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Digit: A novel design for a low-cost compact high-resolution tactile sensor with application to in-hand manipulation</strong>, IEEE Robotics and Automation Letters, 2020.<br>
Lambeta, Mike and Chou, Po-Wei and Tian, Stephen and Yang, Brian and Maloon, Benjamin and Most, Victoria Rose and Stroud, Dave and Santos, Raymond and Byagowi, Ahmad and Kammerer, Gregg and others.<br>
[<a href="https://arxiv.org/pdf/2005.14679" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Deep tactile experience: Estimating tactile sensor output from depth sensor data</strong>, IROS, 2020.<br>
Patel, Karankumar and Iba, Soshi and Jamali, Nawid.<br>
[<a href="https://arxiv.org/pdf/2110.08946" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>3D shape reconstruction from vision and touch</strong>, NeurIPS, 2020.<br>
Smith, Edward and Calandra, Roberto and Romero, Adriana and Gkioxari, Georgia and Meger, David and Malik, Jitendra and Drozdzal, Michal.<br>
[<a href="https://proceedings.neurips.cc/paper/2020/file/a3842ed7b3d0fe3ac263bcabd2999790-Paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Supervised autoencoder joint learning on heterogeneous tactile sensory data: Improving material classification performance</strong>, IROS, 2020.<br>
Gao, Ruihan and Taunyazov, Tasbolat and Lin, Zhiping and Wu, Yan.<br>
[<a href="https://yan-wu.com/wp-content/uploads/2020/08/gao2020supervised.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Making sense of vision and touch: Learning multimodal representations for contact-rich tasks</strong>, IEEE Transactions on Robotics, 2020.<br>
Lee, Michelle A and Zhu, Yuke and Zachares, Peter and Tan, Matthew and Srinivasan, Krishnan and Savarese, Silvio and Fei-Fei, Li and Garg, Animesh and Bohg, Jeannette.<br>
[<a href="https://arxiv.org/pdf/1907.13098" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Learning efficient haptic shape exploration with a rigid tactile sensor array</strong>, PloS One, 2020.<br>
Fleer, Sascha and Moringen, Alexandra and Klatzky, Roberta L and Ritter, Helge.<br>
[<a href="https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0226880&amp;type=printable" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Interpreting and predicting tactile signals via a physics-based and data-driven framework</strong>, ArXiv, 2020.<br>
Narang, Yashraj S and Van Wyk, Karl and Mousavian, Arsalan and Fox, Dieter.<br>
[<a href="https://arxiv.org/pdf/2006.03777" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Fast texture classification using tactile neural coding and spiking neural network</strong>, IROS, 2020.<br>
Taunyazov, Tasbolat and Chua, Yansong and Gao, Ruihan and Soh, Harold and Wu, Yan.<br>
[<a href="https://ruihangao.github.io/files/taunyazov2020fast.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Simulation of the SynTouch BioTac sensor</strong>, Intelligent Autonomous Systems 15: Proceedings of the 15th International Conference IAS-15, 2019.<br>
Ruppel, Philipp and Jonetzko, Yannick and Grner, Michael and Hendrich, Norman and Zhang, Jianwei.<br>
[<a href="https://www.researchgate.net/profile/Yannick-Jonetzko/publication/330014756_Simulation_of_the_SynTouch_BioTac_Sensor_Proceedings_of_the_15th_International_Conference_IAS-15/links/5cc7ed694585156cd7bbc519/Simulation-of-the-SynTouch-BioTac-Sensor-Proceedings-of-the-15th-International-Conference-IAS-15.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Robust learning of tactile force estimation through robot interaction</strong>, ICRA, 2019.<br>
Sundaralingam, Balakumar and Lambert, Alexander Sasha and Handa, Ankur and Boots, Byron and Hermans, Tucker and Birchfield, Stan and Ratliff, Nathan and Fox, Dieter.<br>
[<a href="https://arxiv.org/pdf/1810.06187" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>From pixels to percepts: Highly robust edge perception and contour following using deep learning and an optical biomimetic tactile sensor</strong>, IEEE Robotics and Automation Letters, 2019.<br>
Lepora, Nathan F and Church, Alex and De Kerckhove, Conrad and Hadsell, Raia and Lloyd, John.<br>
[<a href="https://arxiv.org/pdf/1812.02941" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Tactile mapping and localization from high-resolution tactile imprints</strong>, ICRA, 2019.<br>
Bauza, Maria and Canal, Oleguer and Rodriguez, Alberto.<br>
[<a href="https://arxiv.org/pdf/1904.10944" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Convolutional autoencoder for feature extraction in tactile sensing</strong>, IEEE Robotics and Automation Letters, 2019.<br>
Polic, Marsela and Krajacic, Ivona and Lepora, Nathan and Orsag, Matko.<br>
[<a href="https://ieeexplore.ieee.org/abstract/document/8758942/" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Learning to identify object instances by touch: Tactile recognition via multimodal matching</strong>, ICRA, 2019.<br>
Lin, Justin and Calandra, Roberto and Levine, Sergey.<br>
[<a href="https://arxiv.org/pdf/1903.03591" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>The tactip family: Soft optical tactile sensors with 3D-printed biomimetic morphologies</strong>, Soft Robotics, 2018.<br>
Ward-Cherrier, Benjamin and Pestell, Nicholas and Cramphorn, Luke and Winstone, Benjamin and Giannaccini, Maria Elena and Rossiter, Jonathan and Lepora, Nathan F.<br>
[<a href="https://www.liebertpub.com/doi/pdf/10.1089/soro.2017.0052" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>3D shape perception from monocular vision, touch, and shape priors</strong>, IROS, 2018.<br>
Wang, Shaoxiong and Wu, Jiajun and Sun, Xingyuan and Yuan, Wenzhen and Freeman, William T and Tenenbaum, Joshua B and Adelson, Edward H.<br>
[<a href="https://arxiv.org/pdf/2209.13916" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>GelSight: High-Resolution Robot Tactile Sensors for Estimating Geometry and Force</strong>, Sensors, 2017.<br>
Yuan, Wenzhen and Dong, Siyuan and Adelson, Edward H.<br>
[<a href="https://www.mdpi.com/1424-8220/17/12/2762/pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>The feeling of success: Does touch sensing help predict grasp outcomes?</strong>, arXiv, 2017.<br>
Calandra, Roberto and Owens, Andrew and Upadhyaya, Manu and Yuan, Wenzhen and Lin, Justin and Adelson, Edward H and Levine, Sergey.<br>
[<a href="https://arxiv.org/pdf/1710.05512" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Improved GelSight tactile sensor for measuring geometry and slip</strong>, IROS, 2017.<br>
Dong, Siyuan and Yuan, Wenzhen and Adelson, Edward H.<br>
[<a href="https://arxiv.org/pdf/1708.00922" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>GelSight: High-resolution robot tactile sensors for estimating geometry and force</strong>, Sensors, vol. 17, no. 12, pp. 2762, 2017.<br>
Yuan, Wenzhen and Dong, Siyuan and Adelson, Edward H.<br>
[<a href="https://www.mdpi.com/1424-8220/17/12/2762/pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Connecting look and feel: Associating the visual and tactile properties of physical materials</strong>, CVPR, 2017.<br>
Yuan, Wenzhen and Wang, Shaoxiong and Dong, Siyuan and Adelson, Edward.<br>
[<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Yuan_Connecting_Look_and_CVPR_2017_paper.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Stable reinforcement learning with autoencoders for tactile and visual data</strong>, IROS, 2016.<br>
Van Hoof, Herke and Chen, Nutan and Karl, Maximilian and van der Smagt, Patrick and Peters, Jan.<br>
[<a href="https://www.academia.edu/download/47652433/Hoof2016.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Sensing tactile microvibrations with the BioTacComparison with human sensitivity</strong>, BioRob, 2012.<br>
Fishel, Jeremy A and Loeb, Gerald E.<br>
[<a href="https://www.researchgate.net/profile/Gerald-Loeb/publication/256748883_Sensing_tactile_microvibrations_with_the_BioTac_Comparison_with_human_sensitivity/links/5dbcacae299bf1a47b0a3fa6/Sensing-tactile-microvibrations-with-the-BioTac-Comparison-with-human-sensitivity.pdf" rel="nofollow">page</a>]</p>
</li>
</ul>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><a id="user-content-interaction"> Embodied Interaction </a><a href="#table-of-contents"></a> </h2><a id="user-content--embodied-interaction--" class="anchor" aria-label="Permalink:  Embodied Interaction " href="#-embodied-interaction--"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>
<p dir="auto"><strong>Beyond the Destination: A Novel Benchmark for Exploration-Aware Embodied Question Answering</strong>, arxiv, 2025<br>
Kaixuan Jiang, Yang Liu, Weixing Chen, Jingzhou Luo, Ziliang Chen, Ling Pan, Guanbin Li, Liang Lin.<br>
<a href="https://arxiv.org/pdf/2503.11117" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Cross-Embodiment Dexterous Grasping with Reinforcement Learning</strong>, arxiv, 2024<br>
Haoqi Yuan, Bohan Zhou, Yuhui Fu, Zongqing Lu.<br>
<a href="https://arxiv.org/pdf/2410.02479" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation</strong>, arxiv, 2024<br>
Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang.<br>
<a href="https://arxiv.org/pdf/2403.08321" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>MANUS: Markerless Grasp Capture using Articulated 3D Gaussians</strong>, CVPR, 2024<br>
Chandradeep Pokhariya, Ishaan Nikhil Shah, Angela Xing, Zekun Li, Kefan Chen, Avinash Sharma, Srinath Sridhar.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Pokhariya_MANUS_Markerless_Grasp_Capture_using_Articulated_3D_Gaussians_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Language-driven Grasp Detection</strong>, CVPR, 2024<br>
An Dinh Vuong, Minh Nhat Vu, Baoru Huang, Nghia Nguyen, Hieu Le, Thieu Vo, Anh Nguyen.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Vuong_Language-driven_Grasp_Detection_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Generalizing 6-DoF Grasp Detection via Domain Prior Knowledge</strong>, CVPR, 2024<br>
Haoxiang Ma, Modi Shi, Boyang Gao, Di Huang.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_Generalizing_6-DoF_Grasp_Detection_via_Domain_Prior_Knowledge_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Dexterous Grasp Transformer</strong>, CVPR, 2024<br>
Guo-Hao Xu, Yi-Lin Wei, Dian Zheng, Xiao-Ming Wu, Wei-Shi Zheng.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Dexterous_Grasp_Transformer_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Single-View Scene Point Cloud Human Grasp Generation</strong>, CVPR, 2024<br>
Yan-Kang Wang, Chengyi Xing, Yi-Lin Wei, Xiao-Ming Wu, Wei-Shi Zheng.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Single-View_Scene_Point_Cloud_Human_Grasp_Generation_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis</strong>, CVPR, 2024<br>
Yufei Ye, Abhinav Gupta, Kris Kitani, Shubham Tulsiani.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ye_G-HOP_Generative_Hand-Object_Prior_for_Interaction_Reconstruction_and_Grasp_Synthesis_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Grasping Diverse Objects with Simulated Humanoids</strong> ArXiv, 2024.<br>
Zhengyi Luo, Jinkun Cao, Sammy Christen, Alexander Winkler, Kris Kitani, Weipeng Xu<br>
<a href="https://arxiv.org/pdf/2407.11385" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Task-Oriented Dexterous Grasp Synthesis via Differentiable Grasp Wrench Boundary Estimator</strong>, IROS, 2024.<br>
Jiayi Chen,Yuxing Chen,Jialiang Zhang, He Wang<br>
<a href="https://arxiv.org/pdf/2309.13586" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Open6DOR: Benchmarking Open-instruction 6-DoF Object Rearrangement and A VLM-based Approach</strong>, IROS, 2024.<br>
Yufei Ding,Haoran Geng , Chaoyi Xu ,Xiaomeng Fang,Jiazhao Zhang,Songlin Wei, Qiyu Dai, Zhizheng Zhang, He Wang<br>
<a href="https://pku-epic.github.io/Open6DOR/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>ASGrasp: Generalizable Transparent Object Reconstruction and 6-DoF Grasp Detection from RGB-D Active Stereo Camera</strong>, ICRA, 2024.<br>
Jun Shi, Yong A, Yixiang Jin, Dingzhe Li, Haoyu Niu, Zhezhu Jin, He Wang<br>
<a href="https://arxiv.org/pdf/2405.05648" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>OpenEQA: Embodied Question Answering in the Era of Foundation Models</strong>, CVPR, 2024<br>
Majumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and others<br>
<a href="http://openaccess.thecvf.com/content/CVPR2024/papers/Majumdar_OpenEQA_Embodied_Question_Answering_in_the_Era_of_Foundation_Models_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Explore until Confident: Efficient Exploration for Embodied Question Answering</strong>, ICRA Workshop VLMNM, 2024<br>
Ren, Allen Z and Clark, Jaden and Dixit, Anushri and Itkina, Masha and Majumdar, Anirudha and Sadigh, Dorsa<br>
<a href="https://arxiv.org/pdf/2403.15941" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>S-EQA: Tackling Situational Queries in Embodied Question Answering</strong>, arXix, 2024<br>
Dorbala, Vishnu Sashank and Goyal, Prasoon and Piramuthu, Robinson and Johnston, Michael and Manocha, Dinesh and Ghanadhan, Reza<br>
<a href="https://arxiv.org/pdf/2405.04732" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Map-based Modular Approach for Zero-shot Embodied Question Answering</strong>, arXiv, 2024<br>
Sakamoto, Koya and Azuma, Daichi and Miyanishi, Taiki and Kurita, Shuhei and Kawanabe, Motoaki<br>
<a href="https://ui.adsabs.harvard.edu/abs/2024arXiv240516559S/abstract" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Embodied Question Answering via Multi-LLM Systems</strong>, arXiv, 2024<br>
Bhrij Patel and Vishnu Sashank Dorbala and Amrit Singh Bedi<br>
<a href="https://arxiv.org/pdf/2406.10918" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>MultiGripperGrasp: A Dataset for Robotic Grasping from Parallel Jaw Grippers to Dexterous Hands</strong>, arXiv, 2024<br>
Murrilo, Luis Felipe Casas and Khargonkar, Ninad and Prabhakaran, Balakrishnan and Xiang, Yu<br>
<a href="https://arxiv.org/abs/2403.09841" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Reasoning Grasping via Multimodal Large Language Model</strong>, arXiv, 2024<br>
Jin, Shiyu and Xu, Jinxuan and Lei, Yutian and Zhang, Liangjun<br>
<a href="https://arxiv.org/abs/2402.06798" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>SemGrasp: Semantic Grasp Generation via Language Aligned Discretization</strong>, CoRR, 2024<br>
Li, Kailin and Wang, Jingbo and Yang, Lixin and Lu, Cewu and Dai, Bo<br>
<a href="https://openreview.net/forum?id=WUbr8NV1G6" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping</strong>, arXiv, 2024<br>
Zheng, Yuhang and Chen, Xiangyu and Zheng, Yupeng and Gu, Songen and Yang, Runyi and Jin, Bu and Li, Pengfei and Zhong, Chengliang and Wang, Zengmao and Liu, Lina and others<br>
<a href="https://arxiv.org/abs/2403.09637" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Knowledge-based Embodied Question Answering</strong>, TPAMI, 2023<br>
Tan, Sinan and Ge, Mengmeng and Guo, Di and Liu, Huaping and Sun, Fuchun<br>
<a href="https://pubmed.ncbi.nlm.nih.gov/37195849/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Deep Learning Approaches to Grasp Synthesis: A Review</strong>, IEEE Transactions on Robotics, 2023<br>
Newbury, Rhys and Gu, Morris and Chumbley, Lachlan and Mousavian, Arsalan and Eppner, Clemens and Leitner, J{"u}rgen and Bohg, Jeannette and Morales, Antonio and Asfour, Tamim and Kragic, Danica and others<br>
<a href="https://dl.acm.org/doi/abs/10.1109/TRO.2023.3280597" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Language-guided Robot Grasping: CLIP-based Referring Grasp Synthesis in Clutter</strong>, CoRL, 2023<br>
Tziafas, Georgios and Xu, Yucheng and Goel, Arushi and Kasaei, Mohammadreza and Li, Zhibin and Kasaei, Hamidreza<br>
<a href="https://www.research.ed.ac.uk/en/publications/language-guided-robot-grasping-clip-based-referring-grasp-synthes" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Reasoning Tuning Grasp: Adapting Multi-Modal Large Language Models for Robotic Grasping</strong>, CoRL, 2023<br>
Xu, Jinxuan and Jin, Shiyu and Lei, Yutian and Zhang, Yuqian and Zhang, Liangjun<br>
<a href="https://openreview.net/forum?id=3mKb5iyZ2V" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation</strong>, CoRL, 2023<br>
Shen, William and Yang, Ge and Yu, Alan and Wong, Jansen and Kaelbling, Leslie Pack and Isola, Phillip<br>
<a href="https://proceedings.mlr.press/v229/shen23a.html" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>AnyGrasp: Robust and Efficient Grasp Perception in Spatial and Temporal Domains</strong>, IEEE Transactions on Robotics, 2023
Fang, Hao-Shu and Wang, Chenxi and Fang, Hongjie and Gou, Minghao and Liu, Jirong and Yan, Hengxu and Liu, Wenhai and Xie, Yichen and Lu, Cewu<br>
<a href="https://ieeexplore.ieee.org/abstract/document/10167687" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>DexGraspNet: A Large-Scale Robotic Dexterous Grasp Dataset for General Objects Based on Simulation</strong>, ICRA, 2023.<br>
Ruicheng Wang, Jialiang Zhang, Jiayi Chen, Yinzhen Xu, Puhao Li, Tengyu Liu, He Wang<br>
<a href="https://pku-epic.github.io/DexGraspNet/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy</strong>, CVPR, 2023.<br>
Yinzhen Xu, Weikang Wan, Jialiang Zhang, Haoran Liu, Zikang Shan, Hao Shen, Ruicheng Wang, Haoran Geng, Yijia Weng, Jiayi Chen, Tengyu Liu, Li Yi, He Wang<br>
<a href="https://pku-epic.github.io/UniDexGrasp/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning</strong>, ICCV, 2023.<br>
Weikang Wan, Haoran Geng, Yun Liu, Zikang Shan, Yaodong Yang, Li Yi, He Wang<br>
<a href="https://pku-epic.github.io/UniDexGrasp++/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>CLIPort: What and Where Pathways for Robotic Manipulation</strong>, CoRL, 2022<br>
Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter<br>
<a href="https://proceedings.mlr.press/v164/shridhar22a.html" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>ACRONYM: A Large-Scale Grasp Dataset Based on Simulation</strong>, ICRA, 2021<br>
Eppner, Clemens and Mousavian, Arsalan and Fox, Dieter<br>
<a href="https://ieeexplore.ieee.org/abstract/document/9560844" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI</strong>, NeurIPS, 2021<br>
Ramakrishnan, Santhosh K and Gokaslan, Aaron and Wijmans, Erik and Maksymets, Oleksandr and Clegg, Alex and Turner, John and Undersander, Eric and Galuba, Wojciech and Westbury, Andrew and Chang, Angel X and others<br>
<a href="https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/34173cb38f07f89ddbebc2ac9128303f-Paper-round2.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB</strong>, ICRA, 2021<br>
Ainetter, Stefan and Fraundorfer, Friedrich<br>
<a href="https://elib.dlr.de/146134/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Revisiting EmbodiedQA: A Simple Baseline and Beyond</strong>, IEEE Transactions on Image Processing, 2020<br>
Wu, Yu and Jiang, Lu and Yang, Yi<br>
<a href="https://opus.lib.uts.edu.au/rest/bitstreams/ee2d1faf-ce3b-4f63-a133-4217d19e9db1/retrieve" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Multi-agent Embodied Question Answering in Interactive Environments</strong>, ECCV, 2020<br>
Tan, Sinan and Xiang, Weilai and Liu, Huaping and Guo, Di and Sun, Fuchun<br>
<a href="https://dl.acm.org/doi/abs/10.1007/978-3-030-58601-0_39" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Language Models are Few-Shot Learners</strong>, NIPS, 2020<br>
Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others<br>
<a href="https://dl.acm.org/doi/pdf/10.5555/3495724.3495883" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>GraspNet-1Billion: A Large-Scale Benchmark for General Object Grasping</strong>, CVPR, 2020<br>
Fang, Hao-Shu and Wang, Chenxi and Gou, Minghao and Lu, Cewu<br>
<a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fang_GraspNet-1Billion_A_Large-Scale_Benchmark_for_General_Object_Grasping_CVPR_2020_paper.html" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Multi-Target Embodied Question Answering</strong>, CVPR, 2019<br>
Yu, Licheng and Chen, Xinlei and Gkioxari, Georgia and Bansal, Mohit and Berg, Tamara L and Batra, Dhruv<br>
<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_Multi-Target_Embodied_Question_Answering_CVPR_2019_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Embodied Question Answering in Photorealistic Environments with Point Cloud Perception</strong>, CVPR, 2019<br>
Wijmans, Erik and Datta, Samyak and Maksymets, Oleksandr and Das, Abhishek and Gkioxari, Georgia and Lee, Stefan and Essa, Irfan and Parikh, Devi and Batra, Dhruv<br>
<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wijmans_Embodied_Question_Answering_in_Photorealistic_Environments_With_Point_Cloud_Perception_CVPR_2019_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>VideoNavQA: Bridging the Gap between Visual and Embodied Question Answering</strong>, BMVC, 2019<br>
Cangea, C{\u{a}}t{\u{a}}lina and Belilovsky, Eugene and Li{`o}, Pietro and Courville, Aaron<br>
<a href="https://arxiv.org/pdf/1908.04950" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>6-DOF GraspNet: Variational Grasp Generation for Object Manipulation</strong>, ICCV, 2019<br>
Mousavian, Arsalan and Eppner, Clemens and Fox, Dieter<br>
<a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Mousavian_6-DOF_GraspNet_Variational_Grasp_Generation_for_Object_Manipulation_ICCV_2019_paper.html" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Embodied Question Answering</strong>, CVPR, 2018<br>
Das, Abhishek and Datta, Samyak and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv<br>
<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Das_Embodied_Question_Answering_CVPR_2018_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>IQA: Visual Question Answering in Interactive Environments</strong>, CVPR, 2018<br>
Gordon, Daniel and Kembhavi, Aniruddha and Rastegari, Mohammad and Redmon, Joseph and Fox, Dieter and Farhadi, Ali
<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Gordon_IQA_Visual_Question_CVPR_2018_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Building Generalizable Agents with a Realistic and Rich 3D Environment</strong>, ECCV, 2018<br>
Wu, Yi and Wu, Yuxin and Gkioxari, Georgia and Tian, Yuandong<br>
<a href="https://arxiv.org/pdf/1801.02209" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments</strong>, ECCV, 2018<br>
Savva, Manolis and Chang, Angel X and Dosovitskiy, Alexey and Funkhouser, Thomas and Koltun, Vladlen<br>
<a href="https://arxiv.org/pdf/1712.03931" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Neural Modular Control for Embodied Question Answering</strong>, ECCV, 2018<br>
Das, Abhishek and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv<br>
<a href="https://authors.library.caltech.edu/records/ykvm4-2ed40/files/1810.11181.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Jacquard: A Large Scale Dataset for Robotic Grasp Detection</strong>, IROS, 2018<br>
Depierre, Amaury and Dellandr{'e}a, Emmanuel and Chen, Liming<br>
<a href="https://ieeexplore.ieee.org/abstract/document/8593950" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Matterport3D: Learning from rgb-d data in indoor environments,</strong>, IEEE International Conference on 3D Vision, 2017<br>
Chang, Angel and Dai, Angela and Funkhouser, Thomas and Halber, Maciej and Niessner, Matthias and Savva, Manolis and Song, Shuran and Zeng, Andy and Zhang, Yinda<br>
<a href="https://arxiv.org/pdf/1709.06158" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</strong>, CVPR, 2017<br>
Dai, Angela and Chang, Angel X and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\ss}ner, Matthias
<a href="https://www.computer.org/csdl/proceedings-article/cvpr/2017/0457c432/12OmNyRg4C5" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Shape Completion Enabled Robotic Grasping</strong>, IROS, 2017<br>
Varley, Jacob and DeChant, Chad and Richardson, Adam and Ruales, Joaqu{'\i}n and Allen, Peter<br>
<a href="https://ieeexplore.ieee.org/abstract/document/8206060" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Efficient grasping from RGBD images: Learning using a new rectangle representation</strong>, IEEE International Conference on Robotics and Automation, 2011<br>
Jiang, Yun and Moseson, Stephen and Saxena, Ashutosh<br>
<a href="https://ieeexplore.ieee.org/abstract/document/5980145" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>A frontier-based approach for autonomous exploration</strong>, CIRA, 1997<br>
Yamauchi, Brian<br>
<a href="https://dl.acm.org/doi/abs/10.5555/523996.793157" rel="nofollow">[page]</a></p>
</li>
</ul>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><a id="user-content-agent"> Embodied Agent </a><a href="#table-of-contents"></a> </h2><a id="user-content--embodied-agent--" class="anchor" aria-label="Permalink:  Embodied Agent " href="#-embodied-agent--"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">Embodied Multimodal Foundation Models and VLA Methods</h3><a id="user-content-embodied-multimodal-foundation-models-and-vla-methods" class="anchor" aria-label="Permalink: Embodied Multimodal Foundation Models and VLA Methods" href="#embodied-multimodal-foundation-models-and-vla-methods"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>
<p dir="auto"><strong>Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks</strong>, arXiv, 2025.<br>
Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, Weiming Lu, Peng Li, Yueting Zhuang<br>
[<a href="https://arxiv.org/abs/2503.21696" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World</strong>, arXiv, 2024.<br>
Weixin Mao, Weiheng Zhong, Zhou Jiang, Dong Fang, Zhongyue Zhang, Zihan Lan, Fan Jia, Tiancai Wang, Haoqiang Fan, Osamu Yoshie.<br>
[<a href="https://arxiv.org/pdf/2412.00171" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Spatially Visual Perception for End-to-End Robotic Learning</strong>, arXiv, 2024.<br>
Travis Davies, Jiahuan Yan, Xiang Chen, Yu Tian, Yueting Zhuang, Yiqi Huang, Luhui Hu.<br>
[<a href="https://arxiv.org/pdf/2411.17458" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation</strong>, arXiv, 2024.<br>
Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, Minzhao Zhu.<br>
[<a href="https://arxiv.org/pdf/2410.06158" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers</strong>, arXiv, 2024.<br>
Lirui Wang, Xinlei Chen, Jialiang Zhao, Kaiming He.<br>
[<a href="https://arxiv.org/pdf/2409.20537" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Spatial Reasoning and Planning for Deep Embodied Agents</strong>, arXiv, 2024.<br>
Shu Ishida.<br>
[<a href="https://arxiv.org/pdf/2409.19479" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Grounding Large Language Models In Embodied Environment With Imperfect World Models</strong>, arXiv, 2024.<br>
Haolan Liu, Jishen Zhao.<br>
[<a href="https://arxiv.org/pdf/2410.02742" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>SELU: Self-Learning Embodied MLLMs in Unknown Environments</strong>, arXiv, 2024.<br>
Boyu Li, Haobin Jiang, Ziluo Ding, Xinrun Xu, Haoran Li, Dongbin Zhao, Zongqing Lu.<br>
[<a href="https://arxiv.org/pdf/2410.03303" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Autort: Embodied foundation models for large scale orchestration of robotic agents</strong>, arXiv, 2024.<br>
Ahn, Michael, Debidatta, Dwibedi, Chelsea, Finn, Montse Gonzalez, Arenas, Keerthana, Gopalakrishnan, Karol, Hausman, Brian, Ichter, Alex, Irpan, Nikhil, Joshi, Ryan, Julian, others.<br>
[<a href="https://arxiv.org/pdf/2401.12963" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learningn</strong>, arXiv, 2024.<br>
Norman Di Palo, Leonard Hasenclever, Jan Humplik, Arunkumar Byravan.<br>
[<a href="https://arxiv.org/pdf/2407.20798" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Rt-h: Action hierarchies using language</strong>, ArXiv, 2024.<br>
Belkhale, Suneel, Tianli, Ding, Ted, Xiao, Pierre, Sermanet, Quon, Vuong, Jonathan, Tompson, Yevgen, Chebotar, Debidatta, Dwibedi, Dorsa, Sadigh.<br>
[<a href="https://arxiv.org/pdf/2403.01823" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Do as i can, not as i say: Grounding language in robotic affordances</strong>, Conference on robot learning. 2023.<br>
Brohan, Anthony, Yevgen, Chebotar, Chelsea, Finn, Karol, Hausman, Alexander, Herzog, Daniel, Ho, Julian, Ibarz, Alex, Irpan, Eric, Jang, Ryan, Julian.<br>
[<a href="https://arxiv.org/pdf/2204.01691" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Embodiedgpt: Vision-language pre-training via embodied chain of thought</strong>, NeurIPS, 2024.<br>
Mu, Yao, Qinglong, Zhang, Mengkang, Hu, Wenhai, Wang, Mingyu, Ding, Jun, Jin, Bin, Wang, Jifeng, Dai, Yu, Qiao, Ping, Luo.<br>
[<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/4ec43957eda1126ad4887995d05fae3b-Paper-Conference.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Q-transformer: Scalable offline reinforcement learning via autoregressive q-functions</strong>, Conference on Robot Learning. 2023.<br>
Chebotar, Yevgen, Quan, Vuong, Karol, Hausman, Fei, Xia, Yao, Lu, Alex, Irpan, Aviral, Kumar, Tianhe, Yu, Alexander, Herzog, Karl, Pertsch, others.<br>
[<a href="https://proceedings.mlr.press/v229/chebotar23a/chebotar23a.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Sara-rt: Scaling up robotics transformers with self-adaptive robust attention</strong>, arXiv, 2023.<br>
Leal, Isabel, Krzysztof, Choromanski, Deepali, Jain, Avinava, Dubey, Jake, Varley, Michael, Ryoo, Yao, Lu, Frederick, Liu, Vikas, Sindhwani, Quan, Vuong, others.<br>
[<a href="https://arxiv.org/pdf/2312.01990" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Palm-e: An embodied multimodal language model</strong>, ArXiv, 2023.<br>
Driess, Danny, Fei, Xia, Mehdi SM, Sajjadi, Corey, Lynch, Aakanksha, Chowdhery, Brian, Ichter, Ayzaan, Wahid, Jonathan, Tompson, Quan, Vuong, Tianhe, Yu, others.<br>
[<a href="https://arxiv.org/pdf/2303.03378" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Rt-2: Vision-language-action models transfer web knowledge to robotic control</strong>, Conference on Robot Learning. 2023.<br>
Zitkovich, Brianna, Tianhe, Yu, Sichun, Xu, Peng, Xu, Ted, Xiao, Fei, Xia, Jialin, Wu, Paul, Wohlhart, Stefan, Welker, Ayzaan, Wahid, others.<br>
[<a href="https://arxiv.org/pdf/2307.15818" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Open x-embodiment: Robotic learning datasets and rt-x models</strong>, arXiv, 2023.<br>
Padalkar, others.<br>
[<a href="https://arxiv.org/pdf/2310.08864" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Vision-language foundation models as effective robot imitators</strong>, arXiv, 2023.<br>
Li, Xinghang, Minghuan, Liu, Hanbo, Zhang, Cunjun, Yu, Jie, Xu, Hongtao, Wu, Chilam, Cheang, Ya, Jing, Weinan, Zhang, Huaping, Liu, others.<br>
[<a href="https://arxiv.org/pdf/2311.01378" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Rt-1: Robotics transformer for real-world control at scale</strong>, ArXiv, 2022.<br>
Brohan, Anthony, Noah, Brown, Justice, Carbajal, Yevgen, Chebotar, Joseph, Dabis, Chelsea, Finn, Keerthana, Gopalakrishnan, Karol, Hausman, Alex, Herzog, Jasmine, Hsu, others.<br>
[<a href="https://arxiv.org/pdf/2212.06817" rel="nofollow">page</a>]</p>
</li>
</ul>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">Embodied Manipulation &amp; Control</h3><a id="user-content-embodied-manipulation--control" class="anchor" aria-label="Permalink: Embodied Manipulation &amp; Control" href="#embodied-manipulation--control"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>
<p dir="auto"><strong>PEAC: Unsupervised Pre-training for Cross-Embodiment Reinforcement Learning</strong>, NeurIPS, 2024.<br>
Chengyang Ying, Zhongkai Hao, Xinning Zhou, Xuezhou Xu, Hang Su, Xingxing Zhang, Jun Zhu.<br>
[<a href="https://arxiv.org/pdf/2405.14073" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Fourier Controller Networks for Real-Time Decision-Making in Embodied Learning</strong>, ICML, 2024.<br>
Hengkai Tan, Songming Liu, Kai Ma, Chengyang Ying, Xingxing Zhang, Hang Su, Jun Zhu.<br>
[<a href="https://arxiv.org/pdf/2405.19885" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation</strong>, ArXiv, 2024.<br>
Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, Jun Zhu.<br>
[<a href="https://arxiv.org/pdf/2410.07864" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>ManiBox: Enhancing Spatial Grasping Generalization via Scalable Simulation Data Generation</strong>, ArXiv, 2024.<br>
Hengkai Tan, Xuezhou Xu, Chengyang Ying, Xinyi Mao, Songming Liu, Xingxing Zhang, Hang Su, Jun Zhu.<br>
[<a href="https://arxiv.org/pdf/2411.01850" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>RoboGSim: A Real2Sim2Real Robotic Gaussian Splatting Simulator</strong>, ArXiv, 2024.<br>
Xinhai Li, Jialin Li, Ziheng Zhang, Rui Zhang, Fan Jia, Tiancai Wang, Haoqiang Fan, Kuo-Kun Tseng, Ruiping Wang.<br>
[<a href="https://arxiv.org/pdf/2411.11839" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>SPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for Long-Horizon Manipulation</strong>, ArXiv, 2024.<br>
Zihan Zhou, Animesh Garg, Dieter Fox, Caelan Garrett, Ajay Mandlekar.<br>
[<a href="https://arxiv.org/pdf/2410.18065" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Diffusion Transformer Policy</strong>, ArXiv, 2024.<br>
Zhi Hou, Tianyi Zhang, Yuwen Xiong, Hengjun Pu, Chengyang Zhao, Ronglei Tong, Yu Qiao, Jifeng Dai, Yuntao Chen.<br>
[<a href="https://arxiv.org/pdf/2410.15959" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Dexcap: Scalable and portable mocap data collection system for dexterous manipulation</strong>, ArXiv, 2024.<br>
Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, C Karen Liu.<br>
[<a href="https://arxiv.org/pdf/2403.07788" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Lota-bench: Benchmarking language-oriented task planners for embodied agents</strong>, ArXiv, 2024.<br>
Choi, Jae-Woo, Youngwoo, Yoon, Hyobin, Ong, Jaehong, Kim, Minsu, Jang.<br>
[<a href="https://arxiv.org/pdf/2402.08178" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Socratic Planner: Inquiry-Based Zero-Shot Planning for Embodied Instruction Following</strong>, Arxiv, 2024.<br>
Suyeon Shin, Sujin jeon, Junghyun Kim, Gi-Cheon Kang, Byoung-Tak Zhang.<br>
[<a href="https://arxiv.org/pdf/2404.15190" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Large Language Models as Commonsense Knowledge for Large-Scale Task Planning</strong>, NeurIPS, 2024.<br>
Zhao, Zirui, Wee Sun, Lee, David, Hsu.<br>
[<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/65a39213d7d0e1eb5d192aa77e77eeb7-Paper-Conference.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Generalized Planning in PDDL Domains with Pretrained Large Language Models</strong>, AAAI, 2024.<br>
Silver, Tom, Soham, Dan, Kavitha, Srinivas, Joshua B., Tenenbaum, Leslie Pack, Kaelbling, Michael, Katz.<br>
[<a href="https://ojs.aaai.org/index.php/AAAI/article/download/30006/31766" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration</strong> arXiv, 2024.<br>
Zhang, Yang, Shixin, Yang, Chenjia, Bai, Fei, Wu, Xiu, Li, Xuelong, Li, Zhen, Wang.<br>
[<a href="https://arxiv.org/pdf/2405.14314" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Embodied Instruction Following in Unknown Environments</strong>, arXiv, 2024.<br>
Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan.<br>
[<a href="https://arxiv.org/pdf/2406.11818" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>A Backbone for Long-Horizon Robot Task Understanding</strong>, arxiv, 2024.<br>
Xiaoshuai Chen, Wei Chen, Dongmyoung Lee, Yukun Ge, Nicolas Rojas, and Petar Kormushev.<br>
<a href="https://arxiv.org/pdf/2408.01334" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>RoboMamba: Multimodal State Space Model for Efficient Robot Reasoning and Manipulation</strong>, arXiv, 2024.<br>
Liu, Jiaming, Mengzhen, Liu, Zhenyu, Wang, Lily, Lee, Kaichen, Zhou, Pengju, An, Senqiao, Yang, Renrui, Zhang, Yandong, Guo, Shanghang, Zhang.<br>
[<a href="https://arxiv.org/pdf/2406.04339" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic Manipulation</strong>, arxiv, 2024.<br>
Ruoxuan Feng, Di Hu1, Wenke Ma, Xuelong Li.<br>
<a href="https://arxiv.org/pdf/2408.01366" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Egocentric Vision Language Planning</strong>, arxiv, 2024.<br>
Zhirui Fang, Ming Yang, Weishuai Zeng, Boyu Li, Junpeng Yue, Ziluo Ding, Xiu Li, Zongqing Lu.<br>
<a href="https://arxiv.org/pdf/2408.05802" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual Grounding and Large Language Models</strong>, IROS, 2024.<br>
Tianyu Wang, Haitao Lin, Junqiu Yu, Yanwei Fu.<br>
<a href="https://arxiv.org/pdf/2408.07975" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>LLM-SAP: Large Language Model Situational Awareness Based Planning</strong>, ICME 2024 Workshop MML4SG.<br>
Liman Wang, Hanyang Zhong.<br>
<a href="https://github.com/HanyangZhong/Situational_Planning_datasets">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>FMB: a Functional Manipulation Benchmark for Generalizable Robotic Learning</strong>, ArXiv, 2024.<br>
Jianlan Luo, Charles Xu, Fangchen Liu, Liam Tan, Zipeng Lin, Jeffrey Wu, Pieter Abbeel, and Sergey Levine.<br>
<a href="https://arxiv.org/pdf/2401.08553" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models</strong> IROS, 2024.<br>
Siyuan Huang, Iaroslav Ponomarenko, Zhengkai Jiang, Xiaoqi Li, Xiaobin Hu, Peng Gao, Hongsheng Li, and Hao Dong.<br>
<a href="https://github.com/SiyuanHuang95/ManipVQA">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>A3VLM: Actionable Articulation-Aware Vision Language Model</strong> ArXiv, 2024.<br>
Siyuan Huang, Haonan Chang, Yuhan Liu, Yimeng Zhu, Hao Dong, Peng Gao, Abdeslam Boularias, and Hongsheng Li.<br>
<a href="https://github.com/changhaonan/A3VLM">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld</strong>, CVPR, 2024.<br>
Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li, Li Shen, Xiaodong He, Jing Jiang, Yuhui Shi.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Embodied_Multi-Modal_Agent_trained_by_an_LLM_from_a_Parallel_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Retrieval-Augmented Embodied Agents</strong>, CVPR, 2024.<br>
Yichen Zhu, Zhicai Ou, Xiaofeng Mou, Jian Tang.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_Retrieval-Augmented_Embodied_Agents_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Multi-agent Collaborative Perception via Motion-aware Robust Communication Network</strong>, CVPR, 2024.<br>
Shixin Hong, Yu Liu, Zhi Li, Shaohui Li, You He.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hong_Multi-agent_Collaborative_Perception_via_Motion-aware_Robust_Communication_Network_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models</strong>, ICCV, 2023.<br>
Chan Hee Song, Jiaman Wu, Clay Washington, Brian M. Sadler, Wei-Lun Chao, Yu Su.<br>
[[page](LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models)]</p>
</li>
<li>
<p dir="auto"><strong>Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models</strong> EMNLP, 2023.<br>
Sarch, Gabriel, Yue, Wu, Michael J., Tarr, Katerina, Fragkiadaki.<br>
[<a href="https://arxiv.org/pdf/2310.15127" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Voyager: An Open-Ended Embodied Agent with Large Language Models</strong>, TMLR, 2023.<br>
Wang, Guanzhi, Yuqi, Xie, Yunfan, Jiang, Ajay, Mandlekar, Chaowei, Xiao, Yuke, Zhu, Linxi, Fan, Anima, Anandkumar.<br>
[<a href="https://arxiv.org/pdf/2305.16291" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>ReAct: Synergizing Reasoning and Acting in Language Models</strong>, ICLR, 2023.<br>
Yao, Shunyu, Jeffrey, Zhao, Dian, Yu, Nan, Du, Izhak, Shafran, Karthik, Narasimhan, Yuan, Cao.<br>
[<a href="https://arxiv.org/pdf/2210.03629" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>ProgPrompt: Generating Situated Robot Task Plans Using Large Language Models</strong> , ICRA, 2023.<br>
Singh, Ishika, Valts, Blukis, Arsalan, Mousavian, Ankit, Goyal, Danfei, Xu, Jonathan, Tremblay, Dieter, Fox, Jesse, Thomason, Animesh, Garg.<br>
[<a href="https://arxiv.org/pdf/2209.11302" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>ChatGPT for Robotics: Design Principles and Model Abilities</strong>, IEEE Access 12. (2023): 55682-55696.<br>
Sai Vemprala, Rogerio Bonatti, Arthur Fender C. Bucker, Ashish Kapoor.<br>
[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10500490" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Code as Policies: Language Model Programs for Embodied Control</strong>, ICRA, 2023.<br>
Jacky Liang, , Wenlong Huang, F. Xia, Peng Xu, Karol Hausman, Brian Ichter, Peter R. Florence, Andy Zeng.<br>
[<a href="https://arxiv.org/pdf/2209.07753" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Reasoning with Language Model Is Planning with World Model</strong>, Arxiv, 2023.<br>
Hao, Shibo, Yi, Gu, Haodi, Ma, Joshua Jiahua, Hong, Zhen, Wang, Daisy Zhe, Wang, Zhiting, Hu.<br>
[<a href="https://arxiv.org/pdf/2305.14992" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement</strong>, arXiv, 2023.<br>
Haonan Chang, Kai Gao, Kowndinya Boyalakuntla, Alex Lee, Baichuan Huang, Harish Udhaya Kumar, Jinjin Yu, Abdeslam Boularias.<br>
[<a href="https://arxiv.org/pdf/2309.15821" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Translating Natural Language to Planning Goals with Large-Language Models</strong>, arXiv, 2023.<br>
Xie, Yaqi, Chen, Yu, Tongyao, Zhu, Jinbin, Bai, Ze, Gong, Harold, Soh.<br>
[<a href="https://arxiv.org/pdf/2302.05128" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>LLM+P: Empowering Large Language Models with Optimal Planning Proficiency</strong>, arXiv, 2023.<br>
Liu, Bo, Yuqian, Jiang, Xiaohan, Zhang, Qiang, Liu, Shiqi, Zhang, Joydeep, Biswas, Peter, Stone.<br>
[<a href="https://arxiv.org/pdf/2304.11477" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Dynamic Planning with a LLM</strong>, arXiv, 2023.<br>
Dagan, Gautier, Frank, Keller, Alex, Lascarides.<br>
[<a href="https://arxiv.org/pdf/2308.06391" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Embodied Task Planning with Large Language Models</strong>, arXiv, 2023.<br>
Wu, Zhenyu, Ziwei, Wang, Xiuwei, Xu, Jiwen, Lu, Haibin, Yan.<br>
[<a href="https://arxiv.org/pdf/2307.01848" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning</strong>, Conference on Robot Learning. 2023.<br>
Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian D. Reid, Niko Sunderhauf.<br>
[<a href="https://arxiv.org/pdf/2307.06135" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning</strong>, ArXiv, 2023.<br>
Qiao Gu, Ali Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Ramalingam Chellappa, Chuang Gan, Celso Miguel de Melo, Joshua B Tenenbaum, Antonio Torralba, Florian Shkurti, Liam Paull.<br>
[<a href="https://arxiv.org/pdf/2309.16650" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks</strong>, arXiv, 2023.<br>
Yaran Chen, Wenbo Cui, Yuanwen Chen, Mining Tan, Xinyao Zhang, Dong Zhao, He Wang.<br>
[<a href="https://arxiv.org/pdf/2311.15649" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Chat with the Environment: Interactive Multimodal Perception Using Large Language Models</strong>, IROS, 2023.<br>
Zhao, Xufeng, Mengdi, Li, Cornelius, Weber, Muhammad Burhan, Hafez, Stefan, Wermter.<br>
[<a href="https://arxiv.org/pdf/2303.08268" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Video Language Planning</strong>, arxiv, 2023.<br>
Du, Yilun, Mengjiao, Yang, Pete, Florence, Fei, Xia, Ayzaan, Wahid, Brian, Ichter, Pierre, Sermanet, Tianhe, Yu, Pieter, Abbeel, Joshua B., Tenenbaum, Leslie, Kaelbling, Andy, Zeng, Jonathan, Tompson.<br>
[<a href="https://arxiv.org/pdf/2310.10625" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Code as Policies: Language Model Programs for Embodied Control</strong>, ICRA, 2023,<br>
Jacky Liang, Wenlong Huang, F. Xia, Peng Xu, Karol Hausman, Brian Ichter, Peter R. Florence, Andy Zeng.<br>
[<a href="https://arxiv.org/pdf/2209.07753" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Reflexion: an autonomous agent with dynamic memory and self-reflection</strong>, ArXiv, 2023.<br>
Noah Shinn, Beck Labash, A. Gopinath.<br>
[<a href="/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents</strong>, Proceedings of the 37th International Conference on Neural Information Processing Systems, 2023.<br>
Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang.<br>
[<a href="https://arxiv.org/pdf/2302.01560" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model</strong>, ArXiv, 2023.<br>
Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, and Hongsheng Li.<br>
<a href="https://github.com/OpenGVLab/Instruct2Act">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Cliport: What and where pathways for robotic manipulation</strong>, Conference on robot learning, 2022.<br>
Shridhar, Mohit, Lucas, Manuelli, Dieter, Fox.<br>
[<a href="https://proceedings.mlr.press/v164/shridhar22a/shridhar22a.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents</strong>, ICML, 2022.<br>
Huang, Wenlong, Pieter, Abbeel, Deepak, Pathak, Igor, Mordatch.<br>
[<a href="https://proceedings.mlr.press/v162/huang22a/huang22a.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Inner Monologue: Embodied Reasoning through Planning with Language Models</strong>, Conference on Robot Learning, 2022.<br>
Huang, Wenlong, Fei, Xia, Ted, Xiao, Harris, Chan, Jacky, Liang, Pete, Florence, Andy, Zeng, Jonathan, Tompson, Igor, Mordatch, Yevgen, Chebotar, Pierre, Sermanet, Noah, Brown, Tomas, Jackson, Linda, Luu, Sergey, Levine, Karol, Hausman, Brian, Ichter.<br>
[<a href="https://arxiv.org/pdf/2207.05608" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents</strong>, ICML, 2022.<br>
Huang, Wenlong, Pieter, Abbeel, Deepak, Pathak, Igor, Mordatch.<br>
[<a href="https://proceedings.mlr.press/v162/huang22a/huang22a.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language</strong>, ICLR, 2022.<br>
Zeng, Andy, Maria, Attarian, Brian, Ichter, Krzysztof, Choromanski, Adrian, Wong, Stefan, Welker, Federico, Tombari, Aveek, Purohit, Michael, Ryoo, Vikas, Sindhwani, Johnny, Lee, Vincent, Vanhoucke, Pete, Florence.<br>
[[page](Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language)]</p>
</li>
<li>
<p dir="auto"><strong>Skill Induction and Planning with Latent Language</strong>, ACL, 2021.<br>
Pratyusha Sharma, Antonio Torralba, Jacob Andreas.<br>
[<a href="https://arxiv.org/pdf/2110.01517" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>PDDL-the planning domain definition language</strong>, Technical Report. 1998.<br>
Drew McDermott, Malik Ghallab, Adele E. Howe, Craig A. Knoblock, Ashwin Ram, Manuela M. Veloso, Daniel S. Weld, David E. Wilkins.<br>
[<a href="https://www.researchgate.net/profile/Craig-Knoblock/publication/2278933_PDDL_-_The_Planning_Domain_Definition_Language/links/0912f50c0c99385e19000000/PDDL-The-Planning-Domain-Definition-Language.pdf" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>Strips: A new approach to the application of theorem proving to problem solving</strong>, Artificial Intelligence 2. 3(1971): 189-208.<br>
Richard E. Fikes, Nils J. Nilsson.<br>
[<a href="https://ntrs.nasa.gov/api/citations/19730013831/downloads/19730013831.pdf#page=98" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>A Formal Basis for the Heuristic Determination of Minimum Cost Paths</strong>, IEEE Trans. Syst. Sci. Cybern. 4. (1968): 100-107.<br>
Peter E. Hart, Nils J. Nilsson, Bertram Raphael.<br>
[<a href="https://ieeexplore.ieee.org/abstract/document/4082128" rel="nofollow">page</a>]</p>
</li>
<li>
<p dir="auto"><strong>The Monte Carlo method</strong>, Journal of the American Statistical Association 44 247. (1949): 335-41.<br>
Nicholas C. Metropolis, S. M. Ulam.<br>
[<a href="https://web.williams.edu/Mathematics/sjmiller/public_html/341Fa09/handouts/MetropolisUlam_TheMonteCarloMethod.pdf" rel="nofollow">page</a>]</p>
</li>
</ul>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><a id="user-content-sim-to-real"> Sim-to-Real Adaptation </a><a href="#table-of-contents"></a> </h2><a id="user-content--sim-to-real-adaptation--" class="anchor" aria-label="Permalink:  Sim-to-Real Adaptation " href="#-sim-to-real-adaptation--"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>
<p dir="auto"><strong>VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks</strong>, arxiv, 2024<br>
Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, Xipeng Qiu<br>
<a href="https://arxiv.org/pdf/2412.18194" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>PIVOT-R: Primitive-Driven Waypoint-Aware World Model for Robotic Manipulation</strong>, NeurIPS, 2024<br>
Kaidong Zhang, Pengzhen Ren, Bingqian Lin, Junfan Lin, Shikui Ma, Hang Xu, Xiaodan Liang<br>
<a href="https://arxiv.org/pdf/2410.10394" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Data Scaling Laws in Imitation Learning for Robotic Manipulation</strong>, arxiv, 2024<br>
Fanqi Lin, Yingdong Hu, Pingyue Sheng, Chuan Wen, Jiacheng You, Yang Gao<br>
<a href="https://arxiv.org/pdf/2410.18647" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Evaluating Real-World Robot Manipulation Policies in Simulation</strong>, arxiv, 2024<br>
Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, Ted Xiao<br>
<a href="https://arxiv.org/pdf/2405.05941" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Body Transformer: Leveraging Robot Embodiment for Policy Learning</strong>, arxiv, 2024<br>
Carmelo Sferrazza, Dun-Ming Huang, Fangchen Liu, Jongmin Lee, Pieter Abbeel<br>
<a href="https://arxiv.org/pdf/2408.06316" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Autonomous Behavior Planning For Humanoid Loco-manipulation Through Grounded Language Model</strong>, arxiv, 2024<br>
Jin Wang, Arturo Laurenzi, Nikos Tsagarakis<br>
<a href="https://arxiv.org/pdf/2408.082827" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Robust agents learn causal world models</strong>, ICLR, 2024<br>
Richens, Jonathan, and Tom Everitt<br>
<a href="https://arxiv.org/pdf/2402.10877" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots</strong>, arXiv 2024<br>
Chi, Cheng and Xu, Zhenjia and Pan, Chuer and Cousineau, Eric and Burchfiel, Benjamin and Feng, Siyuan and Tedrake, Russ and Song, Shuran<br>
<a href="https://arxiv.org/abs/2402.10329" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation</strong>, arXiv, 2024<br>
Fu, Zipeng and Zhao, Tony Z and Finn, Chelsea<br>
<a href="https://mobile-aloha.github.io/resources/mobile-aloha.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Human-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition</strong>, arXiv, 2024<br>
Luo, Shengcheng and Peng, Quanquan and Lv, Jun and Hong, Kaiwen and Driggs-Campbell, Katherine Rose and Lu, Cewu and Li, Yong-Lu<br>
<a href="https://arxiv.org/pdf/2407.00299" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation</strong>, arXiv, 2024<br>
Torne, Marcel and Simeonov, Anthony and Li, Zechu and Chan, April and Chen, Tao and Gupta, Abhishek and Agrawal, Pulkit<br>
<a href="https://arxiv.org/abs/2403.03949" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction</strong>, arXiv, 2024<br>
Jiang, Yunfan and Wang, Chen and Zhang, Ruohan and Wu, Jiajun and Fei-Fei, Li<br>
<a href="https://arxiv.org/abs/2405.10315" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Natural Language Can Help Bridge the Sim2Real Gap</strong>, arXiv, 2024<br>
Yu, Albert and Foote, Adeline and Mooney, Raymond and Mart{'\i}n-Mart{'\i}n, Roberto<br>
<a href="https://arxiv.org/pdf/2405.10020" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Visual Whole-Body Control for Legged Loco-Manipulation</strong>, arXiv, 2024<br>
Liu, Minghuan and Chen, Zixuan and Cheng, Xuxin and Ji, Yandong and Yang, Ruihan and Wang, Xiaolong<br>
<a href="https://arxiv.org/pdf/2403.16967" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Expressive Whole-Body Control for Humanoid Robots</strong>, arXiv, 2024<br>
Cheng, Xuxin and Ji, Yandong and Chen, Junming and Yang, Ruihan and Yang, Ge and Wang, Xiaolong<br>
<a href="https://arxiv.org/pdf/2402.16796" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Pandora: Towards General World Model with Natural Language Actions and Video States</strong>, arXiv, 2024<br>
Xiang, Jiannan and Liu, Guangyi and Gu, Yi and Gao, Qiyue and Ning, Yuting and Zha, Yuheng and Feng, Zeyu and Tao, Tianhua and Hao, Shibo and Shi, Yemin and others<br>
<a href="https://arxiv.org/abs/2406.09455" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>3D-VLA: A 3D Vision-Language-Action Generative World Model</strong>, ICML, 2024<br>
Zhen, Haoyu and Qiu, Xiaowen and Chen, Peihao and Yang, Jincheng and Yan, Xin and Du, Yilun and Hong, Yining and Gan, Chuang<br>
<a href="https://openreview.net/forum?id=EZcFK8HupF" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning</strong>, arXiv, 2024<br>
Ding, Zihan and Zhang, Amy and Tian, Yuandong and Zheng, Qinqing<br>
<a href="https://arxiv.org/abs/2402.03570" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features</strong>, ICLR, 2024<br>
Bardes, Adrien and Ponce, Jean and LeCun, Yann<br>
<a href="https://openreview.net/forum?id=9XdLlbxZCC" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Learning and Leveraging World Models in Visual Representation Learning</strong>, arXiv, 2024<br>
Garrido, Quentin and Assran, Mahmoud and Ballas, Nicolas and Bardes, Adrien and Najman, Laurent and LeCun, Yann<br>
<a href="https://arxiv.org/abs/2403.00504" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>iVideoGPT: Interactive VideoGPTs are Scalable World Models</strong>, arXiv, 2024<br>
Wu, Jialong and Yin, Shaofeng and Feng, Ningya and He, Xu and Li, Dong and Hao, Jianye and Long, Mingsheng<br>
<a href="https://arxiv.org/abs/2405.15223" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Spatiotemporal Predictive Pre-training for Robotic Motor Control</strong>, arXiv, 2024<br>
Yang, Jiange and Liu, Bei and Fu, Jianlong and Pan, Bocheng and Wu, Gangshan and Wang, Limin<br>
<a href="https://arxiv.org/abs/2403.05304" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>LEGENT: Open Platform for Embodied Agents</strong>, arXiv, 2024<br>
Cheng, Zhili and Wang, Zhitong and Hu, Jinyi and Hu, Shengding and Liu, An and Tu, Yuge and Li, Pengkai and Shi, Lei and Liu, Zhiyuan and Sun, Maosong<br>
<a href="https://arxiv.org/abs/2404.18243" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Point-JEPA: A Joint Embedding Predictive Architecture for Self-Supervised Learning on Point Cloud</strong>, arXiv, 2024<br>
Saito, Ayumu and Poovvancheri, Jiju<br>
<a href="https://arxiv.org/abs/2404.16432" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>MuDreamer: Learning Predictive World Models without Reconstruction</strong>, ICLR, 2024<br>
Burchi, Maxime and Timofte, Radu<br>
<a href="https://openreview.net/forum?id=9pe38WpsbX" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought</strong>, arXiv, 2024<br>
Wong, Lionel and Grand, Gabriel and Lew, Alexander K and Goodman, Noah D and Mansinghka, Vikash K and Andreas, Jacob and Tenenbaum, Joshua B<br>
<a href="https://arxiv.org/abs/2306.12672" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>ElastoGen: 4D Generative Elastodynamics</strong>, arXiv, 2024<br>
Feng, Yutao and Shang, Yintong and Feng, Xiang and Lan, Lei and Zhe, Shandian and Shao, Tianjia and Wu, Hongzhi and Zhou, Kun and Su, Hao and Jiang, Chenfanfu and others<br>
<a href="https://arxiv.org/abs/2405.15056" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Lifelike agility and play in quadrupedal robots using reinforcement learning and generative pre-trained models</strong>, Nature Machine Intelligence, 2024.<br>
Lei Han, Qingxu Zhu, Jiapeng Sheng, Chong Zhang, Tingguang Li, Yizheng Zhang, He Zhang et al.<br>
<a href="https://www.nature.com/articles/s42256-024-00861-3" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Model Adaptation for Time Constrained Embodied Control</strong>, CVPR, 2024.<br>
Jaehyun Song, Minjong Yoo, Honguk Woo.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Song_Model_Adaptation_for_Time_Constrained_Embodied_Control_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation</strong>, CVPR, 2024.<br>
Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, Hao Dong.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_ManipLLM_Embodied_Multimodal_Large_Language_Model_for_Object-Centric_Robotic_Manipulation_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation</strong>, CVPR, 2024.<br>
Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, Hao Dong.<br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_ManipLLM_Embodied_Multimodal_Large_Language_Model_for_Object-Centric_Robotic_Manipulation_CVPR_2024_paper.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>GenH2R: Learning Generalizable Human-to-Robot Handover via Scalable Simulation, Demonstration, and Imitation</strong>, CVPR, 2024.<br>
Zifan Wang, Junyu Chen, Ziqing Chen, Pengwei Xie, Rui Chen, Li Yi.<br>
<a href="https://genh2r.github.io/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>SAGE: Bridging Semantic and Actionable Parts for Generalizable Manipulation of Articulated Objects</strong>, RSS, 2024.<br>
Haoran Geng, Songlin Wei, Congyue Deng, Bokui Shen, He Wang, Leonidas Guibas.<br>
<a href="https://arxiv.org/abs/2312.01307" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>GAMMA: Graspability-Aware Mobile MAnipulation Policy Learning based on Online Grasping Pose Fusion</strong>, ICRA, 2024.<br>
Jiazhao Zhang, Nandiraju Gireesh, Jilong Wang, Xiaomeng Fang, Chaoyi Xu, Weiguang Chen, Liu Dai, He Wang.<br>
<a href="https://arxiv.org/pdf/2309.15459" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>ReALFRED: An Embodied Instruction Following Benchmark in Photo-Realistic Environments</strong>, ECCV, 2024.<br>
Taewoong Kim, Cheolhong Min, Byeonghwi Kim, Jinyeon Kim, Wonje Jeung, Jonghyun Choi.<br>
<a href="https://arxiv.org/pdf/2407.18550" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>DISCO: Embodied Navigation and Interaction via Differentiable Scene Semantics and Dual-level Control</strong>, ECCV, 2024.<br>
Xinyu Xu, Shengcheng Luo, Yanchao Yang, Yong-Lu Li, Cewu Lu.<br>
<a href="https://arxiv.org/pdf/2407.14758" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>DynSyn: Dynamical Synergistic Representation for Efficient Learning and Control in Overactuated Embodied Systems</strong>, ICML, 2024.<br>
Kaibo He, Chenhui Zuo, Chengtian Ma, Yanan Sui.<br>
<a href="https://arxiv.org/pdf/2407.11472" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>A-JEPA: Joint-Embedding Predictive Architecture Can Listen</strong>, arXiv, 2023<br>
Fei, Zhengcong and Fan, Mingyuan and Huang, Junshi<br>
<a href="https://arxiv.org/abs/2311.15830" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization</strong>, NeurIPS, 2023<br>
Liu, Minghua and Xu, Chao and Jin, Haian and Chen, Linghao and Varma T, Mukund and Xu, Zexiang and Su, Hao<br>
<a href="https://openreview.net/forum?id=A6X9y8n4sT" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Introduction to Latent Variable Energy-Based Models: A Path Towards Autonomous Machine Intelligence</strong>, arXiv, 2023<br>
Dawid, Anna and LeCun, Yann<br>
<a href="https://arxiv.org/abs/2306.02572" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts</strong>, CVPR, 2023<br>
Geng, Haoran and Xu, Helin and Zhao, Chengyang and Xu, Chao and Yi, Li and Huang, Siyuan and Wang, He<br>
<a href="https://ieeexplore.ieee.org/abstract/document/10203924" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Reward-Adaptive Reinforcement Learning: Dynamic Policy Gradient Optimization for Bipedal Locomotion</strong>, IEEE TPAMI, 2023<br>
Huang, Changxin and Wang, Guangrun and Zhou, Zhibo and Zhang, Ronghui and Lin, Liang<br>
<a href="https://www.computer.org/csdl/journal/tp/2023/06/09956746/1Iu2CDAJBcc" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware</strong>, ICML, 2023<br>
Zhao, Tony Z and Kumar, Vikash and Levine, Sergey and Finn, Chelsea<br>
<a href="https://openreview.net/forum?id=e8Eu1lqLaf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Surfer: Progressive Reasoning with World Models for Robotic Manipulation</strong>, arxiv, 2023.<br>
Pengzhen Ren, Kaidong Zhang, Hetao Zheng, Zixuan Li, Yuhang Wen, Fengda Zhu, Mas Ma, Xiaodan Liang.<br>
<a href="https://arxiv.org/pdf/2306.11335" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>PartManip: Learning Cross-Category Generalizable Part Manipulation Policy from Point Cloud Observations</strong>, CVPR, 2023.<br>
Haoran Geng, Ziming Li, Yiran Geng, Jiayi Chen, Hao Dong, He Wang.<br>
<a href="https://pku-epic.github.io/PartManip/" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27</strong>, Open Review, 2022<br>
Yann LeCun<br>
<a href="https://openreview.net/pdf?id=BZ5a1r-kVsf&amp;" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Real2Sim2Real: Self-Supervised Learning of Physical Single-Step Dynamic Actions for Planar Robot Casting</strong>, ICRA, 2022<br>
Lim, Vincent and Huang, Huang and Chen, Lawrence Yunliang and Wang, Jonathan and Ichnowski, Jeffrey and Seita, Daniel and Laskey, Michael and Goldberg, Ken<br>
<a href="https://dl.acm.org/doi/abs/10.1109/ICRA46639.2022.9811651" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Continuous Jumping for Legged Robots on Stepping Stones via Trajectory Optimization and Model Predictive Control</strong>, IEEE CDC, 2022<br>
Nguyen, Chuong and Bao, Lingfan and Nguyen, Quan<br>
<a href="https://arxiv.org/pdf/2204.01147" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Reward-Adaptive Reinforcement Learning: Dynamic Policy Gradient Optimization for Bipedal Locomotion</strong>, TPAMI, 2022.<br>
Changxin Huang, Guangrun Wang, Zhibo Zhou, Ronghui Zhang, Liang Lin.<br>
<a href="https://ieeexplore.ieee.org/abstract/document/9956746" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Transporter Networks: Rearranging the Visual World for Robotic Manipulation</strong>, CoRL, 2021<br>
Zeng, Andy and Florence, Pete and Tompson, Jonathan and Welker, Stefan and Chien, Jonathan and Attarian, Maria and Armstrong, Travis and Krasin, Ivan and Duong, Dan and Sindhwani, Vikas and others<br>
<a href="https://proceedings.mlr.press/v155/zeng21a.html" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>The MIT Humanoid Robot: Design, Motion Planning, and Control for Acrobatic Behaviors</strong>, IEEE-RAS 20th International Conference on Humanoid Robots (Humanoids), 2021<br>
Chignoli, Matthew and Kim, Donghyun and Stanger-Jones, Elijah and Kim, Sangbae<br>
<a href="https://arxiv.longhoe.net/pdf/2104.09025" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization</strong>, IROS, 2020<br>
Kaspar, Manuel and Osorio, Juan D Mu{~n}oz and Bock, Jurgen<br>
<a href="https://ieeexplore.ieee.org/abstract/document/9341260" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Learning Dexterous In-Hand Manipulation</strong>, The International Journal of Robotics Research, 2020<br>
Andrychowicz, OpenAI: Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and others<br>
<a href="https://journals.sagepub.com/doi/full/10.1177/0278364919887447" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>DeepGait: Planning and Control of Quadrupedal Gaits using Deep Reinforcement Learning</strong>, IEEE Robotics and Automation Letters, 2020<br>
Tsounis, Vassilios and Alge, Mitja and Lee, Joonho and Farshidian, Farbod and Hutter, Marco<br>
<a href="https://arxiv.org/pdf/1909.08399" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Optimized Jumping on the MIT Cheetah 3 Robot</strong>, ICRA, 2019<br>
Nguyen, Quan and Powell, Matthew J and Katz, Benjamin and Di Carlo, Jared and Kim, Sangbae<br>
<a href="https://ieeexplore.ieee.org/abstract/document/8794449" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>World Models</strong>, NIPS, 2018<br>
Ha, David and Schmidhuber, Jurgen<br>
<a href="https://mx.nthu.edu.tw/~jlliu/teaching/AI17/Auto8.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>MIT Cheetah 3: Design and Control of a Robust, Dynamic Quadruped Robot</strong>, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2018<br>
Bledt, Gerardo and Powell, Matthew J and Katz, Benjamin and Di Carlo, Jared and Wensing, Patrick M and Kim, Sangbae<br>
<a href="https://dspace.mit.edu/bitstream/handle/1721.1/126619/iros.pdf?sequence=2" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Sim-to-Real Reinforcement Learning for Deformable Object Manipulation</strong>, CoRL, 2018<br>
Matas, Jan and James, Stephen and Davison, Andrew J<br>
<a href="http://proceedings.mlr.press/v87/matas18a/matas18a.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Dynamic Walking on Randomly-Varying Discrete Terrain With One-Step Preview</strong>, Robotics: Science and Systems, 2017<br>
Nguyen, Quan and Agrawal, Ayush and Da, Xingye and Martin, William C and Geyer, Hartmut and Grizzle, Jessy W and Sreenath, Koushil<br>
<a href="https://hybrid-robotics.berkeley.edu/publications/RSS2017_DiscreteTerrain_Walking.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Deep Kernels for Optimizing Locomotion Controllers</strong>, CoRL, 2017<br>
Antonova, Rika and Rai, Akshara and Atkeson, Christopher G<br>
<a href="http://proceedings.mlr.press/v78/antonova17a/antonova17a.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Preparing for the Unknown: Learning a Universal Policy with Online System Identification</strong>, RSS, 2017<br>
Yu, Wenhao and Tan, Jie and Liu, C Karen and Turk, Greg<br>
<a href="https://arxiv.org/abs/1702.02453" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World</strong>, IROS, 2017<br>
Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter<br>
<a href="https://ieeexplore.ieee.org/abstract/document/8202133" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Practice Makes Perfect: An Optimization-Based Approach to Controlling Agile Motions for a Quadruped Robot</strong>, IEEE Robotics &amp; Automation Magazine, 2016<br>
Gehring, Christian and Coros, Stelian and Hutter, Marco and Bellicoso, Carmine Dario and Heijnen, Huub and Diethelm, Remo and Bloesch, Michael and Fankhauser, P{'e}ter and Hwangbo, Jemin and Hoepflinger, Mark and others<br>
<a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/183161.1/1/eth-49107-01.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>ANYmal - a highly mobile and dynamic quadrupedal robot</strong>, IEEE/RSJ international conference on intelligent robots and systems (IROS), 2016<br>
Hutter, Marco and Gehring, Christian and Jud, Dominic and Lauber, Andreas and Bellicoso, C Dario and Tsounis, Vassilios and Hwangbo, Jemin and Bodie, Karen and Fankhauser, Peter and Bloesch, Michael and others<br>
<a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/118642/eth-49454-01.pdf;sequence=1" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Optimization Based Full Body Control for the Atlas Robot</strong>, IEEE-RAS International Conference on Humanoid Robots, 2014<br>
Feng, Siyuan and Whitman, Eric and Xinjilefu, X and Atkeson, Christopher G<br>
<a href="http://www.cs.cmu.edu/afs/cs/user/sfeng/www/sf_hum14.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>A Compliant Hybrid Zero Dynamics Controller for Stable, Efficient and Fast Bipedal Walking on MABEL</strong>, The International Journal of Robotics Research, 2011<br>
Sreenath, Koushil and Park, Hae-Won and Poulakakis, Ioannis and Grizzle, Jessy W<br>
<a href="https://sites.udel.edu/poulakas/files/2022/10/J07-A-Compliant-Hybrid-Zero-Dynamics-Controller.pdf" rel="nofollow">[page]</a></p>
</li>
<li>
<p dir="auto"><strong>Dynamic walk of a biped</strong>, The International Journal of Robotics Research, 1984<br>
Miura, Hirofumi and Shimoyama, Isao<br>
<a href="https://journals.sagepub.com/doi/abs/10.1177/027836498400300206" rel="nofollow">[page]</a></p>
</li>
</ul>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><a id="user-content-datasets"> Datasets </a><a href="#table-of-contents"></a> </h2><a id="user-content--datasets--" class="anchor" aria-label="Permalink:  Datasets " href="#-datasets--"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To be updated...</p>
<ul dir="auto">
<li><strong>RefSpatial</strong>, 2025. <a href="https://huggingface.co/datasets/JingkunAn/RefSpatial" rel="nofollow">[link]</a></li>
<li><strong>VisualAgentBench</strong>, 2023.<a href="https://github.com/THUDM/VisualAgentBench">link</a></li>
<li><strong>Open X-Embodiment</strong>, 2023.<a href="https://robotics-transformer-x.github.io/" rel="nofollow">link</a></li>
<li><strong>RH20T-P</strong>, 2024.<a href="https://sites.google.com/view/rh20t-primitive/main" rel="nofollow">link</a></li>
<li><strong>ALOHA 2</strong>, 2024.<a href="https://aloha-2.github.io/" rel="nofollow">link</a></li>
<li><strong>GRUtopia</strong>, 2024.<a href="https://github.com/OpenRobotLab/GRUtopia">link</a></li>
<li><strong>ARIO (All Robots In One)</strong>, 2024.<a href="https://imaei.github.io/project_pages/ario/" rel="nofollow">link</a></li>
<li><strong>VLABench</strong>, 2024.<a href="https://vlabench.github.io/" rel="nofollow">link</a></li>
<li><strong>Matterport3D</strong>, 2017. <a href="https://github.com/niessner/Matterport">[link]</a></li>
<li><strong>RoboMIND</strong>, 2025. <a href="https://x-humanoid-robomind.github.io/" rel="nofollow">[link]</a></li>
</ul>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">Embodied Perception</h3><a id="user-content-embodied-perception" class="anchor" aria-label="Permalink: Embodied Perception" href="#embodied-perception"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div class="markdown-heading" dir="auto"><h4 tabindex="-1" class="heading-element" dir="auto">Vision</h4><a id="user-content-vision" class="anchor" aria-label="Permalink: Vision" href="#vision"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><strong>BEHAVIOR Vision Suite</strong>, 2024. <a href="https://behavior-vision-suite.github.io/" rel="nofollow">[link]</a></li>
<li><strong>SpatialQA</strong>, 2024.<a href="https://github.com/BAAI-DCAI/SpatialBot">[link]</a></li>
<li><strong>SpatialBench</strong>, 2024. <a href="https://huggingface.co/datasets/RussRobin/SpatialBench" rel="nofollow">[link]</a></li>
<li><strong>Uni3DScenes</strong>, 2024. <a href="https://huggingface.co/datasets/RussRobinSpatialBench" rel="nofollow">[link]</a></li>
<li><strong>Active Recognition Dataset</strong>, 2023. <a href="https://leifan95.github.io/_pages/AR-dataset/index.html" rel="nofollow">[link]</a></li>
<li><strong>Baxter_UR5_95_Objects_Dataset</strong>, 2023. <a href="https://www.eecs.tufts.edu/~gtatiya/pages/2022/Baxter_UR5_95_Objects_Dataset.html" rel="nofollow">[link]</a></li>
<li><strong>Caltech-256</strong>, 2022. <a href="https://data.caltech.edu/records/nyy15-4j048" rel="nofollow">[link]</a></li>
<li><strong>DIDI Dataset</strong>, 2020. <a href="https://github.com/google-research/google-research/blob/master/didi_dataset/README.md">[link]</a></li>
<li><strong>Replica</strong>, 2019. <a href="https://github.com/facebookresearch/Replica-Dataset">[link]</a></li>
<li><strong>ScanObjectNN</strong>, 2019. <a href="https://hkust-vgd.github.io/scanobjectnn/" rel="nofollow">[link]</a></li>
<li><strong>OCID Dataset</strong>, 2019. <a href="https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/object-clutter-indoor-dataset/" rel="nofollow">[link]</a></li>
<li><strong>L3RScan</strong>, 2019. <a href="https://github.com/WaldJohannaU/3RScan">[link]</a></li>
<li><strong>EmbodiedScan</strong>, 2019. <a href="https://docs.google.com/forms/d/e/1FAIpQLScUXEDTksGiqHZp31j7Zp7zlCNV7p_08uViwP_Nbzfn3g6hhw/viewform" rel="nofollow">[link]</a></li>
<li><strong>UZH-FPV Dataset</strong>, 2019. <a href="https://fpv.ifi.uzh.ch/" rel="nofollow">[link]</a></li>
<li><strong>LM Data</strong>, 2019. <a href="https://peringlab.org/lmdata/" rel="nofollow">[link]</a></li>
<li><strong>TUM Visual-Inertial Dataset</strong>, 2018. <a href="https://cvg.cit.tum.de/data/datasets/visual-inertial-dataset" rel="nofollow">[link]</a></li>
<li><strong>ScanNet</strong>, 2017. <a href="https://github.com/ScanNet/ScanNet">[link]</a></li>
<li><strong>SUNCG</strong>, 2017. <a href="http://suncg.cs.princeton.edu/" rel="nofollow">[link]</a></li>
<li><strong>Semantic 3D</strong>, 2017. <a href="http://www.semantic3d.net/" rel="nofollow">[link]</a></li>
<li><strong>ScanNet v2</strong>, 2017. <a href="https://github.com/ScanNet/ScanNet">[link]</a></li>
<li><strong>S3DIS</strong>, 2016. <a href="http://buildingparser.stanford.edu/" rel="nofollow">[link]</a></li>
<li><strong>Synthia</strong>, 2016. <a href="https://synthia-dataset.net/" rel="nofollow">[link]</a></li>
<li><strong>ModelNet</strong>, 2015. <a href="https://modelnet.cs.princeton.edu/" rel="nofollow">[link]</a></li>
<li><strong>ORBvoc</strong>, 2015. <a href="https://github.com/raulmur/ORB_SLAM">[link]</a></li>
<li><strong>Sketch dataset</strong>, 2015. <a href="https://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/" rel="nofollow">[link]</a></li>
<li><strong>SUN RGBD</strong>, 2015. <a href="https://rgbd.cs.princeton.edu/" rel="nofollow">[link]</a></li>
<li><strong>ShapeNet</strong>, 2015. <a href="https://shapenet.org/" rel="nofollow">[link]</a></li>
<li><strong>MVS Dataset</strong>, 2014. <a href="http://roboimagedata.compute.dtu.dk/?page_id=36" rel="nofollow">[link]</a></li>
<li><strong>SUOD</strong>, 2013. <a href="https://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml" rel="nofollow">[link]</a></li>
<li><strong>SUN360</strong>, 2012. <a href="https://vision.cs.princeton.edu/projects/2012/SUN360/data/" rel="nofollow">[link]</a></li>
<li><strong>NYU Depth Dataset V2</strong>, 2012. <a href="https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html" rel="nofollow">[link]</a></li>
<li><strong>TUM-RGBD</strong>, 2012. <a href="https://cvg.cit.tum.de/data/datasets/rgbd-dataset/download" rel="nofollow">[link]</a></li>
<li><strong>EuRoC MAV Dataset</strong>, 2012. <a href="https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets" rel="nofollow">[link]</a></li>
<li><strong>Semantic KITTI</strong>, 2012. <a href="https://www.semantic-kitti.org/dataset.html#download" rel="nofollow">[link]</a></li>
<li><strong>KITTI Object Recognition</strong>, 2012. <a href="http://www.cvlibs.net/datasets/kitti/eval_object.php" rel="nofollow">[link]</a></li>
<li><strong>Stanford Track Collection</strong>, 2011. <a href="http://cs.stanford.edu/people/teichman/stc/" rel="nofollow">[link]</a></li>
</ul>
<div class="markdown-heading" dir="auto"><h4 tabindex="-1" class="heading-element" dir="auto">Tactile</h4><a id="user-content-tactile" class="anchor" aria-label="Permalink: Tactile" href="#tactile"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><strong>Touch100k</strong>, 2024. <a href="https://cocacola-lab.github.io/Touch100k/" rel="nofollow">[link]</a></li>
<li><strong>ARIO (All Robots In One)</strong>, 2024. <a href="https://imaei.github.io/project_pages/ario/" rel="nofollow">[link]</a></li>
<li><strong>TaRF</strong>, 2024. <a href="https://dou-yiming.github.io/TaRF/" rel="nofollow">[link]</a></li>
<li><strong>TVL</strong>, 2024. <a href="https://huggingface.co/datasets/mlfu7/Touch-Vision-Language-Dataset" rel="nofollow">[link]</a></li>
<li><strong>YCB-Slide</strong>, 2022. <a href="https://github.com/rpl-cmu/YCB-Slide">[link]</a></li>
<li><strong>Touch and Go</strong>, 2022. <a href="https://drive.google.com/drive/folders/1NDasyshDCL9aaQzxjn_-Q5MBURRT360B" rel="nofollow">[link]</a></li>
<li><strong>SSVTP</strong>, 2022. <a href="https://drive.google.com/file/d/1H0B-jJ4l3tJu2zuqf-HbZy2bjEl-vL3f/view?usp=sharing" rel="nofollow">[link]</a></li>
<li><strong>ObjectFolder</strong>, 2021-2023. <a href="https://github.com/rhgao/ObjectFolder">[link]</a></li>
<li><strong>Decoding the BioTac</strong>, 2020. <a href="https://drive.google.com/drive/folders/1-BkqiFN9q6cz9Dk74oDlfmDs2m7ZvbWC" rel="nofollow">[link]</a></li>
<li><strong>SynTouch</strong>, 2019. <a href="https://tams.informatik.uni-hamburg.de/research/datasets/index.php#biotac_single_contact_response" rel="nofollow">[link]</a></li>
<li><strong>The Feeling of Success</strong>, 2017. <a href="https://sites.google.com/view/the-feeling-of-success/" rel="nofollow">[link]</a></li>
</ul>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">Embodied Navigation</h3><a id="user-content-embodied-navigation" class="anchor" aria-label="Permalink: Embodied Navigation" href="#embodied-navigation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><strong>ALFRED</strong>, 2020. <a href="https://askforalfred.com/" rel="nofollow">[link]</a></li>
<li><strong>REVERIE</strong>, 2020. <a href="https://github.com/YuankaiQi/REVERIE">[link]</a></li>
<li><strong>CVDN</strong>, 2019. <a href="https://github.com/mmurray/cvdn/">[link]</a></li>
<li><strong>Room to Room (R2R)</strong>, 2017. <a href="https://paperswithcode.com/dataset/room-to-room" rel="nofollow">[link]</a></li>
<li><strong>DivScene</strong>, 2024.<a href="https://github.com/zhaowei-wang-nlp/DivScene">[link]</a></li>
<li><strong>LH-VLN</strong>, 2025. <a href="https://hcplab-sysu.github.io/LH-VLN/" rel="nofollow">[link]</a></li>
</ul>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">Embodied Question Answering</h3><a id="user-content-embodied-question-answering" class="anchor" aria-label="Permalink: Embodied Question Answering" href="#embodied-question-answering"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><strong>SpatialQA</strong>, 2024. <a href="https://github.com/BAAI-DCAI/SpatialBot">[link]</a></li>
<li><strong>S-EQA</strong>, 2024. <a href="https://gamma.umd.edu/researchdirections/embodied/seqa/" rel="nofollow">[link]</a></li>
<li><strong>HM-EQA</strong>, 2024. <a href="https://github.com/Stanford-ILIAD/explore-eqa">[link]</a></li>
<li><strong>K-EQA</strong>, 2023. <a href="https://arxiv.org/abs/2109.07872" rel="nofollow">[link]</a></li>
<li><strong>SQA3D</strong>, 2023. <a href="https://sqa3d.github.io/" rel="nofollow">[link]</a></li>
<li><strong>VideoNavQA</strong>, 2019. <a href="https://github.com/catalina17/VideoNavQA">[link]</a></li>
<li><strong>MP3D-EQA</strong>, 2019. <a href="https://askforalfred.com/" rel="nofollow">[link]</a></li>
<li><strong>MT-EQA</strong>, 2019. <a href="https://github.com/facebookresearch/MT-EQA">[link]</a></li>
<li><strong>IQUAD V1</strong>, 2018. <a href="https://github.com/danielgordon10/thor-iqa-cvpr-2018">[link]</a></li>
<li><strong>EQA</strong>, 2018. <a href="https://embodiedqa.org/data" rel="nofollow">[link]</a></li>
</ul>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">Embodied Manipulation</h3><a id="user-content-embodied-manipulation" class="anchor" aria-label="Permalink: Embodied Manipulation" href="#embodied-manipulation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><strong>OAKINK2</strong>, 2024. <a href="https://oakink.net/v2/" rel="nofollow">[link]</a></li>
</ul>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto">Other Useful Embodied Projects &amp; Tools</h2><a id="user-content-other-useful-embodied-projects--tools" class="anchor" aria-label="Permalink: Other Useful Embodied Projects &amp; Tools" href="#other-useful-embodied-projects--tools"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">Resources</h3><a id="user-content-resources" class="anchor" aria-label="Permalink: Resources" href="#resources"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="https://github.com/zchoi/Awesome-Embodied-Agent-with-LLMs">Awesome-Embodied-Agent-with-LLMs</a><br>
<a href="https://github.com/ChanganVR/awesome-embodied-vision">Awesome Embodied Vision</a><br>
<a href="https://github.com/linchangyi1/Awesome-Touch">Awesome Touch</a></p>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">Simulate Platforms &amp; Enviroments</h3><a id="user-content-simulate-platforms--enviroments" class="anchor" aria-label="Permalink: Simulate Platforms &amp; Enviroments" href="#simulate-platforms--enviroments"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="https://github.com/facebookresearch/habitat-lab">Habitat-Lab</a><br>
<a href="https://github.com/facebookresearch/habitat-sim">Habitat-Sim</a><br>
<a href="https://github.com/StanfordVL/GibsonEnv">GibsonEnv</a><br>
<a href="https://github.com/thunlp/LEGENT">LEGENT</a><br>
<a href="https://metadriverse.github.io/metaurban/" rel="nofollow">MetaUrban</a><br>
<a href="https://github.com/OpenRobotLab/GRUtopia">GRUtopia</a><br>
<a href="https://genh2r.github.io/" rel="nofollow">GenH2R</a><br>
<a href="https://sites.google.com/view/humanthor/" rel="nofollow">Demonstrating HumanTHOR</a><br>
<a href="https://github.com/AutonoBot-Lab/BestMan_Pybullet">BestMan</a><br>
<a href="https://github.com/pzhren/InfiniteWorld">InfiniteWorld</a><br>
<a href="https://genesis-embodied-ai.github.io/" rel="nofollow">Genesis</a><br>
<a href="https://www.nvidia.com/en-us/ai/cosmos/" rel="nofollow">Cosmos</a></p>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto">Projects</h3><a id="user-content-projects" class="anchor" aria-label="Permalink: Projects" href="#projects"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>Manipulation</li>
</ul>
<p dir="auto"><a href="https://sites.google.com/view/robomamba-web" rel="nofollow">RoboMamba</a><br>
<a href="https://robot-ma.github.io/" rel="nofollow">MANIPULATE-ANYTHING</a><br>
<a href="https://pku-epic.github.io/DexGraspNet/" rel="nofollow">DexGraspNet</a><br>
<a href="https://pku-epic.github.io/UniDexGrasp/" rel="nofollow">UniDexGrasp</a><br>
<a href="https://pku-epic.github.io/UniDexGrasp++/" rel="nofollow">UniDexGrasp++</a><br>
<a href="https://oakink.net/v2" rel="nofollow">OAKINK2</a><br>
<a href="https://github.com/OpenDriveLab/agibot-world">AgiBot-World</a></p>
<ul dir="auto">
<li>Embodied Interaction</li>
</ul>
<p dir="auto"><a href="https://github.com/facebookresearch/EmbodiedQA">EmbodiedQA</a></p>
<ul dir="auto">
<li>Embodied Perception</li>
</ul>
<p dir="auto"><a href="https://github.com/OpenRobotLab/EmbodiedScan">EmbodiedScan</a></p>
<ul dir="auto">
<li>Models &amp; Tools</li>
</ul>
<p dir="auto"><a href="https://github.com/dongyh20/Octopus">Octopus</a><br>
<a href="https://github.com/allenai/Holodeck">Holodeck</a><br>
<a href="https://github.com/allenai/allenact">AllenAct</a></p>
<ul dir="auto">
<li>Agents</li>
</ul>
<p dir="auto"><a href="https://github.com/embodied-generalist/embodied-generalist">LEO</a><br>
<a href="https://github.com/MineDojo/Voyager">Voyager</a></p>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"> Citation</h2><a id="user-content-newspaper-citation" class="anchor" aria-label="Permalink: :newspaper: Citation" href="#newspaper-citation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">If you think this survey is helpful, please feel free to leave a star  and cite our paper:</p>
<div class="highlight highlight-text-bibtex notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="@article{liu2024aligning,
  title={Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI},
  author={Liu, Yang and Chen, Weixing and Bai, Yongjie and Liang, Xiaodan and Li, Guanbin and Gao, Wen and Lin, Liang},
  journal={arXiv preprint arXiv:2407.06886},
  year={2024}
}"><pre><span class="pl-k">@article</span>{<span class="pl-en">liu2024aligning</span>,
  <span class="pl-s">title</span>=<span class="pl-s"><span class="pl-pds">{</span>Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI<span class="pl-pds">}</span></span>,
  <span class="pl-s">author</span>=<span class="pl-s"><span class="pl-pds">{</span>Liu, Yang and Chen, Weixing and Bai, Yongjie and Liang, Xiaodan and Li, Guanbin and Gao, Wen and Lin, Liang<span class="pl-pds">}</span></span>,
  <span class="pl-s">journal</span>=<span class="pl-s"><span class="pl-pds">{</span>arXiv preprint arXiv:2407.06886<span class="pl-pds">}</span></span>,
  <span class="pl-s">year</span>=<span class="pl-s"><span class="pl-pds">{</span>2024<span class="pl-pds">}</span></span>
}</pre></div>
<div class="highlight highlight-text-bibtex notranslate position-relative overflow-auto" dir="auto" data-snippet-clipboard-copy-content="@article{liu2025aligning,
  title={Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI},
  author={Liu, Yang and Chen, Weixing and Bai, Yongjie and Liang, Xiaodan and Li, Guanbin and Gao, Wen and Lin, Liang},
  journal={IEEE/ASME Transactions on Mechatronics},
  year={2025}
}"><pre><span class="pl-k">@article</span>{<span class="pl-en">liu2025aligning</span>,
  <span class="pl-s">title</span>=<span class="pl-s"><span class="pl-pds">{</span>Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI<span class="pl-pds">}</span></span>,
  <span class="pl-s">author</span>=<span class="pl-s"><span class="pl-pds">{</span>Liu, Yang and Chen, Weixing and Bai, Yongjie and Liang, Xiaodan and Li, Guanbin and Gao, Wen and Lin, Liang<span class="pl-pds">}</span></span>,
  <span class="pl-s">journal</span>=<span class="pl-s"><span class="pl-pds">{</span>IEEE/ASME Transactions on Mechatronics<span class="pl-pds">}</span></span>,
  <span class="pl-s">year</span>=<span class="pl-s"><span class="pl-pds">{</span>2025<span class="pl-pds">}</span></span>
}</pre></div>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"> Acknowledgements</h2><a id="user-content--acknowledgements" class="anchor" aria-label="Permalink:  Acknowledgements" href="#-acknowledgements"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We sincerely thank Jingzhou Luo, Xinshuai Song, Kaixuan Jiang, Junyi Lin, Zhida Li, and Ganlong Zhao for their contributions.</p>
</article></div></div></div></div></div> <!-- --> <!-- --> <script type="application/json" id="__PRIMER_DATA_:R0:__">{"resolvedServerColorMode":"day"}</script></div>
</react-partial>


      <input type="hidden" data-csrf="true" value="nYKcZdhrdldQAz+rmyOJKF7vAdI0EqFvWbSvCvuDQv5U6RII62QY+YKy8aEZerL7SeCIUnHgbxJJG2S60oGARg==" />
</div>
  <div data-view-component="true" class="Layout-sidebar">      

      <div class="BorderGrid about-margin" data-pjax>
        <div class="BorderGrid-row">
          <div class="BorderGrid-cell">
            <div class="hide-sm hide-md">
  <h2 class="mb-3 h4">About</h2>

      <p class="f4 my-3">
        [Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI
      </p>

    <h3 class="sr-only">Topics</h3>
    <div class="my-3">
        <div class="f6">
      <a href="/topics/agent" title="Topic: agent" data-view-component="true" class="topic-tag topic-tag-link">
  agent
</a>
      <a href="/topics/robotics" title="Topic: robotics" data-view-component="true" class="topic-tag topic-tag-link">
  robotics
</a>
      <a href="/topics/navigation" title="Topic: navigation" data-view-component="true" class="topic-tag topic-tag-link">
  navigation
</a>
      <a href="/topics/survey" title="Topic: survey" data-view-component="true" class="topic-tag topic-tag-link">
  survey
</a>
      <a href="/topics/manipulation" title="Topic: manipulation" data-view-component="true" class="topic-tag topic-tag-link">
  manipulation
</a>
      <a href="/topics/interaction" title="Topic: interaction" data-view-component="true" class="topic-tag topic-tag-link">
  interaction
</a>
      <a href="/topics/causality" title="Topic: causality" data-view-component="true" class="topic-tag topic-tag-link">
  causality
</a>
      <a href="/topics/reasoning" title="Topic: reasoning" data-view-component="true" class="topic-tag topic-tag-link">
  reasoning
</a>
      <a href="/topics/embodied-ai" title="Topic: embodied-ai" data-view-component="true" class="topic-tag topic-tag-link">
  embodied-ai
</a>
      <a href="/topics/percpetion" title="Topic: percpetion" data-view-component="true" class="topic-tag topic-tag-link">
  percpetion
</a>
  </div>

    </div>

    <h3 class="sr-only">Resources</h3>
    <div class="mt-2">
      <a class="Link--muted" data-analytics-event="{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:readme&quot;}" href="#readme-ov-file">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-book mr-2">
    <path d="M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z"></path>
</svg>
        Readme
</a>    </div>

  





  <include-fragment src="/HCPLab-SYSU/Embodied_AI_Paper_List/hovercards/citation/sidebar_partial?tree_name=main" data-nonce="v2:b0864956-446a-ccd6-cec6-a695a0cbc5f5" data-view-component="true">
  

  <div data-show-on-forbidden-error hidden>
    <div class="Box">
  <div class="blankslate-container">
    <div data-view-component="true" class="blankslate blankslate-spacious color-bg-default rounded-2">
      

      <h3 data-view-component="true" class="blankslate-heading">        Uh oh!
</h3>
      <p data-view-component="true">        <p class="color-fg-muted my-2 mb-2 ws-normal">There was an error while loading. <a class="Link--inTextBlock" data-turbo="false" href="" aria-label="Please reload this page">Please reload this page</a>.</p>
</p>

</div>  </div>
</div>  </div>
</include-fragment>
    <div class="mt-2">
      <a href="/HCPLab-SYSU/Embodied_AI_Paper_List/activity" data-view-component="true" class="Link Link--muted"><svg text="gray" aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-pulse mr-2">
    <path d="M6 2c.306 0 .582.187.696.471L10 10.731l1.304-3.26A.751.751 0 0 1 12 7h3.25a.75.75 0 0 1 0 1.5h-2.742l-1.812 4.528a.751.751 0 0 1-1.392 0L6 4.77 4.696 8.03A.75.75 0 0 1 4 8.5H.75a.75.75 0 0 1 0-1.5h2.742l1.812-4.529A.751.751 0 0 1 6 2Z"></path>
</svg>
        <span class="color-fg-muted">Activity</span></a>    </div>


    <h3 class="sr-only">Stars</h3>
    <div class="mt-2">
      <a href="/HCPLab-SYSU/Embodied_AI_Paper_List/stargazers" data-view-component="true" class="Link Link--muted"><svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-star mr-2">
    <path d="M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z"></path>
</svg>
        <strong>1.8k</strong>
        stars</a>    </div>

    <h3 class="sr-only">Watchers</h3>
    <div class="mt-2">
      <a href="/HCPLab-SYSU/Embodied_AI_Paper_List/watchers" data-view-component="true" class="Link Link--muted"><svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-eye mr-2">
    <path d="M8 2c1.981 0 3.671.992 4.933 2.078 1.27 1.091 2.187 2.345 2.637 3.023a1.62 1.62 0 0 1 0 1.798c-.45.678-1.367 1.932-2.637 3.023C11.67 13.008 9.981 14 8 14c-1.981 0-3.671-.992-4.933-2.078C1.797 10.83.88 9.576.43 8.898a1.62 1.62 0 0 1 0-1.798c.45-.677 1.367-1.931 2.637-3.022C4.33 2.992 6.019 2 8 2ZM1.679 7.932a.12.12 0 0 0 0 .136c.411.622 1.241 1.75 2.366 2.717C5.176 11.758 6.527 12.5 8 12.5c1.473 0 2.825-.742 3.955-1.715 1.124-.967 1.954-2.096 2.366-2.717a.12.12 0 0 0 0-.136c-.412-.621-1.242-1.75-2.366-2.717C10.824 4.242 9.473 3.5 8 3.5c-1.473 0-2.825.742-3.955 1.715-1.124.967-1.954 2.096-2.366 2.717ZM8 10a2 2 0 1 1-.001-3.999A2 2 0 0 1 8 10Z"></path>
</svg>
        <strong>24</strong>
        watching</a>    </div>

    <h3 class="sr-only">Forks</h3>
    <div class="mt-2">
      <a href="/HCPLab-SYSU/Embodied_AI_Paper_List/forks" data-view-component="true" class="Link Link--muted"><svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo-forked mr-2">
    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
</svg>
        <strong>128</strong>
        forks</a>    </div>


    <div class="mt-2">
      <a class="Link--muted" href="/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FHCPLab-SYSU%2FEmbodied_AI_Paper_List&amp;report=HCPLab-SYSU+%28user%29">
          Report repository
</a>    </div>
</div>

          </div>
        </div>

        
            <div class="BorderGrid-row">
              <div class="BorderGrid-cell">
                <h2 class="h4 mb-3" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame">
  <a href="/HCPLab-SYSU/Embodied_AI_Paper_List/releases" data-view-component="true" class="Link--primary no-underline Link">Releases</a></h2>

    <div class="text-small color-fg-muted">No releases published</div>

              </div>
            </div>

        
        
            <div class="BorderGrid-row">
              <div class="BorderGrid-cell">
                
  <h2 class="h4 mb-3">
  <a href="/users/HCPLab-SYSU/packages?repo_name=Embodied_AI_Paper_List" data-view-component="true" class="Link--primary no-underline Link d-flex flex-items-center">Packages
      <span title="0" hidden="hidden" data-view-component="true" class="Counter ml-1">0</span></a></h2>


      <div class="text-small color-fg-muted" >
        No packages published <br>
      </div>



              </div>
            </div>

        
        
            <div class="BorderGrid-row">
              <div class="BorderGrid-cell">
                <h2 class="h4 mb-3">
  <a href="/HCPLab-SYSU/Embodied_AI_Paper_List/graphs/contributors" data-view-component="true" class="Link--primary no-underline Link d-flex flex-items-center">Contributors
      <span title="15" data-view-component="true" class="Counter ml-1">15</span></a></h2>


    
  <ul class="list-style-none d-flex flex-wrap mb-n2">
    <li class="mb-2 mr-2"
        >
      <a href="https://github.com/HCPLab-SYSU"
          class=""
            data-hovercard-type="user" data-hovercard-url="/users/HCPLab-SYSU/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self"
          
        >
        <img src="https://avatars.githubusercontent.com/u/38847349?s=64&amp;v=4" alt="@HCPLab-SYSU" size="32" height="32" width="32" data-view-component="true" class="avatar circle" />
      </a>
    </li>
    <li class="mb-2 mr-2"
        >
      <a href="https://github.com/Linda-JYLin"
          class=""
            data-hovercard-type="user" data-hovercard-url="/users/Linda-JYLin/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self"
          
        >
        <img src="https://avatars.githubusercontent.com/u/66871642?s=64&amp;v=4" alt="@Linda-JYLin" size="32" height="32" width="32" data-view-component="true" class="avatar circle" />
      </a>
    </li>
    <li class="mb-2 mr-2"
        >
      <a href="https://github.com/WissingChen"
          class=""
            data-hovercard-type="user" data-hovercard-url="/users/WissingChen/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self"
          
        >
        <img src="https://avatars.githubusercontent.com/u/31480397?s=64&amp;v=4" alt="@WissingChen" size="32" height="32" width="32" data-view-component="true" class="avatar circle" />
      </a>
    </li>
    <li class="mb-2 mr-2"
        >
      <a href="https://github.com/YangLiu9208"
          class=""
            data-hovercard-type="user" data-hovercard-url="/users/YangLiu9208/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self"
          
        >
        <img src="https://avatars.githubusercontent.com/u/26245388?s=64&amp;v=4" alt="@YangLiu9208" size="32" height="32" width="32" data-view-component="true" class="avatar circle" />
      </a>
    </li>
    <li class="mb-2 mr-2"
        >
      <a href="https://github.com/zhaowei-wang-nlp"
          class=""
            data-hovercard-type="user" data-hovercard-url="/users/zhaowei-wang-nlp/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self"
          
        >
        <img src="https://avatars.githubusercontent.com/u/22047467?s=64&amp;v=4" alt="@zhaowei-wang-nlp" size="32" height="32" width="32" data-view-component="true" class="avatar circle" />
      </a>
    </li>
    <li class="mb-2 mr-2"
        >
      <a href="https://github.com/sxshco"
          class=""
            data-hovercard-type="user" data-hovercard-url="/users/sxshco/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self"
          
        >
        <img src="https://avatars.githubusercontent.com/u/90235729?s=64&amp;v=4" alt="@sxshco" size="32" height="32" width="32" data-view-component="true" class="avatar circle" />
      </a>
    </li>
    <li class="mb-2 mr-2"
        >
      <a href="https://github.com/thkkk"
          class=""
            data-hovercard-type="user" data-hovercard-url="/users/thkkk/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self"
          
        >
        <img src="https://avatars.githubusercontent.com/u/30761156?s=64&amp;v=4" alt="@thkkk" size="32" height="32" width="32" data-view-component="true" class="avatar circle" />
      </a>
    </li>
    <li class="mb-2 mr-2"
        >
      <a href="https://github.com/zwq2018"
          class=""
            data-hovercard-type="user" data-hovercard-url="/users/zwq2018/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self"
          
        >
        <img src="https://avatars.githubusercontent.com/u/44236100?s=64&amp;v=4" alt="@zwq2018" size="32" height="32" width="32" data-view-component="true" class="avatar circle" />
      </a>
    </li>
    <li class="mb-2 mr-2"
        >
      <a href="https://github.com/jonyzhang2023"
          class=""
            data-hovercard-type="user" data-hovercard-url="/users/jonyzhang2023/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self"
          
        >
        <img src="https://avatars.githubusercontent.com/u/50441823?s=64&amp;v=4" alt="@jonyzhang2023" size="32" height="32" width="32" data-view-component="true" class="avatar circle" />
      </a>
    </li>
    <li class="mb-2 mr-2"
        >
      <a href="https://github.com/yingchengyang"
          class=""
            data-hovercard-type="user" data-hovercard-url="/users/yingchengyang/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self"
          
        >
        <img src="https://avatars.githubusercontent.com/u/52985597?s=64&amp;v=4" alt="@yingchengyang" size="32" height="32" width="32" data-view-component="true" class="avatar circle" />
      </a>
    </li>
    <li class="mb-2 mr-2"
        >
      <a href="https://github.com/Kenn3o3"
          class=""
            data-hovercard-type="user" data-hovercard-url="/users/Kenn3o3/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self"
          
        >
        <img src="https://avatars.githubusercontent.com/u/84194648?s=64&amp;v=4" alt="@Kenn3o3" size="32" height="32" width="32" data-view-component="true" class="avatar circle" />
      </a>
    </li>
    <li class="mb-2 mr-2"
        >
      <a href="https://github.com/Anjingkun"
          class=""
            data-hovercard-type="user" data-hovercard-url="/users/Anjingkun/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self"
          
        >
        <img src="https://avatars.githubusercontent.com/u/90504584?s=64&amp;v=4" alt="@Anjingkun" size="32" height="32" width="32" data-view-component="true" class="avatar circle" />
      </a>
    </li>
    <li class="mb-2 mr-2"
        >
      <a href="https://github.com/RealSuSeven"
          class=""
            data-hovercard-type="user" data-hovercard-url="/users/RealSuSeven/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self"
          
        >
        <img src="https://avatars.githubusercontent.com/u/122971162?s=64&amp;v=4" alt="@RealSuSeven" size="32" height="32" width="32" data-view-component="true" class="avatar circle" />
      </a>
    </li>
    <li class="mb-2 mr-2"
        >
      <a href="https://github.com/limanwang"
          class=""
            data-hovercard-type="user" data-hovercard-url="/users/limanwang/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self"
          
        >
        <img src="https://avatars.githubusercontent.com/u/173160746?s=64&amp;v=4" alt="@limanwang" size="32" height="32" width="32" data-view-component="true" class="avatar circle" />
      </a>
    </li>
    <li class="mb-2 mr-2"
        >
      <a href="https://github.com/x-humanoid-robomind"
          class=""
            data-hovercard-type="user" data-hovercard-url="/users/x-humanoid-robomind/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self"
          
        >
        <img src="https://avatars.githubusercontent.com/u/192090949?s=64&amp;v=4" alt="@x-humanoid-robomind" size="32" height="32" width="32" data-view-component="true" class="avatar circle" />
      </a>
    </li>
</ul>





              </div>
            </div>

        
        
              </div>
</div>
  
</div></div>

  </div>


  </div>

</turbo-frame>


    </main>
  </div>

  </div>

          <footer class="footer pt-8 pb-6 f6 color-fg-muted p-responsive" role="contentinfo" >
  <h2 class='sr-only'>Footer</h2>

  


  <div class="d-flex flex-justify-center flex-items-center flex-column-reverse flex-lg-row flex-wrap flex-lg-nowrap">
    <div class="d-flex flex-items-center flex-shrink-0 mx-2">
      <a aria-label="GitHub Homepage" class="footer-octicon mr-2" href="https://github.com">
        <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-mark-github">
    <path d="M12 1C5.923 1 1 5.923 1 12c0 4.867 3.149 8.979 7.521 10.436.55.096.756-.233.756-.522 0-.262-.013-1.128-.013-2.049-2.764.509-3.479-.674-3.699-1.292-.124-.317-.66-1.293-1.127-1.554-.385-.207-.936-.715-.014-.729.866-.014 1.485.797 1.691 1.128.99 1.663 2.571 1.196 3.204.907.096-.715.385-1.196.701-1.471-2.448-.275-5.005-1.224-5.005-5.432 0-1.196.426-2.186 1.128-2.956-.111-.275-.496-1.402.11-2.915 0 0 .921-.288 3.024 1.128a10.193 10.193 0 0 1 2.75-.371c.936 0 1.871.123 2.75.371 2.104-1.43 3.025-1.128 3.025-1.128.605 1.513.221 2.64.111 2.915.701.77 1.127 1.747 1.127 2.956 0 4.222-2.571 5.157-5.019 5.432.399.344.743 1.004.743 2.035 0 1.471-.014 2.654-.014 3.025 0 .289.206.632.756.522C19.851 20.979 23 16.854 23 12c0-6.077-4.922-11-11-11Z"></path>
</svg>
</a>
      <span>
        &copy; 2025 GitHub,&nbsp;Inc.
      </span>
    </div>

    <nav aria-label="Footer">
      <h3 class="sr-only" id="sr-footer-heading">Footer navigation</h3>

      <ul class="list-style-none d-flex flex-justify-center flex-wrap mb-2 mb-lg-0" aria-labelledby="sr-footer-heading">

          <li class="mx-2">
            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to Terms&quot;,&quot;label&quot;:&quot;text:terms&quot;}" href="https://docs.github.com/site-policy/github-terms/github-terms-of-service" data-view-component="true" class="Link--secondary Link">Terms</a>
          </li>

          <li class="mx-2">
            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to privacy&quot;,&quot;label&quot;:&quot;text:privacy&quot;}" href="https://docs.github.com/site-policy/privacy-policies/github-privacy-statement" data-view-component="true" class="Link--secondary Link">Privacy</a>
          </li>

          <li class="mx-2">
            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to security&quot;,&quot;label&quot;:&quot;text:security&quot;}" href="https://github.com/security" data-view-component="true" class="Link--secondary Link">Security</a>
          </li>

          <li class="mx-2">
            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to status&quot;,&quot;label&quot;:&quot;text:status&quot;}" href="https://www.githubstatus.com/" data-view-component="true" class="Link--secondary Link">Status</a>
          </li>

          <li class="mx-2">
            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to community&quot;,&quot;label&quot;:&quot;text:community&quot;}" href="https://github.community/" data-view-component="true" class="Link--secondary Link">Community</a>
          </li>

          <li class="mx-2">
            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to docs&quot;,&quot;label&quot;:&quot;text:docs&quot;}" href="https://docs.github.com/" data-view-component="true" class="Link--secondary Link">Docs</a>
          </li>

          <li class="mx-2">
            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to contact&quot;,&quot;label&quot;:&quot;text:contact&quot;}" href="https://support.github.com?tags=dotcom-footer" data-view-component="true" class="Link--secondary Link">Contact</a>
          </li>

          <li class="mx-2" >
  <cookie-consent-link>
    <button
      type="button"
      class="Link--secondary underline-on-hover border-0 p-0 color-bg-transparent"
      data-action="click:cookie-consent-link#showConsentManagement"
      data-analytics-event="{&quot;location&quot;:&quot;footer&quot;,&quot;action&quot;:&quot;cookies&quot;,&quot;context&quot;:&quot;subfooter&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;cookies_link_subfooter_footer&quot;}"
    >
       Manage cookies
    </button>
  </cookie-consent-link>
</li>

<li class="mx-2">
  <cookie-consent-link>
    <button
      type="button"
      class="Link--secondary underline-on-hover border-0 p-0 color-bg-transparent text-left"
      data-action="click:cookie-consent-link#showConsentManagement"
      data-analytics-event="{&quot;location&quot;:&quot;footer&quot;,&quot;action&quot;:&quot;dont_share_info&quot;,&quot;context&quot;:&quot;subfooter&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;dont_share_info_link_subfooter_footer&quot;}"
    >
      Do not share my personal information
    </button>
  </cookie-consent-link>
</li>

      </ul>
    </nav>
  </div>
</footer>



    <ghcc-consent id="ghcc" class="position-fixed bottom-0 left-0" style="z-index: 999999"
      data-locale="en"
      data-initial-cookie-consent-allowed=""
      data-cookie-consent-required="false"
    ></ghcc-consent>




  <div id="ajax-error-message" class="ajax-error-message flash flash-error" hidden>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-alert">
    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
    <button type="button" class="flash-close js-ajax-error-dismiss" aria-label="Dismiss error">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
    </button>
    You cant perform that action at this time.
  </div>

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open>
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog>
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    <div class="Popover js-hovercard-content position-absolute" style="display: none; outline: none;">
  <div class="Popover-message Popover-message--bottom-left Popover-message--large Box color-shadow-large" style="width:360px;">
  </div>
</div>

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div>
    <div id="js-global-screen-reader-notice" class="sr-only mt-n1" aria-live="polite" aria-atomic="true" ></div>
    <div id="js-global-screen-reader-notice-assertive" class="sr-only mt-n1" aria-live="assertive" aria-atomic="true"></div>
  </body>
</html>


