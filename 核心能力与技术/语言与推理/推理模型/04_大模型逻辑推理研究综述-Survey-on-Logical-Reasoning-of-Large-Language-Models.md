# 大模型逻辑推理研究综述 (Survey on Logical Reasoning of Large Language Models)

**来源**: [https://aclanthology.org/2024.ccl-2.3/](https://aclanthology.org/2024.ccl-2.3/)

---

# 大模型逻辑推理研究综述(Survey on Logical Reasoning of Large Pre-trained Language Models) - ACL Anthology

原文链接: https://aclanthology.org/2024.ccl-2.3/

## [大模型逻辑推理研究综述(Survey on Logical Reasoning of Large Pre-trained Language Models)](https://aclanthology.org/2024.ccl-2.3.pdf)

[Hanmeng Liu (刘汉蒙)](/people/hanmeng-liu/),
[Yue Zhang (张岳)](/people/yue-zhang/)

##### Correct Metadata for

×

**Important**: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See [our corrections guidelines](https://aclanthology.org/info/corrections/) if you need to change the PDF.

Title
Adjust the title. Retain tags such as <fixed-case>.

Authors
Adjust author names and order to match the PDF.Add Author

Abstract
Correct abstract if needed. Retain XML formatting tags such as <tex-math>.

Verification against PDF
Ensure that the new title/authors match the snapshot below. (If there is no snapshot or it is too small, consult [the PDF](#).)

[![]()](#)

Authors concatenated from the text boxes above:

ALL author names match the snapshot above—including middle initials, hyphens, and accents.

Submit

---

##### Abstract

“理解自然语言的逻辑结构和关系是机器理解的核心任务,也是人工智能领域的关键研究议题。随着大数据和计算能力的提升,预训练语言模型在逻辑推理方面取得了显著进展,使得大规模模型的逻辑推理能力成为研究的新焦点。本综述旨在全面梳理大模型在逻辑推理领域的研究进展,探讨其对人工智能系统智能水平评估的重要性及其在推动人工智能发展中的作用。 本文首先界定了大模型逻辑推理能力的研究范畴,系统性地讨论了逻辑推理的类型和 特点,并回顾了相关理论的发展,为研究提供了清晰的框架。接着,从任务形式和数 据基准的角度,详细介绍了逻辑推理研究的基础工作,为理解大模型的性能提供了基 准。进一步,本文深入分析了大模型在逻辑推理能力上的现状,通过不同推理类型的 案例研究,展示了大模型的能力表现。同时,本文还探讨了提升大模型逻辑推理能力 的方法,包括预训练、指令微调、解码策略和神经符号混合方法,并对这些方法进行 了比较分析。最后,本文提出了对未来研究方向的展望,旨在激发更多的学术讨论和 探索,推动逻辑推理能力研究的进一步发展。”

Anthology ID:
:   2024.ccl-2.3

Volume:
:   [Proceedings of the 23rd Chinese National Conference on Computational Linguistics (Volume 2: Frontier Forum)](/volumes/2024.ccl-2/)

Month:
:   July

Year:
:   2024

Address:
:   Taiyuan, China

Editor:
:   [Zhao Xin](/people/zhao-xin/)

Venue:
:   [CCL](/venues/ccl/)

SIG:


Publisher:
:   Chinese Information Processing Society of China

Note:


Pages:
:   48–62

Language:
:   Chinese

URL:
:   <https://aclanthology.org/2024.ccl-2.3/>

DOI:


Bibkey:
:   hanmeng-yue-2024-da

Cite (ACL):
:   Hanmeng Liu and Yue Zhang. 2024. [大模型逻辑推理研究综述(Survey on Logical Reasoning of Large Pre-trained Language Models)](https://aclanthology.org/2024.ccl-2.3/). In *Proceedings of the 23rd Chinese National Conference on Computational Linguistics (Volume 2: Frontier Forum)*, pages 48–62, Taiyuan, China. Chinese Information Processing Society of China.

Cite (Informal):
:   [大模型逻辑推理研究综述(Survey on Logical Reasoning of Large Pre-trained Language Models)](https://aclanthology.org/2024.ccl-2.3/) (Liu & Zhang, CCL 2024)

Copy Citation:
:   BibTeX
    Markdown
    MODS XML
    Endnote
    More options…

PDF:
:   <https://aclanthology.org/2024.ccl-2.3.pdf>

[PDF](https://aclanthology.org/2024.ccl-2.3.pdf "Open PDF of '大模型逻辑推理研究综述(Survey on Logical Reasoning of Large Pre-trained Language Models)'")[Cite](# "Open dialog for exporting citations")[Search](https://www.semanticscholar.org/search?q=%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%80%BB%E8%BE%91%E6%8E%A8%E7%90%86%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0%28Survey+on+Logical+Reasoning+of+Large+Pre-trained+Language+Models%29 "Search for '大模型逻辑推理研究综述(Survey on Logical Reasoning of Large Pre-trained Language Models)' on Semantic Scholar")[Fix data](# "Correct problems with title, author list, and abstract")

---

##### Export citation

×

* [BibTeX](#citeBibtex)
* [MODS XML](#citeMods)
* [Endnote](#citeEndnote)
* [Preformatted](#citeMarkdown)

```
@inproceedings{hanmeng-yue-2024-da,
    title = "大模型逻辑推理研究综述(Survey on Logical Reasoning of Large Pre-trained Language Models)",
    author = "Liu, Hanmeng  and
      Zhang, Yue",
    editor = "Xin, Zhao",
    booktitle = "Proceedings of the 23rd Chinese National Conference on Computational Linguistics (Volume 2: Frontier Forum)",
    month = jul,
    year = "2024",
    address = "Taiyuan, China",
    publisher = "Chinese Information Processing Society of China",
    url = "https://aclanthology.org/2024.ccl-2.3/",
    pages = "48--62",
    language = "zho",
    abstract = "``理解自然语言的逻辑结构和关系是机器理解的核心任务,也是人工智能领域的关键研究议题。随着大数据和计算能力的提升,预训练语言模型在逻辑推理方面取得了显著进展,使得大规模模型的逻辑推理能力成为研究的新焦点。本综述旨在全面梳理大模型在逻辑推理领域的研究进展,探讨其对人工智能系统智能水平评估的重要性及其在推动人工智能发展中的作用。 本文首先界定了大模型逻辑推理能力的研究范畴,系统性地讨论了逻辑推理的类型和 特点,并回顾了相关理论的发展,为研究提供了清晰的框架。接着,从任务形式和数 据基准的角度,详细介绍了逻辑推理研究的基础工作,为理解大模型的性能提供了基 准。进一步,本文深入分析了大模型在逻辑推理能力上的现状,通过不同推理类型的 案例研究,展示了大模型的能力表现。同时,本文还探讨了提升大模型逻辑推理能力 的方法,包括预训练、指令微调、解码策略和神经符号混合方法,并对这些方法进行 了比较分析。最后,本文提出了对未来研究方向的展望,旨在激发更多的学术讨论和 探索,推动逻辑推理能力研究的进一步发展。''"
}
```

Download as File
Copy to Clipboard

```
<?xml version="1.0" encoding="UTF-8"?>
<modsCollection xmlns="http://www.loc.gov/mods/v3">
<mods ID="hanmeng-yue-2024-da">
    <titleInfo>
        <title>大模型逻辑推理研究综述(Survey on Logical Reasoning of Large Pre-trained Language Models)</title>
    </titleInfo>
    <name type="personal">
        <namePart type="given">Hanmeng</namePart>
        <namePart type="family">Liu</namePart>
        <role>
            <roleTerm authority="marcrelator" type="text">author</roleTerm>
        </role>
    </name>
    <name type="personal">
        <namePart type="given">Yue</namePart>
        <namePart type="family">Zhang</namePart>
        <role>
            <roleTerm authority="marcrelator" type="text">author</roleTerm>
        </role>
    </name>
    <originInfo>
        <dateIssued>2024-07</dateIssued>
    </originInfo>
    <typeOfResource>text</typeOfResource>
    <language>
        <languageTerm type="text">zho</languageTerm>
    </language>
    <relatedItem type="host">
        <titleInfo>
            <title>Proceedings of the 23rd Chinese National Conference on Computational Linguistics (Volume 2: Frontier Forum)</title>
        </titleInfo>
        <name type="personal">
            <namePart type="given">Zhao</namePart>
            <namePart type="family">Xin</namePart>
            <role>
                <roleTerm authority="marcrelator" type="text">editor</roleTerm>
            </role>
        </name>
        <originInfo>
            <publisher>Chinese Information Processing Society of China</publisher>
            <place>
                <placeTerm type="text">Taiyuan, China</placeTerm>
            </place>
        </originInfo>
        <genre authority="marcgt">conference publication</genre>
    </relatedItem>
    <abstract>“理解自然语言的逻辑结构和关系是机器理解的核心任务,也是人工智能领域的关键研究议题。随着大数据和计算能力的提升,预训练语言模型在逻辑推理方面取得了显著进展,使得大规模模型的逻辑推理能力成为研究的新焦点。本综述旨在全面梳理大模型在逻辑推理领域的研究进展,探讨其对人工智能系统智能水平评估的重要性及其在推动人工智能发展中的作用。 本文首先界定了大模型逻辑推理能力的研究范畴,系统性地讨论了逻辑推理的类型和 特点,并回顾了相关理论的发展,为研究提供了清晰的框架。接着,从任务形式和数 据基准的角度,详细介绍了逻辑推理研究的基础工作,为理解大模型的性能提供了基 准。进一步,本文深入分析了大模型在逻辑推理能力上的现状,通过不同推理类型的 案例研究,展示了大模型的能力表现。同时,本文还探讨了提升大模型逻辑推理能力 的方法,包括预训练、指令微调、解码策略和神经符号混合方法,并对这些方法进行 了比较分析。最后,本文提出了对未来研究方向的展望,旨在激发更多的学术讨论和 探索,推动逻辑推理能力研究的进一步发展。”</abstract>
    <identifier type="citekey">hanmeng-yue-2024-da</identifier>
    <location>
        <url>https://aclanthology.org/2024.ccl-2.3/</url>
    </location>
    <part>
        <date>2024-07</date>
        <extent unit="page">
            <start>48</start>
            <end>62</end>
        </extent>
    </part>
</mods>
</modsCollection>
```

Download as File
Copy to Clipboard

```
%0 Conference Proceedings
%T 大模型逻辑推理研究综述(Survey on Logical Reasoning of Large Pre-trained Language Models)
%A Liu, Hanmeng
%A Zhang, Yue
%Y Xin, Zhao
%S Proceedings of the 23rd Chinese National Conference on Computational Linguistics (Volume 2: Frontier Forum)
%D 2024
%8 July
%I Chinese Information Processing Society of China
%C Taiyuan, China
%G zho
%F hanmeng-yue-2024-da
%X “理解自然语言的逻辑结构和关系是机器理解的核心任务,也是人工智能领域的关键研究议题。随着大数据和计算能力的提升,预训练语言模型在逻辑推理方面取得了显著进展,使得大规模模型的逻辑推理能力成为研究的新焦点。本综述旨在全面梳理大模型在逻辑推理领域的研究进展,探讨其对人工智能系统智能水平评估的重要性及其在推动人工智能发展中的作用。 本文首先界定了大模型逻辑推理能力的研究范畴,系统性地讨论了逻辑推理的类型和 特点,并回顾了相关理论的发展,为研究提供了清晰的框架。接着,从任务形式和数 据基准的角度,详细介绍了逻辑推理研究的基础工作,为理解大模型的性能提供了基 准。进一步,本文深入分析了大模型在逻辑推理能力上的现状,通过不同推理类型的 案例研究,展示了大模型的能力表现。同时,本文还探讨了提升大模型逻辑推理能力 的方法,包括预训练、指令微调、解码策略和神经符号混合方法,并对这些方法进行 了比较分析。最后,本文提出了对未来研究方向的展望,旨在激发更多的学术讨论和 探索,推动逻辑推理能力研究的进一步发展。”
%U https://aclanthology.org/2024.ccl-2.3/
%P 48-62
```

Download as File
Copy to Clipboard

##### Markdown (Informal)

[大模型逻辑推理研究综述(Survey on Logical Reasoning of Large Pre-trained Language Models)](https://aclanthology.org/2024.ccl-2.3/) (Liu & Zhang, CCL 2024)

* [大模型逻辑推理研究综述(Survey on Logical Reasoning of Large Pre-trained Language Models)](https://aclanthology.org/2024.ccl-2.3/) (Liu & Zhang, CCL 2024)

##### ACL

* Hanmeng Liu and Yue Zhang. 2024. [大模型逻辑推理研究综述(Survey on Logical Reasoning of Large Pre-trained Language Models)](https://aclanthology.org/2024.ccl-2.3/). In *Proceedings of the 23rd Chinese National Conference on Computational Linguistics (Volume 2: Frontier Forum)*, pages 48–62, Taiyuan, China. Chinese Information Processing Society of China.

Copy Markdown to Clipboard
Copy ACL to Clipboard

---

*爬取时间: 2025-11-28 21:50:59*
