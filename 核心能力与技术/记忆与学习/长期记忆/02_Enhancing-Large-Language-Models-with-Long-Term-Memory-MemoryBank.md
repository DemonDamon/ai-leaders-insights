# Enhancing Large Language Models with Long-Term Memory (MemoryBank)

**来源**: [https://ojs.aaai.org/index.php/AAAI/article/view/29946](https://ojs.aaai.org/index.php/AAAI/article/view/29946)

---

# 
		MemoryBank: Enhancing Large Language Models with Long-Term Memory
							| Proceedings of the AAAI Conference on Artificial Intelligence
			

原文链接: https://ojs.aaai.org/index.php/AAAI/article/view/29946

# MemoryBank: Enhancing Large Language Models with Long-Term Memory

## Authors

* Wanjun Zhong

  Sun Yat-sen University
* Lianghong Guo

  Sun Yat-sen University
* Qiqi Gao

  Harbin Institute of Technology
* He Ye

  KTH Royal Institute of Technology
* Yanlin Wang

  Sun Yat-sen University

## DOI:

<https://doi.org/10.1609/aaai.v38i17.29946>

## Keywords:

NLP: (Large) Language Models, HAI: Emotional Intelligence, NLP: Applications, NLP: Conversational AI/Dialog Systems, NLP: Generation

## Abstract

Large Language Models (LLMs) have drastically reshaped our interactions with artificial intelligence (AI) systems, showcasing impressive performance across an extensive array of tasks. Despite this, a notable hindrance remains—the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems, psychological counseling, and secretarial assistance. Recognizing the necessity for long-term memory, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user's personality over time by synthesizing information from previous interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory. This mechanism permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a more human-like memory mechanism and enriched user experience. MemoryBank is versatile in accommodating both closed-source models like ChatGPT and open-source models such as ChatGLM. To validate MemoryBank's effectiveness, we exemplify its application through the creation of an LLM-based chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned with psychological dialog data, SiliconFriend displays heightened empathy and discernment in its interactions. Experiment involves both qualitative analysis with real-world user dialogs and quantitative analysis with simulated dialogs. In the latter, ChatGPT acts as multiple users with diverse characteristics and generates long-term dialog contexts covering a wide array of topics. The results of our analysis reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong capability for long-term companionship as it can provide emphatic response, recall relevant memories and understand user personality.

[![AAAI-24 / IAAI-24 / EAAI-24 Proceedings Cover](https://ojs.aaai.org/public/journals/2/AAAI24Proceedings-Cover.jpg)](https://ojs.aaai.org/index.php/AAAI/issue/view/592)

## Downloads

* [PDF](https://ojs.aaai.org/index.php/AAAI/article/view/29946/31654)

## Published

2024-03-24

## How to Cite

Zhong, W., Guo, L., Gao, Q., Ye, H., & Wang, Y. (2024). MemoryBank: Enhancing Large Language Models with Long-Term Memory. *Proceedings of the AAAI Conference on Artificial Intelligence*, *38*(17), 19724-19731. https://doi.org/10.1609/aaai.v38i17.29946

More Citation Formats

* [ACM](https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/acm-sig-proceedings?submissionId=29946&publicationId=28230)
* [ACS](https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/acs-nano?submissionId=29946&publicationId=28230)
* [APA](https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/apa?submissionId=29946&publicationId=28230)
* [ABNT](https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/associacao-brasileira-de-normas-tecnicas?submissionId=29946&publicationId=28230)
* [Chicago](https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/chicago-author-date?submissionId=29946&publicationId=28230)
* [Harvard](https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/harvard-cite-them-right?submissionId=29946&publicationId=28230)
* [IEEE](https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/ieee?submissionId=29946&publicationId=28230)
* [MLA](https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/modern-language-association?submissionId=29946&publicationId=28230)
* [Turabian](https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/turabian-fullnote-bibliography?submissionId=29946&publicationId=28230)
* [Vancouver](https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/vancouver?submissionId=29946&publicationId=28230)

Download Citation

* [Endnote/Zotero/Mendeley (RIS)](https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/download/ris?submissionId=29946&publicationId=28230)
* [BibTeX](https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/download/bibtex?submissionId=29946&publicationId=28230)

## Issue

[Vol. 38 No. 17: AAAI-24 Technical Tracks 17](https://ojs.aaai.org/index.php/AAAI/issue/view/592)

## Section

AAAI Technical Track on Natural Language Processing II

---

*爬取时间: 2025-11-28 21:51:18*
