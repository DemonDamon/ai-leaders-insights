# What is AI safety? What do we want it to be?

**来源**: [https://link.springer.com/article/10.1007/s11098-025-02367-z](https://link.springer.com/article/10.1007/s11098-025-02367-z)

---

# What is AI safety? What do we want it to be? | Philosophical Studies

原文链接: https://link.springer.com/article/10.1007/s11098-025-02367-z

# What is AI safety? What do we want it to be?

* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)
* Published: 24 June 2025

* Volume 182, pages 1495–1518, (2025)
* [Cite this article](#citeas)

You have full access to this [open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research) article

[Download PDF](/content/pdf/10.1007/s11098-025-02367-z.pdf)

[![](ai-leaders-insights/技术路径与方法论/深度研究流派/安全优先路径/images/0b67cd50ad05640bf5905141862a190c.jpg)
Philosophical Studies](/journal/11098)
[Aims and scope](/journal/11098/aims-and-scope)
[Submit manuscript](https://www.editorialmanager.com/phil)

What is AI safety? What do we want it to be?

[Download PDF](/content/pdf/10.1007/s11098-025-02367-z.pdf)

* [Jacqueline Harding](#auth-Jacqueline-Harding-Aff1)[1](#Aff1) &
* [Cameron Domenico Kirk-Giannini](#auth-Cameron_Domenico-Kirk_Giannini-Aff2) 
  [ORCID: orcid.org/0000-0001-9372-227X](https://orcid.org/0000-0001-9372-227X)[2](#Aff2)

* 2449 Accesses
* 1 Citation
* [Explore all metrics](/article/10.1007/s11098-025-02367-z/metrics)

## Abstract

The field of AI safety seeks to prevent or reduce the harms caused by AI systems. A simple and appealing account of what is distinctive of AI safety as a field holds that this feature is constitutive: a research project falls within the purview of AI safety just in case it aims to prevent or reduce the harms caused by AI systems. Call this appealingly simple account *The Safety Conception of AI safety*. Despite its simplicity and appeal, we argue that The Safety Conception is in tension with at least two trends in the ways AI safety researchers and organizations think and talk about AI safety: first, a tendency to characterize the goal of AI safety research in terms of *catastrophic* risks from *future* systems; second, the increasingly popular idea that AI safety can be thought of as a branch of *safety engineering*. Adopting the methodology of conceptual engineering, we argue that these trends are unfortunate: when we consider what concept of AI safety it would be best to have, there are compelling reasons to think that The Safety Conception is the answer. Descriptively, The Safety Conception allows us to see how work on topics that have historically been treated as central to the field of AI safety is continuous with work on topics that have historically been treated as more marginal, like bias, misinformation, and privacy. Normatively, taking The Safety Conception seriously means approaching all efforts to prevent or mitigate harms from AI systems based on their merits rather than drawing arbitrary distinctions between them.

### Explore related subjects

Discover the latest articles, books and news in related subjects, suggested using machine learning.

* [Chemical Safety](/subjects/chemical-safety)
* [Engineering Ethics](/subjects/engineering-ethics)
* [Machine Learning](/subjects/machine-learning)
* [Philosophy of Artificial Intelligence](/subjects/philosophy-of-artificial-intelligence)
* [Symbolic AI](/subjects/symbolic-ai)
* [Artificial Intelligence](/subjects/artificial-intelligence)

[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=11098)

Avoid common mistakes on your manuscript.

## 1 Introduction

As the development and deployment of artificial intelligence (AI) proceed at breakneck speed, increasing attention is being paid to *AI safety*, an academic discipline which incorporates speculative theoretical work on the alignment of advanced artificial systems with human interests,[Footnote 1](#Fn1) technical research on topics like adversarial robustness,[Footnote 2](#Fn2) anomaly detection,[Footnote 3](#Fn3) model interpretation,[Footnote 4](#Fn4) the evaluation of dangerous capabilities,[Footnote 5](#Fn5) and (increasingly) proposals concerning the governance of AI.[Footnote 6](#Fn6) Despite its prominence in public discussions of AI, its leaders’ growing influence with policymakers, and its rise as a funding area for grantmaking bodies, however, no consensus has emerged in practice about what should mark a research topic as falling within the purview of AI safety.[Footnote 7](#Fn7) In this paper, we aim to fill this lacuna.

It’s important to ward off a potential confusion at this stage. Although we’re arguing about what sort of research ought to be labeled as ‘AI safety’, we’re not interested in terminology for terminology’s sake. As is well recognized by sociologists, how we draw the boundaries of an academic discipline matters in many ways.[Footnote 8](#Fn8) For example, disciplinary boundaries affect every aspect of research: they shape what researchers are expected to read and engage with, who collaborates with whom, how norms for the evaluation of research are set and which research trends are taken seriously. Moreover, they affect the interaction between a discipline and society more broadly: which researchers are viewed as experts in the discipline in question, are included in public and political conversations about the societal implications of research in the discipline, and so on. Both categories of effects will be more pronounced when disciplinary boundaries are more reified (in journals and conferences, funding bodies, academic departments, etc.).[Footnote 9](#Fn9) So the boundaries of the discipline of AI safety matter: they bear on how research priorities within AI safety will be set, and on what sorts of concerns and perspectives will be included in institutional discussions of AI safety.

In what follows, our focus will be on one especially simple idea about what AI safety is. According to this simple idea, which we call *The Safety Conception of AI safety* (or *The Safety Conception* for short), AI safety is the field of inquiry which seeks to prevent or mitigate harms from AI systems. More precisely:

### The Safety Conception of AI safety

A research project belongs to the field of AI safety just in case it is aimed at preventing or mitigating harms from AI systems’ development and deployment.

At first glance, The Safety Conception may seem too obvious or general to be controversial. Indeed, many AI safety researchers would probably express agreement with The Safety Conception if presented with it. But we provide evidence below that members of the AI safety community often speak and act in ways that are in tension with The Safety Conception. In other words, while The Safety Conception may capture something close to the *manifest* concept of AI safety among AI safety researchers, it does not capture the concept of AI safety that is *operative* among AI safety researchers or AI safety governance organizations.[Footnote 10](#Fn10) For example, research on a range of topics of interest to scholars in the Fairness, Accountability, Transparency, and Ethics (*FAccT* or *FATE*) community, including algorithmic bias,[Footnote 11](#Fn11) misinformation,[Footnote 12](#Fn12) breaches of privacy,[Footnote 13](#Fn13) data theft,[Footnote 14](#Fn14) distortions of the democratic process,[Footnote 15](#Fn15) and private concentration of power and resources,[Footnote 16](#Fn16) clearly belongs to the field of AI safety according to The Safety Conception. But research on these topics is often treated as though it is not AI safety research or is only marginally relevant to AI safety.

This situation is unfortunate, since we believe that taking The Safety Conception more seriously would have significant benefits. Indeed, we argue below that when we ask what operative concept of AI safety it would be *best* to have, there are compelling reasons to think that The Safety Conception is the answer. In particular, The Safety Conception has both explanatory and normative advantages. Explanatorily, existing work in AI safety can be usefully viewed through the lens of harm prevention or mitigation, and The Safety Conception allows us to see how work on topics that have historically been treated as central to the field of AI safety is continuous with work on topics that have historically been treated as more marginal. Normatively, taking The Safety Conception seriously means approaching all efforts to prevent or mitigate harms from AI systems based on their merits rather than drawing arbitrary distinctions between them. We argue that applying this approach to research prioritization and AI governance is likely to result in more effective harm reduction than alternatives.

Our thesis that The Safety Conception is the best conception of AI safety has two main consequences:

> 1. (1)
>
>    According to the best conception of AI safety, AI safety research includes work on social harms from AI (such as bias and representational harms, privacy and surveillance, and economic harms) as well as work on ‘catastrophic’ harms (such as enabling large-scale terrorism, state warfare, or harms from autonomous, agentic AI).
> 2. (2)
>
>    Given (1), there should be greater disciplinary integration between researchers working on social harms and those working on catastrophic harms. In particular, political conversations about AI safety should include voices from researchers in both categories.

Our argument in what follows is structured around our two primary goals: first, demonstrating that The Safety Conception is not always the conception of AI safety operative among AI safety researchers; second, using the methodology of conceptual engineering to argue that it ought to be. In Sect. [2](/article/10.1007/s11098-025-02367-z#Sec2), we introduce AI safety as a research area in more detail and provide evidence that The Safety Conception is accepted by a range of researchers at least as their manifest conception of AI safety. In Sect. [3](/article/10.1007/s11098-025-02367-z#Sec7), we describe some ways in which the speech and actions of AI safety researchers often come into conflict with The Safety Conception in practice. In particular, we focus on the common ideas that AI safety is especially or exclusively concerned with *catastrophic* risks from *future* AI systems, and that it should be construed as a branch of *safety engineering*. In Sect. [4](/article/10.1007/s11098-025-02367-z#Sec10), we turn to the second of our two goals, introducing our conceptual engineering methodology and identifying two purposes which a characterization of AI safety should serve. In Sect. [5](/article/10.1007/s11098-025-02367-z#Sec11), we apply our methodology to show that The Safety Conception best serves our two purposes for a concept of AI safety. Section [6](/article/10.1007/s11098-025-02367-z#Sec12) concludes.

## 2 The Safety Conception

Something like The Safety Conception is ubiquitous in discussions by AI safety researchers. Amodei et al. ([2016](/article/10.1007/s11098-025-02367-z#ref-CR4 "Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & Mané, D. (2016). Concrete problems in AI safety. ArXiv preprint. 
                  https://arxiv.org/abs/1606.06565
                  
                ")), in an influential work which served to set AI safety’s early technical agenda, distinguish the field of AI safety by its focus on reducing “unintended and harmful behavior that may emerge from poor design of real-world AI systems” (p. 1). The Center for Security and Emerging Technology (CSET) defines AI safety as “an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably”.[Footnote 17](#Fn17) The recently formed US AI Safety Institute says that it “exists to help advance the understanding and mitigation of risks of advanced AI”.[Footnote 18](#Fn18) Even the Wikipedia entry on AI safety appears to endorse The Safety Conception when it defines AI safety as “an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from AI systems”.[Footnote 19](#Fn19)

If we endorse The Safety Conception, what does the research landscape of AI safety look like? We can group research aimed at preventing or mitigating harms from AI systems into three broad categories: (i) non-empirical research on harms from AI systems, (ii) empirical research on understanding current AI systems, (iii) empirical research on preventing/mitigating harm from current AI systems.

### 2.1 Non-empirical research on harms from AI systems

There is a large body of research on the harms of AI systems that is non-empirical. Much of this work can be thought of as aiming to identify potential harms from AI systems that can then be assessed and tackled in real systems.

Falling into this category are various taxonomies of harms from AI systems, especially systems using generative models as components.[Footnote 20](#Fn20) For example, Weidinger et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR103 "Weidinger, L., Rauh, M., Marchal, N., Manzini, A., Hendricks, L. A., Mateos-Garcia, Bergman, S.… and, & Isaac, W. (2023). Sociotechnical Safety Evaluation of Generative AI Systems. ArXiv Preprint. 
                  https://arxiv.org/abs/2310.11986
                  
                ")) identify several broad categories of harm: representational harms (including internal biases in models as well as social-group-level disparities in AI system behavior), toxicity harms (including the production of hateful content), misinformation harms (when AI systems generate misleading information or facilitate its dissemination), privacy harms (when private or sensitive information in AI systems’ training data can be extracted by users), autonomy harms (when users become overly reliant on models), economic harms (when automation exacerbates existing economic inequalities), and environmental harms (such as the large energy cost of training and deploying generative models).

As models become more powerful, researchers have argued that they will create novel sorts of risk. Hendrycks et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR55 "Hendrycks, D., Mazeika, M., & Woodside, T. (2023). An Overview of Catastrophic AI Risks. ArXiv preprint. 
                  https://arxiv.org/abs/2306.12001
                  
                ")) identify various potential ‘catastrophic’ risks (risks of large-scale harm) from advanced AI systems, including enabling bioterrorism, cyberterrorism, state misinformation and surveillance, and autonomous warfare. An influential body of work also makes the case that advanced AI systems will pose risks in their own right, rather than merely enabling or exacerbating harms from other humans (e.g. Bostrom ([2014](/article/10.1007/s11098-025-02367-z#ref-CR10 "Bostrom, N. (2014). Superintelligence: Paths, Dangers, Strategies. Oxford University Press.")), Carlsmith ([2021](/article/10.1007/s11098-025-02367-z#ref-CR19 "Carlsmith, J. (2021). Is power-seeking AI an existential risk? ArXiv preprint. 
                  https://arxiv.org/abs/2206.13353
                  
                ")), Goldstein and Kirk-Giannini ([2025](/article/10.1007/s11098-025-02367-z#ref-CR41 "Goldstein, S., & Kirk-Giannini, C. D. (2025). Language Agents Reduce the Risk of Existential Catastrophe. AI & Society 40: 959–969. ")), Bales et al. ([2024](/article/10.1007/s11098-025-02367-z#ref-CR7 "Bales, A., D’Alessandro, W., & Kirk-Giannini, C. D. (2024). Artificial intelligence: Arguments for catastrophic risk. Philosophy Compass, 19(2), e12964."))). Arguments for this conclusion proceed in various ways; typically, they involve the claim that as AI systems become more capable they will develop greater degrees of agency, construed roughly as the ability to take sequences of actions towards longer-horizon goals without human supervision. As AI systems become more agentic, the argument goes, it will become likely that their goals are in tension with human flourishing (since, e.g., they will ‘instrumentally converge’ to goals involving the accumulation of resources (Bostrom, [2014](/article/10.1007/s11098-025-02367-z#ref-CR10 "Bostrom, N. (2014). Superintelligence: Paths, Dangers, Strategies. Oxford University Press.")); this premise is concretized by reference to examples of reward misspecification and goal misgeneralization in reinforcement learning (e.g. Clark and Amodei ([2016](/article/10.1007/s11098-025-02367-z#ref-CR24 "Clark, J., & Amodei, D. (2016). Faulty reward function in the wild. OpenAI blog post. 
                  https://openai.com/research/faulty-reward-functions
                  
                ")), Langosco et al. ([2022](/article/10.1007/s11098-025-02367-z#ref-CR69 "Langosco, L., Koch, J., Sharkey, L., Pfau, J., & Krueger, D. (2022). Goal misgeneralization in deep reinforcement learning. Proceedings of the 39th International Conference on Machine Learning (ICML ‘22): 12004–12019."))).

Finally, note that much philosophical work on normative issues in AI’s development and deployment can also be understood as aimed at harm reduction. This includes not only work on topics like algorithmic discrimination and fairness, data theft, and privacy but also — given the arguments above — work evaluating attributions of agency to AI systems. It also includes research modeling different trajectories for AI’s development, involving predicting ways future systems could develop, as well as proposals for the governance of future AI systems.

### 2.2 Research on understanding current AI systems

A prerequisite to preventing harms from AI systems’ deployment is understanding how the systems behave in a variety of deployment settings. The nascent science of *model evaluation* aims to measure AI systems’ capabilities (crudely, their performance ceiling on more general cognitive tasks; see Harding and Sharadin (forthcoming)) as well as the ways these capabilities are mediated by different factors during deployment, such as changes in the distribution of inputs. Model evaluation often uses model-agnostic benchmarks, allowing standardized comparisons between models.

Safety evaluation involves assessing the degree to which a system is capable of producing harmful outputs; for an LLM, for example, this might involve testing the model’s ability to produce hate speech or instructions to synthesize dangerous chemical compounds. Some safety evaluation involves constructing novel benchmarks for dangerous capabilities, whereas some involves more bespoke evaluation (Shevlane et al., [2023](/article/10.1007/s11098-025-02367-z#ref-CR93 "Shevlane, T., Farquhar, S., Garfinkel, B., Phuong, M., Whittlestone, J., Leung, J., Kokotajlo, D., Marchal, N., Anderljung, M., Kolt, N., & Ho, L. (2023). Model evaluation for extreme risks. ArXiv preprint. 
                  https://arxiv.org/abs/2305.15324
                  
                ")). Safety evaluation might also involve assessing for harms which have occurred during the training of particular models, such as environmental harms or harms to workers employed in the data-processing pipeline. As systems improve, researchers have argued that safety evaluation will involve testing for different sorts of cognitive capabilities, such as those associated with systems’ degree of agency (including, for example, the degree to which models are ‘self-aware’ or pursue goals across a range of scenarios). There are some early examples of benchmarks for these capabilities.[Footnote 21](#Fn21)

From a harm reduction perspective, one important aspect of model evaluation is understanding the degree to which models’ behavior can be predicted in different deployment settings. This is often put in terms of models’ *robustness* to changes in (e.g.) the sorts of inputs they process, especially in adversarial settings in which an ‘attacker’ can perturb some aspect of the evaluation. Evaluating a system’s degree of adversarial robustness involves specifying a ‘threat model’ for an attacker (Carlini et al., [2019](/article/10.1007/s11098-025-02367-z#ref-CR18 "Carlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber, J., Tsipras, D., Goodfellow, I., Madry, A., & Kurakin, A. (2019). On evaluating adversarial robustness. ArXiv preprint. 
                  https://arxiv.org/abs/1902.06705
                  
                ")), which spells out her goals, capabilities, and knowledge. *Red-teaming* is a specific sort of adversarial robustness evaluation in which the attackers’ threat model is chosen to closely mirror the environment in which the model will be deployed (that is, the attacker plays the role of an ill-intentioned user).[Footnote 22](#Fn22)

Safety evaluation usually involves attempting to draw conclusions about models from observing their behavior in different settings. However, many researchers have argued that behavioral evidence will be insufficient to provide the kind of safety guarantees needed, especially as models exhibit higher degrees of agency (for example, many have worried about deception, or notions of algorithmic discrimination which are sensitive to *how* the algorithm works, and not merely to its input-output behavior).[Footnote 23](#Fn23) Research on *model interpretability* aims to solve this problem by providing an understanding of how AI systems process inputs to produce outputs, via observation of models’ intermediate computations. Older work on explainable AI tended to deliver relatively coarse-grained understanding of models, such as visualization of which parts of the input had the largest effect on the model’s output. By contrast, *mechanistic interpretability*, an increasingly influential subfield of machine learning, aims at giving a more complete causal story about models’ internal workings. Harding ([2023](/article/10.1007/s11098-025-02367-z#ref-CR44 "Harding, J. (2023). Operationalising Representation in Natural Language Processing. British Journal for the Philosophy of Science. 
                  https://arxiv.org/abs/2306.08193
                  
                ")) argues that mechanistic interpretability delivers representational explanations of model behavior, where the computation the model performs is expressed in terms of manipulation of representations of abstract features of the input. From a harm reduction perspective, the idea is that knowing the ‘high-level algorithm’ the model performs makes predicting (and steering) its behavior easier.

### 2.3 Research on preventing/mitigating harm from current AI systems

Most of this research targets the model development process. This includes research which investigates models’ training data to uncover training examples which perpetuate hateful content or contain dangerous dual-use information. Training corpora can then be filtered to remove these examples, and models which have been trained using these examples can be flagged for potential safety concerns. It also includes research on novel architectures which satisfy various safety-relevant properties, such as transparency, as well as research on fine-tuning models on a next-token-prediction task using supervised examples (SFT) or reinforcement learning based on human or synthetic preference data over model outputs (RLHF/AIF), or related non-RL tuning techniques, such as direct preference optimization (DPO).[Footnote 24](#Fn24)

Assuming models’ capabilities continue to improve, it will be impractical (even, depending on the task, impossible) for humans to supervise them directly. Indeed, the usefulness of RLHF derives from the fact that — for complex natural language tasks — it is already easier for humans to judge whether an output is desired than it is for them to produce an example of a desired output. The research field of *scalable oversight* aims to develop techniques for supervising models whose capabilities or knowledge exceeds those of the supervisor; these are cases in which the supervisor can offer only “weak supervision”.[Footnote 25](#Fn25) Various techniques have been proposed; one of the best known is debate, in which — for a given input (such as a question) — the supervisor observes an interaction (a *debate*) between the AI system and some other system (in practice, the other system is often a copy of the original system), in which the AI system repeatedly attempts to defend its output (such as the answer to the input question) against challenges from the other system.[Footnote 26](#Fn26) The idea is that even if the supervisor is unable to assess the quality of the AI system’s initial output, she will be able to judge the ‘winner’ of the debate (since she will have observed whether the output was able to be defended successfully); she will be able to use this judgment to supervise the system.

Just as supervising models will become more difficult as models develop, so too will assessing when supervision has been successful in removing unwanted behaviors. This makes it challenging to evaluate different proposals for scalable oversight. Bowman et al. ([2022](/article/10.1007/s11098-025-02367-z#ref-CR13 "Bowman, S., Hyun, J., Perez, E., Chen, E., Pettit, C., Heiner, S., & Kaplan, J. (2022). Measuring progress on scalable oversight for large language models. ArXiv preprint. 
                  https://arxiv.org/abs/2211.03540
                  
                ")) propose testing scalable oversight proposals using Cotra’s ([2021](/article/10.1007/s11098-025-02367-z#ref-CR27 "Cotra, A. (2021). The Case for Aligning Narrowly Superhuman Models. Alignment Forum Blog Post. 
                  https://www.alignmentforum.org/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models
                  
                ")) ‘sandwiching’ paradigm. The idea is to test supervision techniques in domains in which a model’s capabilities lie in between (are ‘sandwiched’ between) those of a supervisor and an expert; the expert assesses the degree to which the supervisor has been able to produce the desired behavior from the model. The hope is that supervision techniques which are successful in these domains will also be successful in domains in which no expert exists.

There is also research aimed at mitigating harms during model deployment. One sort of idea involves applying *anomaly detection* techniques to model inputs and outputs during deployment, to screen out undesirable outputs as well as inputs likely to elicit them, such as adversarial attacks (in LLMs, this manifests as content filters on inputs and outputs).[Footnote 27](#Fn27) Another involves inference-time interventions on models to steer their outputs (in LLMs, this manifests as system prompts, or interventions on the model’s intermediate activations to make model responses less harmful, guided by interpretability work).[Footnote 28](#Fn28) A third example involves “watermarking” model outputs to mitigate downstream harms such as plagiarism and disinformation.

Finally, an important category of research aimed at harm reduction is explicitly *sociotechnical*, in that it focuses on the integration of AI systems within larger social systems. This includes, for example, proposals for auditing AI systems throughout their development and deployment.[Footnote 29](#Fn29) It also includes many proposals for the governance of current AI systems.[Footnote 30](#Fn30)

### 2.4 Discussion

There are two points to make about The Safety Conception at this stage.

First, note that The Safety Conception does not distinguish between different types of harms from AI systems when assessing whether research counts as AI safety research; in particular, social harms (such as representational harms) are treated the same way as ‘catastrophic’ harms, such as large-scale cybersecurity breaches. Work aimed at reducing ‘existential’ risks from AI systems’ development and deployment counts as AI safety research, but so too does much research involving issues related to fairness, transparency, and accountability.

Second, one might worry that The Safety Conception is so liberal as to be trivial. It is important to observe, then, that plenty of research on AI-related normative issues does not count as AI safety research under The Safety Conception. This is because there are many normative dimensions to AI’s development and deployment that have little to do with harm reduction.

For example, much normative work in AI concerns questions about *power* and *legitimacy* (Lazar, [2022](/article/10.1007/s11098-025-02367-z#ref-CR70 "Lazar, S. (2022). Power and AI: Nature and Justification. In The Oxford Handbook of AI Governance, edited by Justin B. Bullock, Yu-Che Chen, Johannes Himmelreich, Valerie M. Hudson, Anton Korinek, Matthew M. Young, and Baobao Zhang. Oxford University Press.")); these questions are not naturally posed in the language of harm reduction. Similarly, to the extent we regard *transparency* as an intrinsic value of systems which make normatively significant decisions, work aimed at explaining AI systems’ behavior may be valuable even if it does not aim at reducing harm.[Footnote 31](#Fn31) When the field of algorithmic recourse uses counterfactual explanation as a step in the process of delivering understanding (and, possibly, a means of challenge) to users who have been negatively affected by algorithmic decisions, then, it does not necessarily aim at giving full explanations of model behavior in the sense that could be used to improve model behavior (i.e. it does not aim at delivering interventions to reduce future harm, unlike most work in mechanistic interpretability).[Footnote 32](#Fn32)

Although, as noted above, proposals for accountability might play a role in AI governance proposals aimed at harm reduction, they are primarily aimed at a different normative goal, that of *justice* for those harmed by AI systems. Indeed, current debates over whether ‘fairness’ in AI can be measured (e.g. Green and Hu ([2018](/article/10.1007/s11098-025-02367-z#ref-CR43 "Green, B., Lily, & Hu (2018). The Myth in the Methodology: Towards Recontextualization of Fairness in Machine Learning. Presented at the 35th International Conference on Machine Learning (ICML ’18)."))) bear upon the question of whether work on algorithmic fairness could ever be seen through the lens of harm reduction.

Furthermore, as an anonymous reviewer observes, much normative work on AI relates to building systems which promote human flourishing (broadly construed). For example, researchers might be interested in how technological progress can enable novel forms of political representation. This more ‘optimistic’ normative work falls outside of the remit of AI safety.

Finally, as the discussion above illustrates, some work on normative questions in AI serves multiple purposes; The Safety Conception allows for the possibility that work can fall within the remit of multiple disciplines.

## 3 Tensions with the Safety Conception

So far, we have introduced The Safety Conception and provided evidence that it is taken seriously by at least some AI safety researchers, at least when it comes to their manifest concept of AI safety as a field of study. In this section, we argue that even if The Safety Conception is sometimes explicitly endorsed by AI safety researchers, in practice AI safety as a research area often functions as though The Safety Conception is false. In particular, we discuss (i) the tendency of some AI safety researchers and organizations to explicitly or implicitly restrict the domain of AI safety research so that it exclusively or primarily concerns *catastrophic* risk from *future* systems, and (ii) the common idea that AI safety should be understood as a branch of *safety engineering*, which imposes methodological constraints on the kinds of research aimed at mitigating harms from AI systems that can legitimately claim to belong to the field of AI safety.

### 3.1 Catastrophic/existential harms

The field of AI safety is often explicitly or implicitly defined so that it exclusively or primarily concerns catastrophic harms from future AI systems.[Footnote 33](#Fn33) We will refer to this way of defining AI safety as *The Catastrophic Conception*. The Catastrophic Conception has been around as long as the phrase “AI safety” itself,[Footnote 34](#Fn34) and it persists today. Indeed, a recent study of the ‘epistemic community’ of AI safety characterizes the discipline as follows: “generally, AI safety practitioners are interested in preventing *catastrophic long-term events* precipitated by the deployment of machine learning systems” (Ahmed et al., [2024](/article/10.1007/s11098-025-02367-z#ref-CR1 "Ahmed, S., Jaźwińska, K., Ahlawat, A., Winecoff, A., & Wang, M. (2024). April. ‘Field-Building and the Epistemic Culture of AI Safety’. First Monday, 14."), emphasis added). Dalrymple et al. ([2024](/article/10.1007/s11098-025-02367-z#ref-CR32 "Dalrymple, D., Skalse, J., Bengio, Y., Russell, S., Tegmark, M., Seshia, S., Omohundro, S. (2024). Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems. ArXiv preprint. 
                  https://doi.org/10.48550/arXiv.2405.06624
                  
                ")), whose authors include several prominent AI researchers, describe “The AI Safety Problem” as the claim that “sufficiently advanced AI systems may threaten the survival of the human species, or lead to our permanent disempowerment, especially in the case of AI systems that are more intelligent than humans.” (p. 2).

Definitions of AI safety that prioritize catastrophic/existential risks appear even in sources that may initially appear to endorse The Safety Conception. For example, the Wikipedia article on AI safety, cited above for describing AI safety as “an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from AI systems,” goes on to say that “the field is particularly concerned with existential risks posed by advanced AI models,” with an in-text hyperlink to the article “Existential risk from AI.”[Footnote 35](#Fn35) Similarly, in an influential recent paper on AI safety, researchers from UC Berkeley, Google, and OpenAI write, “We define ML Safety research as ML research aimed at making the adoption of ML more beneficial, *with emphasis on long-term and long-tail risks*” (Hendrycks et al., [2021](/article/10.1007/s11098-025-02367-z#ref-CR56 "Hendrycks, D., Mazeika, M., Zou, A., Patel, S., Zhu, C., Navarro, J., Song, D., Li, B., & Steinhardt, J. (2021). What would Jiminy Cricket do? Toward agents that behave morally. 35th Conference on Neural Information Processing Systems (NeurIPS 2021)."), emphasis added).[Footnote 36](#Fn36)

Similar characterizations of AI safety are offered in the self-descriptions or mission statements of various organizations active in the area. The AI Alignment Forum, a website used by AI safety researchers to discuss technical and theoretical developments in the field, describes the reason for its creation as: “Foremost, because misaligned powerful AIs may pose the greatest risk to our civilization that has ever arisen.”[Footnote 37](#Fn37) The same website highlights a series of posts intended as an introduction to the alignment problem written by Richard Ngo, an AI safety researcher who has worked at OpenAI and DeepMind. Ngo begins:

> “The key concern motivating technical AGI safety research is that we might build autonomous artificially intelligent agents which are much more intelligent than humans, and which pursue goals that conflict with our own… AIs will eventually become more capable than us at the types of tasks by which we maintain and exert that control. If they don’t want to obey us, then humanity might become only Earth’s second most powerful “species”, and lose the ability to create a valuable and worthwhile future.” (Ngo [2020](/article/10.1007/s11098-025-02367-z#ref-CR75 "Ngo, R. (2020). AGI safety from first principles: Introduction. AI Alignment Forum post. 
>                   https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ/p/8xRSjC76HasLnMGSf
>                   
>                 ")).

We read Ngo’s use of the phrase ‘AGI safety’ here as a way of signaling that he embraces The Catastrophic Conception and rejects The Safety Conception. That is, we take it that he is *not* intending to commit to the existence of two disciplines, AI safety and AGI safety. Instead, he is using ‘AGI safety’ to pick out the same discipline others refer to as ‘AI safety’ or ‘ML safety’, and claiming that it is motivated by a concern with future catastrophic risks. This way of using terminology is also adopted by Bowman ([2022](/article/10.1007/s11098-025-02367-z#ref-CR12 "Bowman, S. (2022). ‘AI Safety and Neighboring Communities: A Quick-Start Guide, as of Summer 2022’, 1 September 2022. 
                  https://www.lesswrong.com/posts/EFpQcBmfm2bFfM4zM/ai-safety-and-neighboring-communities-a-quick-start-guide-as.
                  
                ")), in a document providing a high-level overview of the discipline of AI safety, who uses ‘AI safety’ and ‘AGI safety’ interchangeably.[Footnote 38](#Fn38)

In implicitly identifying the field of AI safety with the more specific concerns of those interested in AGI rather than AI in general, and in motivating the need for AI safety research by appealing to the possibility of long-term catastrophic outcomes like civilizational collapse and human disempowerment, these researchers and organizations de-center a range of research topics which clearly fall within the field of AI safety on The Safety Conception, including bias, toxicity, misinformation, privacy harms, economic harms, and environmental harms, as well as technical research on how to reduce the short-term harms of near-future AI systems.

### 3.2 Safety engineering

A second and quite different way in which the practice of AI safety researchers comes into conflict with The Safety Conception is that AI safety is sometimes understood to be a largely technical branch of *safety engineering*. Safety engineering is an interdisciplinary field which aims to reduce harm from the development and deployment of engineered systems (Roland & Moriarty ([1990](/article/10.1007/s11098-025-02367-z#ref-CR86 "Roland, H., & Moriarty, B. (1990). System Safety Engineering and Management. John Wiley & Sons, Ltd."))). Like many engineering fields, safety engineering is best characterized by its objectives and methodologies rather than by the questions it seeks to answer. Broadly speaking, the practice of safety engineering takes as its object of study engineered systems and the environments in which they are developed and deployed, which themselves can be thought of as sociotechnical systems. It takes as its goal the minimization of harm from the development and deployment of these systems and involves activities like hazard identification (identifying cases in which a system’s deployment leads to unwanted outcomes), risk analysis (identifying how likely each identified hazard is to occur), and risk management (providing suggestions for reducing the aggregate risk from a system’s deployment).

Several authors have suggested that “AI safety” should be viewed as continuous with the larger field of safety engineering.[Footnote 39](#Fn39) Different authors make different claims here; some authors merely claim that lessons from safety engineering should be applied to the deployment and development of AI. We are interested in a stronger version of the claim, which takes continuity with safety engineering as constitutive of AI safety as a discipline. This view is suggested, for example, by Weidinger et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR103 "Weidinger, L., Rauh, M., Marchal, N., Manzini, A., Hendricks, L. A., Mateos-Garcia, Bergman, S.… and, & Isaac, W. (2023). Sociotechnical Safety Evaluation of Generative AI Systems. ArXiv Preprint. 
                  https://arxiv.org/abs/2310.11986
                  
                ")) when they advocate a sociotechnical approach to AI safety ”inspired by a system safety approach from the discipline of safety engineering” (p. 8) and then remark:

> current and future… classes of generative AI systems have been claimed to possess novel capabilities that may create ‘extreme’ risks to society, such as from disseminating dangerous information or creating novel types of cyber attacks… Historically, these focus areas — or ethical and safety risks — associated with AI systems have been fragmented and have constituted distinct research communities based on perceived epistemic differences and differences in timely proximity of harms… However, recent advances in generative AI systems are forcing a collapse of these epistemological silos… The sociotechnical approach put forward here accommodates risks that are of concern to both research communities and it can thus serve to coordinate work between these communities on risks from generative AI systems.

For Weidinger et al., the AI ethics and AI safety research communities are united by their subsumption within the larger discipline of (sociotechnical) safety engineering. Let us call this *The Engineering Conception* of AI safety.

What does The Engineering Conception of AI safety amount to? That is, what does it mean to claim that AI safety is continuous with safety engineering? It can’t just be that research in AI safety, like research in safety engineering more broadly, aims at harm prevention and mitigation. If this were all The Engineering Conception consisted in, it would collapse into The Safety Conception — the connection to safety engineering would be entirely epiphenomenal. To have content, the claim must be that (a) there is some set of methodologies which is distinctive of the discipline of safety engineering, and (b) work falls within AI safety to the extent it can be understood as applying these methodologies to the domain of AI.

And indeed, many technical fields in contemporary AI safety can be seen as examples of safety engineering. For example, methods relating to distribution-shift and adversarial robustness, as well as anomaly and trojan detection and calibration, can be understood as efforts to estimate and improve the reliability of different components of AI systems.[Footnote 40](#Fn40) Similarly, work on reward misspecification and goal misgeneralization in reinforcement learning (e.g. Clark and Amodei ([2016](/article/10.1007/s11098-025-02367-z#ref-CR24 "Clark, J., & Amodei, D. (2016). Faulty reward function in the wild. OpenAI blog post. 
                  https://openai.com/research/faulty-reward-functions
                  
                ")), Langosco et al. ([2022](/article/10.1007/s11098-025-02367-z#ref-CR69 "Langosco, L., Koch, J., Sharkey, L., Pfau, J., & Krueger, D. (2022). Goal misgeneralization in deep reinforcement learning. Proceedings of the 39th International Conference on Machine Learning (ICML ‘22): 12004–12019."))) can be understood as hazard identification and analysis.

Yet conceiving of AI safety as a branch of safety engineering is in tension with The Safety Conception. There are many research projects that aim to prevent or mitigate harms from AI systems which do not recognizably employ the methodology of engineering. These include, for example, theoretical work involving premises like the orthogonality thesis and the idea of instrumental convergence (e.g. Bostrom ([2014](/article/10.1007/s11098-025-02367-z#ref-CR10 "Bostrom, N. (2014). Superintelligence: Paths, Dangers, Strategies. Oxford University Press."))), attempts to quantify the risks posed by these kinds of issues (e.g. Carlsmith ([2021](/article/10.1007/s11098-025-02367-z#ref-CR19 "Carlsmith, J. (2021). Is power-seeking AI an existential risk? ArXiv preprint. 
                  https://arxiv.org/abs/2206.13353
                  
                ")), Goldstein and Kirk-Giannini ([2025](/article/10.1007/s11098-025-02367-z#ref-CR41 "Goldstein, S., & Kirk-Giannini, C. D. (2025). Language Agents Reduce the Risk of Existential Catastrophe. AI & Society 40: 959–969. "))), conceptual work on the alignment problem (Gabriel, [2020](/article/10.1007/s11098-025-02367-z#ref-CR38 "Gabriel, I. (2020). Artificial intelligence, values, and alignment. Minds and Machines, 30(3), 411–437.")) and related issues in normative ethics and decision theory (e.g. D’Alessandro ([2024](/article/10.1007/s11098-025-02367-z#ref-CR31 "D’Alessandro, W. (2024). Deontology and safe artificial intelligence. Philosophical Studies. Online First.")), Tubert and Tiehen ([2024](/article/10.1007/s11098-025-02367-z#ref-CR100 "Tubert, A., & Tiehen, J. (2024). Existentialist risk and value misalignment. Philosophical Studies. Online First.")), Thornley ([2024](/article/10.1007/s11098-025-02367-z#ref-CR96 "Thornley, E. (2024). The shutdown problem: an AI engineering puzzle for decision theorists. Philosophical Studies. Online First."))), and governance proposals made by AI safety researchers, such as licensing regimes for models based on their training compute budgets (Shavit, [2023](/article/10.1007/s11098-025-02367-z#ref-CR91 "Shavit, Y. (2023). What Does It Take to Catch a Chinchilla? Verifying Rules on Large-Scale Neural Network Training via Compute Monitoring. ArXiv preprint. 
                  https://arxiv.org/abs/2303.11341
                  
                ")). The Engineering Conception of AI safety excludes these projects, whereas The Safety Conception does not.

## 4 Conceptual engineering

We turn now from showing that The Safety Conception is non-trivial in the sense that taking it seriously would require reconsidering certain trends in the way AI safety researchers and organizations think and talk about AI safety to arguing that The Safety Conception is the conception of AI safety we *ought* to adopt, and in particular to arguing for claims (1) and (2), reproduced below:

> 1. (1)
>
>    According to the best conception of AI safety, AI safety research includes work on social harms from AI (such as bias and representational harms, privacy and surveillance, and economic harms) as well as work on ‘catastrophic’ harms (such as enabling large-scale terrorism, state warfare, or harms from autonomous, agentic AI).
> 2. (2)
>
>    Given (1), there should be greater disciplinary integration between researchers working on social harms and those working on catastrophic harms. In particular, political conversations about AI safety should include voices from researchers in both categories.

Our methodology in arguing for these claims is one of conceptual engineering: the project, as Cappelen puts it, of “assessing and improving our representational devices” (2018, p. 3). We introduce our conceptual engineering project in more detail in this section before defending our claim that The Safety Conception is the best conception of AI safety in Sect. [5](/article/10.1007/s11098-025-02367-z#Sec11) below.

In evaluating various candidate concepts of AI safety, we will be interested both in the extent to which each is explanatorily useful and the extent to which it helps to promote the practical goal of reducing harms from AI systems. Assessing concepts along the first of these dimensions is standard practice in conceptual engineering; it is, for example, the approach taken by Clark and Chalmers ([1998](/article/10.1007/s11098-025-02367-z#ref-CR23 "Clark, A., & Chalmers, D. (1998). The extended mind. Analysis, 58, 7–19.")) in arguing for the thesis that mental states and cognitive processes can extend beyond the boundaries of the brain, which Cappelen ([2018](/article/10.1007/s11098-025-02367-z#ref-CR17 "Cappelen, H. (2018). Fixing Language: An Essay on Conceptual Engineering. Oxford University Press.")) cites as an early paradigm of conceptual engineering.[Footnote 41](#Fn41)

Assessing concepts according to the extent to which they promote practical goals is also a form of conceptual engineering, but one which has usually been discussed using the term *ameliorative inquiry* (Haslanger, [2005](/article/10.1007/s11098-025-02367-z#ref-CR49 "Haslanger, S. (2005). What Are We Talking About? The Semantics and Politics of Social Kinds. Hypatia 20(4):10–26. Reprinted in Haslanger (2012), pp. 365–380.")).[Footnote 42](#Fn42) Ameliorative inquiry is a common methodology in the feminist tradition, which seeks to construct emancipatory accounts of social categories like gender and sexual orientation. As Dembroff ([2016](/article/10.1007/s11098-025-02367-z#ref-CR33 "Dembroff, R. (2016). What is sexual orientation? Philosophers’ Imprint, 16, 1–27."), 4) clarifies, the methodology of ameliorative inquiry has two important components: “Elucidating purposes ideally served by our [target] concept,” and “Re-engineering our [target] concept… in light of [these] purposes.” Our discussion in what follows will be structured around these two components.

What, then, are the purposes ideally served by our concept of AI safety? We will focus on two:

1. (A)

   It should provide a unifying explanation of what makes it the case that paradigmatic research programs in AI safety belong to the same field of inquiry.
2. (B)

   It should be conducive to reducing the harms caused by AI systems.

The first of these purposes speaks to the explanatory utility of a concept of AI safety. A concept that strays too far from the conventional understanding of what is included in the field risks simply changing the subject. At the same time, we must be open to the possibility that the best account of what is distinctive about AI safety as a field of inquiry will issue some surprising verdicts: some research questions which have not traditionally been regarded as AI safety research questions might turn out to fall within the purview of AI safety, and some research questions which have traditionally been regarded as AI safety research questions might turn out not to be.

The second purpose speaks to the practical implications of a concept of AI safety. In proposing an ameliorative concept of AI safety, we do not claim, implausibly, that anyone is directly helped or harmed by any concept, or that the harms caused by AI systems are directly mediated by the representational devices we use to individuate fields of inquiry. Instead, our claim is that the way in which we think about fields of inquiry has indirect effects on the amount of harm caused by AI systems, mediated by decisions about how to prioritize research projects across areas of inquiry and choices about which experts’ input is given consideration in crafting policy. If there are areas of inquiry which would, if adequately prioritized and given a voice in policy discussions, reduce the harms caused by AI systems, and if these areas of inquiry are deprioritized and ignored in AI safety policy discussions because of the concept of AI safety currently operative among AI researchers, we regard this as a practical reason to engage in ameliorative revision of the concept.

## 5 Conceptual engineering supports the Safety Conception

Why think that The Safety Conception is the best way of demarcating AI safety as a research area? In this section, we argue that The Safety Conception accomplishes our purposes (A) and (B) better than alternative proposals. With respect to each purpose, our argument will have the following form: First, we will argue that The Safety Conception does a good job of serving that purpose. Second, we will compare The Safety Conception with The Catastrophic Conception and The Engineering Conception, arguing that The Safety Conception fares better than these alternatives. Third, we will offer some considerations which lead us to generalize to the conclusion that *any* departure from The Safety Conception will yield an understanding of AI safety that fares worse with respect to that purpose.

Consider first purpose (A): A satisfactory concept of AI safety should provide a unifying explanation of what makes it the case that paradigmatic research programs in AI safety belong to the same field of inquiry. Providing a unifying explanation of this kind is not a trivial achievement, since it is not immediately apparent why (for example) philosophical work on whether agents in general might have instrumental reasons to seek power should fall within the same field of inquiry as technical machine learning work on anomaly detection. The Safety Conception explains what ties these kinds of research questions together into a single area of inquiry: they aim at preventing or mitigating harms from AI systems. Indeed, in our view The Safety Conception embodies the ideal level of generality for thinking about AI safety. It provides a unifying explanation of what makes it the case that paradigmatic research programs in AI safety belong to the same field of inquiry while also highlighting the continuity between those research programs and topics like algorithmic bias, misinformation, and distortions of the democratic process, which have historically be treated as marginal AI safety research topics if they have been understood to fall within the domain of AI safety research at all.

So The Safety Conception does a good job of serving purpose (A). But our claim is stronger — that The Safety Conception does better than alternatives at serving purpose (A). Consider, then, how The Safety Conception compares in this context to The Catastrophic Conception. In our view, the latter conception of AI safety fares poorly with respect to purpose (A). To see this, note that, like work in the wider contemporary machine learning landscape, most technical work in AI safety is empirical, based on experiments on existing models. Central questions in AI safety — for example, questions about adversarial robustness, model fine-tuning using human preference data, and interpretability — concern present systems just as much as future ones and have no deep conceptual connection to issues specifically of catastrophic or existential risk — they appear largely agnostic to the kinds of risks to which they are applied. So a concept of AI safety which tied it constitutively to catastrophic future risks would arbitrarily exclude a great deal of paradigmatic AI safety work focused on mitigating near-term non-catastrophic harms.

This exclusion would be especially unfortunate given the significant continuities between non-catastrophic and catastrophic risks from AI. For example, language models produce hate speech for the same reason they produce instructions on making bombs (properties of their pre-training corpora), and we miss something important from the perspective of explanation when we ignore this. Indeed, many of the concrete catastrophic risks identified by AI safety researchers (e.g. by Hendrycks et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR55 "Hendrycks, D., Mazeika, M., & Woodside, T. (2023). An Overview of Catastrophic AI Risks. ArXiv preprint. 
                  https://arxiv.org/abs/2306.12001
                  
                "))), such as AI enabling permanent political disempowerment by undermining democratic processes, are simply ‘scaled up’ versions of risks already well-studied by researchers focusing on social harms from AI.[Footnote 43](#Fn43)

Similar remarks apply to The Engineering Conception. Many central research projects in AI safety, such as the project of assessing whether intelligent artificial agents are likely to have instrumental reasons to act in ways that harm humans, are not best approached using the tools or methods of engineering. Conceiving of AI safety as a kind of engineering would arbitrarily exclude research on these topics.

These remarks about the Catastrophic Conception and the Engineering Conception lead us to believe that no conception of AI safety more restrictive than The Safety Conception could fare better than The Safety Conception with respect to purpose (A) — narrowing the purview of AI safety risks losing out on explanatorily important continuities between different research programs. At the same time, we also believe that any *less* restrictive conception would fare poorly with respect to purpose (A) as compared to The Safety Conception. Such a conception would, by stipulation, include in the domain of AI safety some research projects not aimed at preventing or mitigating harms from AI systems. And it is difficult to see how the resulting collection of research projects could be explanatorily unified. It follows that The Safety Conception describes the best concept of AI safety when it comes to purpose (A).

Consider now purpose (B): A satisfactory concept of AI safety should be conducive to reducing the harms caused by AI systems. Our argument that The Safety Conception fares well with respect to purpose (B) is structured around a comparison between three possible ways of structuring the AI safety research community:

> In the first, which we might call *The Safe Scenario*, the AI safety research community is structured in accordance with The Safety Conception. Research projects are prioritized based on their expected contribution to the goal of preventing or mitigating harms from AI systems, and experts are consulted during political deliberation about AI safety to the extent that they are knowledgeable about how to prevent or mitigate harms from AI systems.
>
> In the second, which we might call *The Catastrophic Scenario*, the AI safety research community is structured in accordance with The Catastrophic Conception. Research projects are prioritized based on their expected contribution to the goal of preventing or mitigating catastrophic harms from future AI systems, and experts are consulted during political deliberation about AI safety to the extent that they are knowledgeable about how to prevent or mitigate catastrophic harms from future AI systems.
>
> In the third, which we might call *The Engineering Scenario*, the AI safety research community is structured in accordance with The Engineering Conception. Research projects are prioritized based on their expected contribution to the goal of preventing or mitigating harms from AI systems, subject to the constraint that they employ methods recognizable as engineering, and experts are consulted during political deliberation about AI safety to the extent that they are engineers knowledgeable about how to prevent or mitigate harms from AI systems.

To begin, note that in The Safe Scenario, the AI safety community is likely to be quite effective at reducing the harms caused by AI systems. This is because it prioritizes research projects solely based on their expected contribution to the goal of preventing or mitigating harms from AI systems and consults experts solely to the extent that they are knowledgeable about how to prevent or mitigate harms from AI systems.

Now contrast The Safe Scenario with The Catastrophic Scenario and The Engineering Scenario. In both The Catastrophic Scenario and The Engineering Scenario, the focus of the AI safety community is restricted in some way: in the former case, to catastrophic harms from future AI systems, in the latter case, to harm reduction efforts that employ engineering methods. We think that both kinds of restrictions are likely to make the AI safety community less effective at reducing the harms caused by AI systems. In The Catastrophic Scenario, there are some research projects focused on reducing the harms caused by AI systems — namely, those research projects which target present and/or non-catastrophic harms — which are automatically deprioritized. Similarly, there are some experts — namely, those experts who are knowledgeable about how to prevent or mitigate present and/or non-catastrophic harms — who will be excluded from political deliberation about AI safety. The same worry applies, mutatis mutandis, in The Engineering Scenario: research projects which fall outside the disciplinary boundaries of engineering will automatically be deprioritized, and experts knowledgeable about such research projects will be excluded from political deliberation about AI safety.

These differences from The Safe Scenario are likely to make the AI safety community less effective at reducing the harms caused by AI systems because they move it away from the practice of prioritizing research directions solely according to their merit *qua* harm reduction effort and consulting experts solely according to their expertise when it comes to harm reduction efforts. The focus on future catastrophic risks in The Catastrophic Scenario ignores the significant continuities between catastrophic and non-catastrophic risks from AI. This oversight strikes us as net safety-negative, since it is plausible that allocating AI safety resources to researchers with broader sociotechnical expertise will lead to new insights and proposals concerning the whole spectrum of risks, including catastrophic risks.

Another point to be made in this connection is that it is much easier to do — and, importantly, assess — technical work on systems that actually exist.[Footnote 44](#Fn44) Construing AI safety as concerned in the first instance only with catastrophic harms from future systems means that work on present systems can be justified as AI safety work only if it can be shown to reduce catastrophic risks from future systems. Any attempt to make this case will rely on an auxiliary premise, namely that there will be sufficient continuities between present and future systems that lessons learned from experiments on the former will apply to the latter (e.g. that effective oversight strategies on models which produce text-only outputs will continue to be effective on different classes of models, or that current insights from mechanistic interpretability will generalize beyond the transformer architecture).

Regardless of the plausibility of this premise (which will vary from case to case), it is independent of the actual research contribution made by experiments on present systems; two papers could perform similar sets of experiments, but only one could frame their contribution in terms of future systems. In practice, the researchers who will make this auxiliary premise explicit (i.e. who directly attempt to connect their contribution to reducing harms from future, more capable systems) will be precisely those who have already bought into The Catastrophic Conception. This kind of gatekeeping strikes us as likely to make the discipline of AI safety less effective at reducing the harms caused by AI systems.

Finally, there is a risk in The Catastrophic Scenario that the idea that AI safety only relates to catastrophic harms from future systems will enable model developers to talk about safety while resisting effective regulatory proposals (i.e. ‘safety-washing’ (Perrigo, [2023](/article/10.1007/s11098-025-02367-z#ref-CR81 "Perrigo, B. (2023). Exclusive: OpenAI Lobbied E.U. to Water Down AI Regulation’. TIME Magazine June 20, 2023. 
                  https://time.com/6288245/openai-eu-lobbying-ai-act/
                  
                "))).

Similar worries arise about The Engineering Scenario. Automatic deprioritization of research projects that do not adopt the methodology of safety engineering is likely to lead to less safe outcomes. And model developers could espouse a commitment to AI safety while ignoring safety-critical theoretical or sociotechnical research projects that do not fall within the disciplinary boundaries of engineering. These include in particular harms from autonomous, agentic AI, for which no analogue exists in traditional safety engineering.

There is a more general point to be made here, beyond the claim that the AI safety community in The Safe Scenario is likely to be more effective than the AI safety communities in The Catastrophic Scenario and The Engineering Scenario: *any* way of choosing research priorities or selecting which experts to consult in political deliberation about AI safety other than the one embodied in The Safe Scenario is likely to be less effective at reducing harm. In so far as The Safe Scenario is the scenario that embodies The Safety Conception, we have reason to believe that The Safety Conception is the best conception of AI safety for purpose (B).

We have argued that there are reasons for thinking that The Safety Conception does better than competitors when it comes to purpose (A), and also that it does better than competitors when it comes to purpose (B). It follows that The Safety Conception is the best conception of AI safety.

Before concluding, it is worth addressing a possible worry about our argument that The Safety Conception fares better than competitors with respect to reducing harms from AI systems. In particular, some might worry that the Safe Scenario is likely to be less safe than the Catastrophic Scenario because in the Safe Scenario, research directions aimed at preventing catastrophic harms from AI systems will be less prioritized.

It is important to realize that the fact that the AI safety community is not exclusively concerned with catastrophic harms in The Safe Scenario does not entail that work on preventing or mitigating catastrophic harms will be less prioritized. The situation is rather that in The Safe Scenario work focused on catastrophic harms will not be prioritized *automatically*. If efforts to prevent or mitigate catastrophic harms from AI systems have more merit *qua* harm reduction effort than other kinds of interventions, then they will be prioritized in The Safe Scenario as they are in The Catastrophic Scenario. Conversely, if it turns out that efforts to prevent or mitigate catastrophic harms do not have more merit *qua* harm reduction effort than other kinds of interventions (as we suspect it will), it seems to us that the proper conclusion to draw is that The Catastrophic Scenario is likely to be less safe than The Safe Scenario.

## 6 Conclusion

AI systems are potentially dangerous in myriad ways, and it is of central importance in deploying them to think carefully about how to prevent or mitigate the harms they can cause. This is the basic premise of AI safety research.

We have argued that this basic premise also picks out the best conception of AI safety as a field: The Safety Conception. When we think carefully about how to demarcate the field of AI safety in a way that is explanatorily fruitful and mitigates the harms AI systems can cause, it becomes clear that The Safety Conception is superior to rival proposals like The Catastrophic Conception and The Engineering Conception. It follows that the Safety Conception ought to be the operative concept of AI safety among AI safety researchers and policymakers, not merely the manifest concept.

Concretely, this means that research on social harms from AI should be presented at and published in the same venues as research on catastrophic harms; if existing conferences and publication venues cannot accommodate this, new ones should be created. It means that researchers on LLM toxicity should be following developments in mechanistic interpretability, and that researchers on AI deception should draw on sociotechnical work on misinformation. It means that AI labs should not have separate “ethics” and “safety” teams, and that AI safety funders should be open to funding research which does not explicitly frame its contribution in terms of reducing catastrophic or existential risks.

These suggestions, we anticipate, will strike many readers as obviously sensible, continuous with many proposals to broaden AI safety’s tent in recent years (Lazar & Nelson, [2023](/article/10.1007/s11098-025-02367-z#ref-CR71 "Lazar, S., & Nelson, A. (2023). AI Safety on Whose Terms? Science, 381, 6654: 138–138.")). We take this to be a virtue of The Safety Conception: it serves to motivate and justify common sense recommendations for disciplinary integration between those working on a large array of different harms from AI.

## Notes

1. See, for example, Bostrom ([2014](/article/10.1007/s11098-025-02367-z#ref-CR10 "Bostrom, N. (2014). Superintelligence: Paths, Dangers, Strategies. Oxford University Press.")), Russell ([2019](/article/10.1007/s11098-025-02367-z#ref-CR88 "Russell, S. (2019). Human Compatible: AI and the Problem of Control. Allen Lane.")), and Ngo et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR76 "Ngo, R., Chan, L., & Mindermann, S. (2023). The Alignment Problem from a Deep Learning Perspective (v5). ArXiv Preprint. 
                     https://arxiv.org/abs/2209.00626
                     
                   ")).
2. Adversarial robustness studies, and aims at improving, the resilience of AI models against perturbations intended to yield unwanted outputs. See, for example, Carlini et al. ([2019](/article/10.1007/s11098-025-02367-z#ref-CR18 "Carlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber, J., Tsipras, D., Goodfellow, I., Madry, A., & Kurakin, A. (2019). On evaluating adversarial robustness. ArXiv preprint. 
                     https://arxiv.org/abs/1902.06705
                     
                   ")), Hendrycks and Dietterich ([2019](/article/10.1007/s11098-025-02367-z#ref-CR54 "Hendrycks, D., & Dietterich, T. (2019). Benchmarking Neural Network Robustness to Common Corruptions and Perturbations. International Conference on Learning Representations 2019.")), and Bai et al. ([2021](/article/10.1007/s11098-025-02367-z#ref-CR6 "Bai, T., Luo, J., Zhao, J., Wen, B., & Wang, Q. (2021). Recent advances in adversarial training for adversarial robustness. International Joint Conference on Artificial Intelligence (IJCAI-21).")).
3. In this context, anomaly detection typically refers to methods for identifying inputs that fall outside of an AI system’s intended range of application. See, for example, Chandola et al. ([2009](/article/10.1007/s11098-025-02367-z#ref-CR20 "Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: A survey. ACM computing surveys (CSUR), 41, 1–58.")), Pang et al. ([2021](/article/10.1007/s11098-025-02367-z#ref-CR79 "Pang, G., Shen, C., Cao, L., & Hengel, A. V. D. (2021). Deep learning for anomaly detection: A review. ACM Computing Surveys (CSUR), 54(2), 1–38.")), and Yang et al. ([2021](/article/10.1007/s11098-025-02367-z#ref-CR107 "Yang, J., Zhou, K., Li, Y., & Liu, Z. (2021). Generalized out-of-distribution detection: A survey. ArXiv Preprint. 
                     https://arxiv.org/abs/2110.11334
                     
                   ")).
4. Model interpretation refers to methods aimed at giving human-understandable insight into how AI models (especially deep learning models) work. See, for example, Elhage et al. ([2021](/article/10.1007/s11098-025-02367-z#ref-CR35 "Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., & Olah, C. (2021). A Mathematical Framework for Transformer Circuits. Blog Post. 
                     https://transformer-circuits.pub/2021/framework/index.html
                     
                   ")), Geiger et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR40 "Geiger, A., Potts, C., & Icard, T. (2023). Causal Abstraction for Faithful Model Interpretation. ArXiv Preprint. 
                     https://arxiv.org/abs/2301.04709
                     
                   ")), and, for a philosophical perspective on recent model interpretability work, Harding ([2023](/article/10.1007/s11098-025-02367-z#ref-CR45 "Harding, J., & Kirk-Giannini, C. D. (2023). AI’s future worries us. So does AI’s present. Boston Globe July 14, 2023.")).
5. See, for example, Park et al. ([2024](/article/10.1007/s11098-025-02367-z#ref-CR80 "Park, P. S., Goldstein, S., O’Gara, A., Chen, M., & Hendrycks, D. (2024). AI deception: A survey of examples, risks, and potential solutions. Patterns, 5(5), 100988.")), Shevlane et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR93 "Shevlane, T., Farquhar, S., Garfinkel, B., Phuong, M., Whittlestone, J., Leung, J., Kokotajlo, D., Marchal, N., Anderljung, M., Kolt, N., & Ho, L. (2023). Model evaluation for extreme risks. ArXiv preprint. 
                     https://arxiv.org/abs/2305.15324
                     
                   ")), and Kinniment et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR64 "Kinniment, M., Sato, L. J. K., Du, H., Goodrich, B., Hasin, M., Chan, L., Miles, L. H., Lin, T. R., Wijk, H., Burget, J., Ho, A., Barnes, E., & Christiano, P. (2023). Evaluating Language-Model Agents on Realistic Autonomous Tasks. Alignment Research Center Report..")).
6. See, for example, Dafoe ([2018](/article/10.1007/s11098-025-02367-z#ref-CR30 "Dafoe, A. (2018). AI Governance: A Research Agenda. Oxford University Future of Humanity Institute Report. 
                     https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf
                     
                   ")), Brundage et al. ([2020](/article/10.1007/s11098-025-02367-z#ref-CR15 "Brundage, M., Avin, S., Wang, J., Belfield, H., Krueger, G., Hadfield, G., Khlaaf, H., Yang, J., Toner, H., Fong, R., Maharaj, T., Koh, P. W., Hooker, S., Leung, J., Trask, A., Bluemke, E., Lebensold, J., O’Keefe, C., Koren, M.… and, & Anderljung, M. (2020). Toward trustworthy AI development: mechanisms for supporting verifiable claims. ArXiv preprint. 
                     https://arxiv.org/abs/2004.07213
                     
                   ")), Anderljung et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR5 "Anderljung, M., Barnhart, J., Korinek, A., Leung, J., O’Keefe, C., Whittlestone, J., Avin, S., Brundage, M., Bullock, J., Cass-Beggs, D., Chang, B., Collins, T., Fist, T., Hadfield, G., Hayes, A., Ho, L., Hooker, S., Horvitz, E., Kolt, N.… and, & Wolf, K. (2023). Frontier AI regulation: Managing emerging risks to public safety. ArXiv preprint. 
                     https://arxiv.org/abs/2307.03718
                     
                   ")).
7. As a reviewer observes, this was not always true. Historically, ‘AI safety’ research used to be produced by a very small collection of researchers, distinguished by their overlaps with Effective Altruist and Rationalist communities (Ahmed et al., [2024](/article/10.1007/s11098-025-02367-z#ref-CR1 "Ahmed, S., Jaźwińska, K., Ahlawat, A., Winecoff, A., & Wang, M. (2024). April. ‘Field-Building and the Epistemic Culture of AI Safety’. First Monday, 14.")). These communities’ primary interests in AI safety stemmed from the mitigation of catastrophic (especially existential) AI-related risks. AI safety therefore has historically been characterized using the Catastrophic Conception (defined in Sect. [3.1](/article/10.1007/s11098-025-02367-z#Sec8)); the merits of this characterization went unquestioned. As AI has developed, such that its use has proliferated in society, there has been growing interest in broadening the scope of ‘AI safety’ (Lazar & Nelson, [2023](/article/10.1007/s11098-025-02367-z#ref-CR71 "Lazar, S., & Nelson, A. (2023). AI Safety on Whose Terms? Science, 381, 6654: 138–138.")). This paper can be seen as providing a philosophical foundation in support of this recent trend; as we read it, Lazar and Nelson’s ‘sociotechnical approach’ to AI safety picks out something like our Safety Conception.
8. Paul Trowler, for example, notes that “Disciplines are enacted as social practices are performed and as micropolitics are played out: teaching; research; conference attendance; departmental meetings;

   collaborative writing; mixed-disciplinary meetings of a political nature — for example where resource allocation is at stake; funding applications, etc.” (2012, 34). See also the other essays in Trowler et al. ([2012](/article/10.1007/s11098-025-02367-z#ref-CR99 "Trowler, P., Saunders, M., & Bamber, V. (Eds.). (2012). Tribes and Territories in the 21st Century: Rethinking the Significance of Disciplines in Higher Education. Routledge.")).
9. In what follows, we use the terms ‘discipline’ and ‘field’ interchangeably to refer to any recognizably cohesive community of researchers engaged in inquiry into a subject matter. We take it that it is clear that AI safety is a discipline in this sense. Some readers may wish to reserve the term ‘discipline’ for research communities that have achieved a high level of institutional recognition, e.g. by being housed in their own academic departments. Such readers are welcome to replace our talk of disciplines with talk of fields.
10. The *operative* concept of *X* in a population is the concept which determines how members of the population apply *X* in particular cases, while the *manifest* concept of *X* in a population is the concept which members of the population *take* to determine how they apply *X* in particular cases. For further discussion, see Haslanger ([1995](/article/10.1007/s11098-025-02367-z#ref-CR47 "Haslanger, S. (1995). Ontology and Social Construction. Philosophical Topics 23: 95–125. Reprinted in Haslanger (2012), pp. 83–112."), [2006](/article/10.1007/s11098-025-02367-z#ref-CR50 "Haslanger, S. (2006). What Good Are Our Intuitions? Philosophical Analysis and Social Kinds. Proceedings of the Aristotelian Society Supplementary Volume 80: 89–118. Reprinted in Haslanger (2012), pp. 381–405.")).
11. See, for example, Buolamwini and Gebru ([2018](/article/10.1007/s11098-025-02367-z#ref-CR16 "Buolamwini, J., & Gebru, T. (2018). Gender shades: Intersectional accuracy disparities in commercial gender classification. Proceedings of the 1st Conference on Fairness Accountability and Transparency, (PMLR) 81, 77–91.")), Noble ([2018](/article/10.1007/s11098-025-02367-z#ref-CR77 "Noble, S. U. (2018). Algorithms of Oppression: How Search Engines Reinforce Racism. NYU.")), and Bellamy et al. ([2019](/article/10.1007/s11098-025-02367-z#ref-CR8 "Bellamy, R. K., Dey, K., Hind, M., Hoffman, S. C., Houde, S., Kannan, K., Lohia, P., Martino, J., Mehta, S., Mojsilović, A., Nagar, S., Ramamurthy, K. N., Richards, J., Saha, D., Sattigeri, P., Singh, M., Varshney, K. R., & Zhang, Y. (2019). AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. IBM Journal of Research and Development, 63(4/5), 4: 1–15.")).
12. See, for example, Bradshaw and Howard ([2018](/article/10.1007/s11098-025-02367-z#ref-CR14 "Bradshaw, S., & Howard, P. N. (2018). Challenging truth and trust: A global inventory of organized social media manipulation. Oxford Internet Institute Computational Propaganda Project Report. 
                      https://demtech.oii.ox.ac.uk/wp-content/uploads/sites/12/2018/07/ct2018.pdf
                      
                    ")), Keller and Klinger ([2019](/article/10.1007/s11098-025-02367-z#ref-CR62 "Keller, T. R., & Klinger, U. (2019). Social bots in election campaigns: Theoretical, empirical, and methodological implications. Political Communication, 36, 171–189.")), and Aïmeur et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR2 "Aïmeur, E., Amri, S., & Brassard, G. (2023). Fake news, disinformation and misinformation in social media: A review. Social Network Analysis and Mining, 13(30), 1–36.")).
13. See, for example, Allen ([2016](/article/10.1007/s11098-025-02367-z#ref-CR3 "Allen, A. L. (2016). Protecting one’s own privacy in a big data economy. Harvard Law Review Forum, 130, 71–78.")), Feldstein ([2019](/article/10.1007/s11098-025-02367-z#ref-CR37 "Feldstein, S. (2019). The Global Expansion of AI Surveillance. Carnegie Endowment for International Peace Working Paper.
                      https://carnegieendowment.org/files/WP-Feldstein-AISurveillance_final1.pdf
                      
                    ")), and Manheim and Kaplan ([2019](/article/10.1007/s11098-025-02367-z#ref-CR73 "Manheim, K., & Kaplan, L. (2019). Artificial intelligence: Risks to privacy and democracy. Yale Journal of Law and Technology, 21, 106–188.")).
14. This includes cases of artists’ work being used as training data without their consent (see, for example, Chen ([2023](/article/10.1007/s11098-025-02367-z#ref-CR21 "Chen, M. (2023). (2023). Artists and Illustrators Are Suing Three A.I. Art Generators for Scraping and Collaging Their Work Without Consent’. Artnet News February 16, 2023. 
                      https://news.artnet.com/art-world/class-action-lawsuit-lensa-ai-prisma-labs-biometric-information-2257096
                      
                    "))).
15. See, for example, Howard et al. ([2018](/article/10.1007/s11098-025-02367-z#ref-CR57 "Howard, P. N., Woolley, S., & Calo, R. (2018). Algorithms, bots, and political communication in the US 2016 election: The challenge of automated political communication for election law and administration. Journal of Information Technology & Politics, 15, 81–93.")), Manheim and Kaplan ([2019](/article/10.1007/s11098-025-02367-z#ref-CR73 "Manheim, K., & Kaplan, L. (2019). Artificial intelligence: Risks to privacy and democracy. Yale Journal of Law and Technology, 21, 106–188.")), Helbing et al. ([2019](/article/10.1007/s11098-025-02367-z#ref-CR52 "Helbing, D., Frey, B. S., Gigerenzer, G., Hafen, E., Hagner, M., Hofstetter, Y., Van Den Hoven, J., Zicari, R. V., & Zwitter, A. (2019). Will democracy survive big data and artificial intelligence? In D. Helbing (Ed.), Towards Digital Enlightenment: Essays on the Dark and Light Sides of the Digital Revolution (pp. 73–98). Springer.")).
16. See, for example, Nemitz ([2018](/article/10.1007/s11098-025-02367-z#ref-CR74 "Nemitz, P. (2018). Constitutional democracy and technology in the age of artificial intelligence. Philosophical Transactions of the Royal Society A, 376, 20180089.")), Crawford ([2021](/article/10.1007/s11098-025-02367-z#ref-CR29 "Crawford, K. (2021). Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. Yale University Press.")), and Bommasani et al. ([2021](/article/10.1007/s11098-025-02367-z#ref-CR9 "Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D.… and, & Liang, P. (2021). On the Opportunities and Risks of Foundation Models. ArXiv preprint. 
                      https://arxiv.org/abs/2108.07258
                      
                    ")).
17. <[https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-an-overview/>](https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-an-overview/%3E).
18. <[https://www.nist.gov/system/files/documents/2024/05/21/AISI-vision-21May2024.pdf>](https://www.nist.gov/system/files/documents/2024/05/21/AISI-vision-21May2024.pdf%3E).
19. <[https://en.wikipedia.org/wiki/AI\_safety>](https://en.wikipedia.org/wiki/AI_safety%3E).
20. See, for example, Weidinger et al. ([2021](/article/10.1007/s11098-025-02367-z#ref-CR102 "Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P. S., & Gabriel, I. (2021). Ethical and social risks of harm from language models. ArXiv preprint. 
                      https://arxiv.org/abs/2112.04359
                      
                    ")), Weidinger et al. ([2022](/article/10.1007/s11098-025-02367-z#ref-CR104 "Weidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang, P. S., Mellor, J., & Gabriel, I. (2022). Taxonomy of risks posed by language models. Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency: 214–229.")), Shelby et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR92 "Shelby, R., Rismani, S., Henne, K., Moon, A. J., Rostamzadeh, N., Nicholas, P., Yilla-Akbari, N.… and, & Virk, G. (2023). Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for Harm Reduction. Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society: 723–741.")), and Solaiman et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR94 "Solaiman, I., Talat, Z., Agnew, W., Ahmad, L., Baker, D., Blodgett, S. L., & Vassilev, A. (2023). Evaluating the Social Impact of Generative AI Systems in Systems and Society. ArXiv preprint. 
                      https://arxiv.org/abs/2306.05949
                      
                    ")).
21. See, for example, Kinniment et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR64 "Kinniment, M., Sato, L. J. K., Du, H., Goodrich, B., Hasin, M., Chan, L., Miles, L. H., Lin, T. R., Wijk, H., Burget, J., Ho, A., Barnes, E., & Christiano, P. (2023). Evaluating Language-Model Agents on Realistic Autonomous Tasks. Alignment Research Center Report..")), Wijk et al. ([2024](/article/10.1007/s11098-025-02367-z#ref-CR105 "Wijk, H., Lin, T., Becker, J., Jawhar, S., Parikh, N., Broadley, T., & Barnes, E. (2024). RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts. ArXiv preprint. 
                      https://arxiv.org/abs/2411.15114
                      
                    ")).
22. See, for example, Ganguli et al. ([2022](/article/10.1007/s11098-025-02367-z#ref-CR39 "Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., & Clark, J. (2022). Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. ArXiv preprint. 
                      https://arxiv.org/abs/2209.07858
                      
                    ")).
23. See, for example, Park et al. ([2024](/article/10.1007/s11098-025-02367-z#ref-CR80 "Park, P. S., Goldstein, S., O’Gara, A., Chen, M., & Hendrycks, D. (2024). AI deception: A survey of examples, risks, and potential solutions. Patterns, 5(5), 100988.")).
24. See Christiano et al. ([2017](/article/10.1007/s11098-025-02367-z#ref-CR22 "Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., & Amodei, D. (2017). Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30.")), Rafailov ([2024](/article/10.1007/s11098-025-02367-z#ref-CR82 "Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., & Finn, C. (2024). Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems 36.")).
25. See, for example, Bowman et al. ([2022](/article/10.1007/s11098-025-02367-z#ref-CR13 "Bowman, S., Hyun, J., Perez, E., Chen, E., Pettit, C., Heiner, S., & Kaplan, J. (2022). Measuring progress on scalable oversight for large language models. ArXiv preprint. 
                      https://arxiv.org/abs/2211.03540
                      
                    ")).
26. Irving et al. ([2018](/article/10.1007/s11098-025-02367-z#ref-CR59 "Irving, G., Christiano, P., & Amodei, D. (2018). AI safety via debate. ArXiv preprint. 
                      https://arxiv.org/abs/1805.00899
                      
                    ")).
27. See Pang et al. ([2021](/article/10.1007/s11098-025-02367-z#ref-CR79 "Pang, G., Shen, C., Cao, L., & Hengel, A. V. D. (2021). Deep learning for anomaly detection: A review. ACM Computing Surveys (CSUR), 54(2), 1–38.")) for an overview.
28. See, for example, Li et al. ([2024](/article/10.1007/s11098-025-02367-z#ref-CR72 "Li, K., Patel, O., Viégas, F., Pfister, H., & Wattenberg, M. (2024). Inference-time intervention: Eliciting truthful answers from a language model. Advances in Neural Information Processing Systems 36.")).
29. See, for example, Raji et al. ([2020](/article/10.1007/s11098-025-02367-z#ref-CR83 "Raji, I. D., Smart, A., White, R. M., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, D., & Barnes, P. (2020). Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing. Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency: 33–44.")), Costanza-Chock et al. ([2022](/article/10.1007/s11098-025-02367-z#ref-CR26 "Costanza-Chock, S., Raji, I. D., & Buolamwini, J. (2022). Who Audits the Auditors? Recommendations from a Field Scan of the Algorithmic Auditing Ecosystem. Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’22), 1571–83.")).
30. See, for example, Dafoe ([2018](/article/10.1007/s11098-025-02367-z#ref-CR30 "Dafoe, A. (2018). AI Governance: A Research Agenda. Oxford University Future of Humanity Institute Report. 
                      https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf
                      
                    ")), Brundage et al. ([2020](/article/10.1007/s11098-025-02367-z#ref-CR15 "Brundage, M., Avin, S., Wang, J., Belfield, H., Krueger, G., Hadfield, G., Khlaaf, H., Yang, J., Toner, H., Fong, R., Maharaj, T., Koh, P. W., Hooker, S., Leung, J., Trask, A., Bluemke, E., Lebensold, J., O’Keefe, C., Koren, M.… and, & Anderljung, M. (2020). Toward trustworthy AI development: mechanisms for supporting verifiable claims. ArXiv preprint. 
                      https://arxiv.org/abs/2004.07213
                      
                    ")), Anderljung et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR5 "Anderljung, M., Barnhart, J., Korinek, A., Leung, J., O’Keefe, C., Whittlestone, J., Avin, S., Brundage, M., Bullock, J., Cass-Beggs, D., Chang, B., Collins, T., Fist, T., Hadfield, G., Hayes, A., Ho, L., Hooker, S., Horvitz, E., Kolt, N.… and, & Wolf, K. (2023). Frontier AI regulation: Managing emerging risks to public safety. ArXiv preprint. 
                      https://arxiv.org/abs/2307.03718
                      
                    ")).
31. For more on the value of transparency, see Grant et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR42 "Grant, D. G., Behrends, J., & Basl, J. (2023). What We Owe to Decision-Subjects: Beyond Transparency and Explanation in Automated Decision-Making. Philosophical Studies. Online First.")).
32. Verma et al. ([2022](/article/10.1007/s11098-025-02367-z#ref-CR101 "Verma, S., Boonsanong, V., Hoang, M., Hines, K. E., Dickerson, J. P., & Shah, C. (2022). Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review. ArXiv Preprint. 
                      https://arxiv.org/abs/2010.10596
                      
                    ")).
33. For the notion of a catastrophic or existential risk, see Ord ([2020](/article/10.1007/s11098-025-02367-z#ref-CR78 "Ord, T. (2020). The Precipice: Existential Risk and the Future of Humanity. Hachette Books.")) and Bostrom and Ćirković ([2008](/article/10.1007/s11098-025-02367-z#ref-CR11 "Bostrom, N., & Ćirković, M. M. (Eds.). (2008). Global Catastrophic Risks. Oxford University Press.")). For non-academic discussions of this way of thinking about AI safety, see Harding and Kirk-Giannini ([2023](/article/10.1007/s11098-025-02367-z#ref-CR45 "Harding, J., & Kirk-Giannini, C. D. (2023). AI’s future worries us. So does AI’s present. Boston Globe July 14, 2023.")), Roose ([2023](/article/10.1007/s11098-025-02367-z#ref-CR87 "Roose, K. (2023). A.I. Poses ‘Risk of Extinction,’ Industry Leaders Warn. New York Times May 30, 2023.")), and Schneier and Sanders ([2023](/article/10.1007/s11098-025-02367-z#ref-CR89 "Schneier, B., & Sanders, N. (2023). The A.I. Wars Have Three Factions, and They All Crave Power. New York Times September 28, 2023.")).
34. The Machine Intelligence Research Institute (MIRI), an organization that was influential in AI safety’s development as a research field, explains the need for AI safety research as follows: “If [an AI] system’s assigned problems/tasks/objectives don’t fully capture our real objectives, it will likely end up with incentives that catastrophically conflict with what we actually want… our take-away from this is that we should prioritize early research into aligning future AI systems with our interests.”
35. <[https://en.wikipedia.org/wiki/AI\_safety>](https://en.wikipedia.org/wiki/AI_safety%3E).
36. In this context, the expression “long-tail risks” should be understood as roughly synonymous with “catastrophic risks.”
37. <[https://www.alignmentforum.org/posts/Yp2vYb4zHXEeoTkJc/welcome-and-faq>](https://www.alignmentforum.org/posts/Yp2vYb4zHXEeoTkJc/welcome-and-faq%3E).
38. Note that our point here would apply even if Ngo *was* intending to commit to the existence of two disciplines. We argue that the best conception of AI safety does not draw a sharp disciplinary boundary between catastrophic/existential and non-catastrophic risks. So our arguments are intended to apply to any view which attempts to carve out a separate discipline which deals only with existential risks from advanced AI, regardless of what label it gives this discipline.
39. See, amongst others, Hutchins ([1995](/article/10.1007/s11098-025-02367-z#ref-CR58 "Hutchins, E. (1995). How a Cockpit Remembers Its Speeds. Cognitive Science, 19(3), 265–288.")), Yampolskiy and Fox ([2013](/article/10.1007/s11098-025-02367-z#ref-CR106 "Yampolskiy, R., & Fox, J. (2013). Safety Engineering for Artificial General Intelligence. Topoi, 32, 217–226.")), Dobbe ([2022](/article/10.1007/s11098-025-02367-z#ref-CR34 "Dobbe, R. I. J. (2022). System Safety and Artificial Intelligence. In B. Justin, Y. C. Bullock, J. Chen, V. M. Himmelreich, A. Hudson, M. M. Korinek, Young, & B. Zhang (Eds.), The Oxford Handbook of AI Governance. Oxford University Press.")), Weidinger et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR103 "Weidinger, L., Rauh, M., Marchal, N., Manzini, A., Hendricks, L. A., Mateos-Garcia, Bergman, S.… and, & Isaac, W. (2023). Sociotechnical Safety Evaluation of Generative AI Systems. ArXiv Preprint. 
                      https://arxiv.org/abs/2310.11986
                      
                    ")), Rismani, Shelby, Smart, Delos Santos et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR84 "Rismani, S., Shelby, R., Smart, A., Delos Santos, R., Moon, A. J., & Rostamzadeh, N. (2023). Beyond the ML Model: Applying Safety Engineering Frameworks to Text-to-Image Development. Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society: 70–83.")), Rismani, Shelby, Smart, Jatho et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR85 "Rismani, S., Shelby, R., Smart, A., Jatho, E., Kroll, J., Moon, A. J., & Rostamzadeh, N. (2023). From Plane Crashes to Algorithmic Harm: Applicability of Safety Engineering Frameworks for Responsible ML. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems: 1–18.")); Fang and Johnson ([2019](/article/10.1007/s11098-025-02367-z#ref-CR36 "Fang, X., & Johnson, N. (2019). Three Reasons Why: Framing the Challenges of Assuring AI. In Alexander Romanovsky, Elena Troubitsyna, Ilir Gashi, Erwin Schoitsch, and Friedemann Bitsch (Eds.) Computer Safety, Reliability, and Security, Springer, 281–87.")); Hendrycks et al. ([2022](/article/10.1007/s11098-025-02367-z#ref-CR53 "Hendrycks, D., Carlini, N., Schulman, J., & Steinhardt, J. (2022). Unsolved Problems in ML Safety. ArXiv Preprint. 
                      https://arxiv.org/abs/2109.13916
                      
                    ")); Khlaaf et al. ([2022](/article/10.1007/s11098-025-02367-z#ref-CR63 "Khlaaf, H., Mishkin, P., Achiam, J., Krueger, G., & Brundage, M. (2022). A Hazard Analysis Framework for Code Synthesis Large Language Models. ArXiv preprint. 
                      https://arxiv.org/abs/2207.14157
                      
                    ")); Koessler and Schuett ([2023](/article/10.1007/s11098-025-02367-z#ref-CR68 "Koessler, L., & Schuett, J. (2023). Risk Assessment at AGI Companies: A Review of Popular Risk Assessment Techniques from Other Safety-Critical Industries. ArXiv Preprint. 
                      https://arxiv.org/abs/2307.08823
                      
                    ")); Trapp et al. ([2018](/article/10.1007/s11098-025-02367-z#ref-CR97 "Trapp, M., Schneider, D., & Weiss, G. (2018). Towards Safety-Awareness and Dynamic Safety Management. 14th European Dependable Computing Conference (EDCC), 107–11.")); Raji et al. ([2020](/article/10.1007/s11098-025-02367-z#ref-CR83 "Raji, I. D., Smart, A., White, R. M., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, D., & Barnes, P. (2020). Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing. Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency: 33–44.")); Costanza-Chock et al. ([2022](/article/10.1007/s11098-025-02367-z#ref-CR26 "Costanza-Chock, S., Raji, I. D., & Buolamwini, J. (2022). Who Audits the Auditors? Recommendations from a Field Scan of the Algorithmic Auditing Ecosystem. Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’22), 1571–83.")).
40. For further discussion, see Corso et al. ([2023](/article/10.1007/s11098-025-02367-z#ref-CR25 "Corso, A., Karamadian, D., Valentin, R., Cooper, M., & Kochenderfer, M. J. (2023). A Holistic Assessment of the Reliability of Machine Learning Systems. ArXiv preprint. 
                      https://arxiv.org/abs/2307.10586
                      
                    ")).
41. Other recent applications of the methodology of conceptual engineering for explanatory utility include Tanswell ([2018](/article/10.1007/s11098-025-02367-z#ref-CR95 "Tanswell, F. S. (2018). Conceptual engineering for mathematical concepts. Inquiry: A Journal Of Medical Care Organization, Provision And Financing, 61, 881–913.")), Isaac ([2020](/article/10.1007/s11098-025-02367-z#ref-CR60 "Isaac, M. G. (2020). How to conceptually engineer conceptual engineering? Inquiry. Online First.")), and Kirk-Giannini ([2023](/article/10.1007/s11098-025-02367-z#ref-CR66 "Kirk-Giannini, C. D. (2023). Dilemmatic gaslighting. Philosophical Studies 180: 745–772."), forthcoming).
42. It is sometimes also called *analytical inquiry* (Haslanger, [2000](/article/10.1007/s11098-025-02367-z#ref-CR48 "Haslanger, S. (2000). Gender and race: (What) are they? (What) do we want them to be? Noûs 34: 31–55. Reprinted in Haslanger (2012), pp. 221–247.")).
43. On this subject, see also Kasirzadeh ([2024](/article/10.1007/s11098-025-02367-z#ref-CR61 "Kasirzadeh, A. (2024). Two Types of AI Existential Risk: Decisive and Accumulative. ArXiv preprint. 
                      https://arxiv.org/abs/2401.07836
                      
                    ")).
44. This is not to say that it is impossible to do empirical work aimed at future systems; see, e.g., our discussion of ‘scalable oversight’ above.

## References

* Ahmed, S., Jaźwińska, K., Ahlawat, A., Winecoff, A., & Wang, M. (2024). April. ‘Field-Building and the Epistemic Culture of AI Safety’. *First Monday*, 14.
* Aïmeur, E., Amri, S., & Brassard, G. (2023). Fake news, disinformation and misinformation in social media: A review. *Social Network Analysis and Mining*, *13*(30), 1–36.

  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Fake%20news%2C%20disinformation%20and%20misinformation%20in%20social%20media%3A%20A%20review&journal=Social%20Network%20Analysis%20and%20Mining&volume=13&issue=30&pages=1-36&publication_year=2023&author=A%C3%AFmeur%2CE&author=Amri%2CS&author=Brassard%2CG)
* Allen, A. L. (2016). Protecting one’s own privacy in a big data economy. *Harvard Law Review Forum*, *130*, 71–78.

  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Protecting%20one%E2%80%99s%20own%20privacy%20in%20a%20big%20data%20economy&journal=Harvard%20Law%20Review%20Forum&volume=130&pages=71-78&publication_year=2016&author=Allen%2CAL)
* Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & Mané, D. (2016). Concrete problems in AI safety. ArXiv preprint. <https://arxiv.org/abs/1606.06565>
* Anderljung, M., Barnhart, J., Korinek, A., Leung, J., O’Keefe, C., Whittlestone, J., Avin, S., Brundage, M., Bullock, J., Cass-Beggs, D., Chang, B., Collins, T., Fist, T., Hadfield, G., Hayes, A., Ho, L., Hooker, S., Horvitz, E., Kolt, N.… and, & Wolf, K. (2023). Frontier AI regulation: Managing emerging risks to public safety. ArXiv preprint. <https://arxiv.org/abs/2307.03718>
* Bai, T., Luo, J., Zhao, J., Wen, B., & Wang, Q. (2021). Recent advances in adversarial training for adversarial robustness. *International Joint Conference on Artificial Intelligence (IJCAI-21).*
* Bales, A., D’Alessandro, W., & Kirk-Giannini, C. D. (2024). Artificial intelligence: Arguments for catastrophic risk. *Philosophy Compass*, *19*(2), e12964.

  [Article](https://doi.org/10.1111%2Fphc3.12964) 
  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Artificial%20intelligence%3A%20Arguments%20for%20catastrophic%20risk&journal=Philosophy%20Compass&doi=10.1111%2Fphc3.12964&volume=19&issue=2&publication_year=2024&author=Bales%2CA&author=D%E2%80%99Alessandro%2CW&author=Kirk-Giannini%2CCD)
* Bellamy, R. K., Dey, K., Hind, M., Hoffman, S. C., Houde, S., Kannan, K., Lohia, P., Martino, J., Mehta, S., Mojsilović, A., Nagar, S., Ramamurthy, K. N., Richards, J., Saha, D., Sattigeri, P., Singh, M., Varshney, K. R., & Zhang, Y. (2019). AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. *IBM Journal of Research and Development*, *63*(4/5), 4: 1–15.

  [Article](https://doi.org/10.1147%2FJRD.2019.2942287) 
  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=AI%20Fairness%20360%3A%20An%20extensible%20toolkit%20for%20detecting%20and%20mitigating%20algorithmic%20bias&journal=IBM%20Journal%20of%20Research%20and%20Development&doi=10.1147%2FJRD.2019.2942287&volume=63&issue=4%2F5&pages=1-15&publication_year=2019&author=Bellamy%2CRK&author=Dey%2CK&author=Hind%2CM&author=Hoffman%2CSC&author=Houde%2CS&author=Kannan%2CK&author=Lohia%2CP&author=Martino%2CJ&author=Mehta%2CS&author=Mojsilovi%C4%87%2CA&author=Nagar%2CS&author=Ramamurthy%2CKN&author=Richards%2CJ&author=Saha%2CD&author=Sattigeri%2CP&author=Singh%2CM&author=Varshney%2CKR&author=Zhang%2CY)
* Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D.… and, & Liang, P. (2021). On the Opportunities and Risks of Foundation Models. ArXiv preprint. <https://arxiv.org/abs/2108.07258>
* Bostrom, N. (2014). *Superintelligence: Paths, Dangers, Strategies*. Oxford University Press.
* Bostrom, N., & Ćirković, M. M. (Eds.). (2008). *Global Catastrophic Risks*. Oxford University Press.
* Bowman, S. (2022). ‘AI Safety and Neighboring Communities: A Quick-Start Guide, as of Summer 2022’, 1 September 2022. <https://www.lesswrong.com/posts/EFpQcBmfm2bFfM4zM/ai-safety-and-neighboring-communities-a-quick-start-guide-as.>
* Bowman, S., Hyun, J., Perez, E., Chen, E., Pettit, C., Heiner, S., & Kaplan, J. (2022). Measuring progress on scalable oversight for large language models. *ArXiv preprint*. <https://arxiv.org/abs/2211.03540>
* Bradshaw, S., & Howard, P. N. (2018). Challenging truth and trust: A global inventory of organized social media manipulation. *Oxford Internet Institute Computational Propaganda Project Report*. <https://demtech.oii.ox.ac.uk/wp-content/uploads/sites/12/2018/07/ct2018.pdf>
* Brundage, M., Avin, S., Wang, J., Belfield, H., Krueger, G., Hadfield, G., Khlaaf, H., Yang, J., Toner, H., Fong, R., Maharaj, T., Koh, P. W., Hooker, S., Leung, J., Trask, A., Bluemke, E., Lebensold, J., O’Keefe, C., Koren, M.… and, & Anderljung, M. (2020). Toward trustworthy AI development: mechanisms for supporting verifiable claims. ArXiv preprint. <https://arxiv.org/abs/2004.07213>
* Buolamwini, J., & Gebru, T. (2018). Gender shades: Intersectional accuracy disparities in commercial gender classification. *Proceedings of the 1st Conference on Fairness Accountability and Transparency*, *(PMLR) 81*, 77–91.

  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Gender%20shades%3A%20Intersectional%20accuracy%20disparities%20in%20commercial%20gender%20classification&journal=Proceedings%20of%20the%201st%20Conference%20on%20Fairness%20Accountability%20and%20Transparency&volume=%28PMLR%29%2081&pages=77-91&publication_year=2018&author=Buolamwini%2CJ&author=Gebru%2CT)
* Cappelen, H. (2018). *Fixing Language: An Essay on Conceptual Engineering*. Oxford University Press.
* Carlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber, J., Tsipras, D., Goodfellow, I., Madry, A., & Kurakin, A. (2019). On evaluating adversarial robustness. ArXiv preprint. <https://arxiv.org/abs/1902.06705>
* Carlsmith, J. (2021). Is power-seeking AI an existential risk? ArXiv preprint. <https://arxiv.org/abs/2206.13353>
* Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: A survey. *ACM computing surveys (CSUR)*, *41*, 1–58.

  [Article](https://doi.org/10.1145%2F1541880.1541882) 
  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Anomaly%20detection%3A%20A%20survey&journal=ACM%20computing%20surveys%20%28CSUR%29&doi=10.1145%2F1541880.1541882&volume=41&pages=1-58&publication_year=2009&author=Chandola%2CV&author=Banerjee%2CA&author=Kumar%2CV)
* Chen, M. (2023). (2023). Artists and Illustrators Are Suing Three A.I. Art Generators for Scraping and Collaging Their Work Without Consent’. *Artnet News* February 16, 2023. <https://news.artnet.com/art-world/class-action-lawsuit-lensa-ai-prisma-labs-biometric-information-2257096>
* Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., & Amodei, D. (2017). Deep reinforcement learning from human preferences. *Advances in neural information processing systems*, 30.
* Clark, J., & Amodei, D. (2016). Faulty reward function in the wild. OpenAI blog post. <https://openai.com/research/faulty-reward-functions>
* Clark, A., & Chalmers, D. (1998). The extended mind. *Analysis*, *58*, 7–19.

  [Article](https://doi.org/10.1093%2Fanalys%2F58.1.7) 
  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20extended%20mind&journal=Analysis&doi=10.1093%2Fanalys%2F58.1.7&volume=58&pages=7-19&publication_year=1998&author=Clark%2CA&author=Chalmers%2CD)
* Corso, A., Karamadian, D., Valentin, R., Cooper, M., & Kochenderfer, M. J. (2023). A Holistic Assessment of the Reliability of Machine Learning Systems. ArXiv preprint. <https://arxiv.org/abs/2307.10586>
* Costanza-Chock, S., Raji, I. D., & Buolamwini, J. (2022). Who Audits the Auditors? Recommendations from a Field Scan of the Algorithmic Auditing Ecosystem. *Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’22)*, 1571–83.
* Cotra, A. (2021). The Case for Aligning Narrowly Superhuman Models. Alignment Forum Blog Post. <https://www.alignmentforum.org/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models>
* Crawford, K. (2021). *Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence*. Yale University Press.
* D’Alessandro, W. (2024). Deontology and safe artificial intelligence. *Philosophical Studies*. Online First.
* Dafoe, A. (2018). AI Governance: A Research Agenda. *Oxford University Future of Humanity Institute Report*. <https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf>
* Dalrymple, D., Skalse, J., Bengio, Y., Russell, S., Tegmark, M., Seshia, S., Omohundro, S. (2024). Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems. ArXiv preprint. <https://doi.org/10.48550/arXiv.2405.06624>
* Dembroff, R. (2016). What is sexual orientation? *Philosophers’ Imprint*, *16*, 1–27.

  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=What%20is%20sexual%20orientation%3F&journal=Philosophers%E2%80%99%20Imprint&volume=16&pages=1-27&publication_year=2016&author=Dembroff%2CR)
* Dobbe, R. I. J. (2022). System Safety and Artificial Intelligence. In B. Justin, Y. C. Bullock, J. Chen, V. M. Himmelreich, A. Hudson, M. M. Korinek, Young, & B. Zhang (Eds.), *The Oxford Handbook of AI Governance*. Oxford University Press.
* Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., & Olah, C. (2021). A Mathematical Framework for Transformer Circuits. Blog Post. <https://transformer-circuits.pub/2021/framework/index.html>
* Fang, X., & Johnson, N. (2019). Three Reasons Why: Framing the Challenges of Assuring AI. In Alexander Romanovsky, Elena Troubitsyna, Ilir Gashi, Erwin Schoitsch, and Friedemann Bitsch (Eds.) *Computer Safety, Reliability, and Security*, Springer, 281–87.
* Feldstein, S. (2019). The Global Expansion of AI Surveillance. *Carnegie Endowment for International Peace Working Paper.*<https://carnegieendowment.org/files/WP-Feldstein-AISurveillance_final1.pdf>
* Gabriel, I. (2020). Artificial intelligence, values, and alignment. *Minds and Machines*, *30*(3), 411–437.

  [Article](https://link.springer.com/doi/10.1007/s11023-020-09539-2) 
  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Artificial%20intelligence%2C%20values%2C%20and%20alignment&journal=Minds%20and%20Machines&doi=10.1007%2Fs11023-020-09539-2&volume=30&issue=3&pages=411-437&publication_year=2020&author=Gabriel%2CI)
* Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., & Clark, J. (2022). Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. ArXiv preprint. <https://arxiv.org/abs/2209.07858>
* Geiger, A., Potts, C., & Icard, T. (2023). Causal Abstraction for Faithful Model Interpretation. ArXiv Preprint. <https://arxiv.org/abs/2301.04709>
* Goldstein, S., & Kirk-Giannini, C. D. (2025). Language Agents Reduce the Risk of Existential Catastrophe. *AI & Society* 40: 959–969.
* Grant, D. G., Behrends, J., & Basl, J. (2023). What We Owe to Decision-Subjects: Beyond Transparency and Explanation in Automated Decision-Making. *Philosophical Studies*. Online First.
* Green, B., Lily, & Hu (2018). The Myth in the Methodology: Towards Recontextualization of Fairness in Machine Learning. *Presented at the 35th International Conference on Machine Learning (ICML ’18).*
* Harding, J. (2023). Operationalising Representation in Natural Language Processing. *British Journal for the Philosophy of Science*. <https://arxiv.org/abs/2306.08193>
* Harding, J., & Kirk-Giannini, C. D. (2023). AI’s future worries us. So does AI’s present. *Boston Globe* July 14, 2023.
* Harding, J., & Sharadin, N. (Forthcoming). What is it for a Machine Learning Model to Have a Capability? *British Journal for the Philosophy of Science*.
* Haslanger, S. (1995). Ontology and Social Construction. *Philosophical Topics* 23: 95–125. Reprinted in Haslanger (2012), pp. 83–112.
* Haslanger, S. (2000). Gender and race: (What) are they? (What) do we want them to be? *Noûs* 34: 31–55. Reprinted in Haslanger (2012), pp. 221–247.
* Haslanger, S. (2005). What Are We Talking About? The Semantics and Politics of Social Kinds. *Hypatia* 20(4):10–26. Reprinted in Haslanger (2012), pp. 365–380.
* Haslanger, S. (2006). What Good Are Our Intuitions? Philosophical Analysis and Social Kinds. *Proceedings of the Aristotelian Society Supplementary Volume* 80: 89–118. Reprinted in Haslanger (2012), pp. 381–405.
* Haslanger, S. (2012). *Resisting Reality: Social Construction and Social Critique*. Oxford University Press.
* Helbing, D., Frey, B. S., Gigerenzer, G., Hafen, E., Hagner, M., Hofstetter, Y., Van Den Hoven, J., Zicari, R. V., & Zwitter, A. (2019). Will democracy survive big data and artificial intelligence? In D. Helbing (Ed.), *Towards Digital Enlightenment: Essays on the Dark and Light Sides of the Digital Revolution* (pp. 73–98). Springer.
* Hendrycks, D., & Dietterich, T. (2019). Benchmarking Neural Network Robustness to Common Corruptions and Perturbations. *International Conference on Learning Representations 2019*.
* Hendrycks, D., Mazeika, M., Zou, A., Patel, S., Zhu, C., Navarro, J., Song, D., Li, B., & Steinhardt, J. (2021). What would Jiminy Cricket do? Toward agents that behave morally. *35th Conference on Neural Information Processing Systems (NeurIPS 2021)*.
* Hendrycks, D., Carlini, N., Schulman, J., & Steinhardt, J. (2022). Unsolved Problems in ML Safety. ArXiv Preprint. <https://arxiv.org/abs/2109.13916>
* Hendrycks, D., Mazeika, M., & Woodside, T. (2023). An Overview of Catastrophic AI Risks. ArXiv preprint. <https://arxiv.org/abs/2306.12001>
* Howard, P. N., Woolley, S., & Calo, R. (2018). Algorithms, bots, and political communication in the US 2016 election: The challenge of automated political communication for election law and administration. *Journal of Information Technology & Politics*, *15*, 81–93.

  [Article](https://doi.org/10.1080%2F19331681.2018.1448735) 
  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Algorithms%2C%20bots%2C%20and%20political%20communication%20in%20the%20US%202016%20election%3A%20The%20challenge%20of%20automated%20political%20communication%20for%20election%20law%20and%20administration&journal=Journal%20of%20Information%20Technology%20%26%20Politics&doi=10.1080%2F19331681.2018.1448735&volume=15&pages=81-93&publication_year=2018&author=Howard%2CPN&author=Woolley%2CS&author=Calo%2CR)
* Hutchins, E. (1995). How a Cockpit Remembers Its Speeds. *Cognitive Science*, *19*(3), 265–288.

  [Article](https://doi.org/10.1207%2Fs15516709cog1903_1) 
  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=How%20a%20Cockpit%20Remembers%20Its%20Speeds&journal=Cognitive%20Science&doi=10.1207%2Fs15516709cog1903_1&volume=19&issue=3&pages=265-288&publication_year=1995&author=Hutchins%2CE)
* Irving, G., Christiano, P., & Amodei, D. (2018). AI safety via debate. ArXiv preprint. <https://arxiv.org/abs/1805.00899>
* Isaac, M. G. (2020). *How to conceptually engineer conceptual engineering? Inquiry*. Online First.
* Kasirzadeh, A. (2024). Two Types of AI Existential Risk: Decisive and Accumulative. ArXiv preprint. <https://arxiv.org/abs/2401.07836>
* Keller, T. R., & Klinger, U. (2019). Social bots in election campaigns: Theoretical, empirical, and methodological implications. *Political Communication*, *36*, 171–189.

  [Article](https://doi.org/10.1080%2F10584609.2018.1526238) 
  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Social%20bots%20in%20election%20campaigns%3A%20Theoretical%2C%20empirical%2C%20and%20methodological%20implications&journal=Political%20Communication&doi=10.1080%2F10584609.2018.1526238&volume=36&pages=171-189&publication_year=2019&author=Keller%2CTR&author=Klinger%2CU)
* Khlaaf, H., Mishkin, P., Achiam, J., Krueger, G., & Brundage, M. (2022). A Hazard Analysis Framework for Code Synthesis Large Language Models. ArXiv preprint. <https://arxiv.org/abs/2207.14157>
* Kinniment, M., Sato, L. J. K., Du, H., Goodrich, B., Hasin, M., Chan, L., Miles, L. H., Lin, T. R., Wijk, H., Burget, J., Ho, A., Barnes, E., & Christiano, P. (2023). Evaluating Language-Model Agents on Realistic Autonomous Tasks. *Alignment Research Center Report*..
* Kirk-Giannini, C. D. (2023). Dilemmatic gaslighting. *Philosophical Studies* 180: 745–772.
* Kirk-Giannini, C. D. (Forthcoming). How to solve the gender inclusion problem. *Hypatia*. <https://philpapers.org/archive/KIRHTS.pdf>
* Koessler, L., & Schuett, J. (2023). Risk Assessment at AGI Companies: A Review of Popular Risk Assessment Techniques from Other Safety-Critical Industries. ArXiv Preprint. <https://arxiv.org/abs/2307.08823>
* Langosco, L., Koch, J., Sharkey, L., Pfau, J., & Krueger, D. (2022). Goal misgeneralization in deep reinforcement learning. *Proceedings of the 39th International Conference on Machine Learning (ICML ‘22)*: 12004–12019.
* Lazar, S. (2022). Power and AI: Nature and Justification. In *The Oxford Handbook of AI Governance*, edited by Justin B. Bullock, Yu-Che Chen, Johannes Himmelreich, Valerie M. Hudson, Anton Korinek, Matthew M. Young, and Baobao Zhang. Oxford University Press.
* Lazar, S., & Nelson, A. (2023). AI Safety on Whose Terms? *Science*, *381*, 6654: 138–138.

  [Article](https://doi.org/10.1126%2Fscience.adi8982) 
  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=AI%20Safety%20on%20Whose%20Terms%3F&journal=Science&doi=10.1126%2Fscience.adi8982&volume=381&pages=138-138&publication_year=2023&author=Lazar%2CS&author=Nelson%2CA)
* Li, K., Patel, O., Viégas, F., Pfister, H., & Wattenberg, M. (2024). Inference-time intervention: Eliciting truthful answers from a language model. *Advances in Neural Information Processing Systems* 36.
* Manheim, K., & Kaplan, L. (2019). Artificial intelligence: Risks to privacy and democracy. *Yale Journal of Law and Technology*, *21*, 106–188.

  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Artificial%20intelligence%3A%20Risks%20to%20privacy%20and%20democracy&journal=Yale%20Journal%20of%20Law%20and%20Technology&volume=21&pages=106-188&publication_year=2019&author=Manheim%2CK&author=Kaplan%2CL)
* Nemitz, P. (2018). Constitutional democracy and technology in the age of artificial intelligence. *Philosophical Transactions of the Royal Society A*, *376*, 20180089.

  [Article](https://doi.org/10.1098%2Frsta.2018.0089) 
  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Constitutional%20democracy%20and%20technology%20in%20the%20age%20of%20artificial%20intelligence&journal=Philosophical%20Transactions%20of%20the%20Royal%20Society%20A&doi=10.1098%2Frsta.2018.0089&volume=376&publication_year=2018&author=Nemitz%2CP)
* Ngo, R. (2020). AGI safety from first principles: Introduction. AI Alignment Forum post. <https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ/p/8xRSjC76HasLnMGSf>
* Ngo, R., Chan, L., & Mindermann, S. (2023). The Alignment Problem from a Deep Learning Perspective (v5). *ArXiv Preprint*. <https://arxiv.org/abs/2209.00626>
* Noble, S. U. (2018). *Algorithms of Oppression: How Search Engines Reinforce Racism*. NYU.
* Ord, T. (2020). *The Precipice: Existential Risk and the Future of Humanity*. Hachette Books.
* Pang, G., Shen, C., Cao, L., & Hengel, A. V. D. (2021). Deep learning for anomaly detection: A review. *ACM Computing Surveys (CSUR)*, *54*(2), 1–38.

  [Article](https://doi.org/10.1145%2F3439950) 
  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Deep%20learning%20for%20anomaly%20detection%3A%20A%20review&journal=ACM%20Computing%20Surveys%20%28CSUR%29&doi=10.1145%2F3439950&volume=54&issue=2&pages=1-38&publication_year=2021&author=Pang%2CG&author=Shen%2CC&author=Cao%2CL&author=Hengel%2CAVD)
* Park, P. S., Goldstein, S., O’Gara, A., Chen, M., & Hendrycks, D. (2024). AI deception: A survey of examples, risks, and potential solutions. *Patterns*, *5*(5), 100988.

  [Article](https://doi.org/10.1016%2Fj.patter.2024.100988) 
  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=AI%20deception%3A%20A%20survey%20of%20examples%2C%20risks%2C%20and%20potential%20solutions&journal=Patterns&doi=10.1016%2Fj.patter.2024.100988&volume=5&issue=5&publication_year=2024&author=Park%2CPS&author=Goldstein%2CS&author=O%E2%80%99Gara%2CA&author=Chen%2CM&author=Hendrycks%2CD)
* Perrigo, B. (2023). Exclusive: OpenAI Lobbied E.U. to Water Down AI Regulation’. *TIME Magazine* June 20, 2023. <https://time.com/6288245/openai-eu-lobbying-ai-act/>
* Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., & Finn, C. (2024). Direct preference optimization: Your language model is secretly a reward model. *Advances in Neural Information Processing Systems* 36.
* Raji, I. D., Smart, A., White, R. M., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, D., & Barnes, P. (2020). Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing. *Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency*: 33–44.
* Rismani, S., Shelby, R., Smart, A., Delos Santos, R., Moon, A. J., & Rostamzadeh, N. (2023). Beyond the ML Model: Applying Safety Engineering Frameworks to Text-to-Image Development. *Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society*: 70–83.
* Rismani, S., Shelby, R., Smart, A., Jatho, E., Kroll, J., Moon, A. J., & Rostamzadeh, N. (2023). From Plane Crashes to Algorithmic Harm: Applicability of Safety Engineering Frameworks for Responsible ML. *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems*: 1–18.
* Roland, H., & Moriarty, B. (1990). *System Safety Engineering and Management*. John Wiley & Sons, Ltd.
* Roose, K. (2023). A.I. Poses ‘Risk of Extinction,’ Industry Leaders Warn. *New York Times* May 30, 2023.
* Russell, S. (2019). *Human Compatible: AI and the Problem of Control*. Allen Lane.
* Schneier, B., & Sanders, N. (2023). The A.I. Wars Have Three Factions, and They All Crave Power. *New York Times* September 28, 2023.
* Shavit, Y. (2023). What Does It Take to Catch a Chinchilla? Verifying Rules on Large-Scale Neural Network Training via Compute Monitoring. ArXiv preprint. <https://arxiv.org/abs/2303.11341>
* Shelby, R., Rismani, S., Henne, K., Moon, A. J., Rostamzadeh, N., Nicholas, P., Yilla-Akbari, N.… and, & Virk, G. (2023). Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for Harm Reduction. *Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society*: 723–741.
* Shevlane, T., Farquhar, S., Garfinkel, B., Phuong, M., Whittlestone, J., Leung, J., Kokotajlo, D., Marchal, N., Anderljung, M., Kolt, N., & Ho, L. (2023). Model evaluation for extreme risks. *ArXiv preprint*. <https://arxiv.org/abs/2305.15324>
* Solaiman, I., Talat, Z., Agnew, W., Ahmad, L., Baker, D., Blodgett, S. L., & Vassilev, A. (2023). Evaluating the Social Impact of Generative AI Systems in Systems and Society. ArXiv preprint. <https://arxiv.org/abs/2306.05949>
* Tanswell, F. S. (2018). Conceptual engineering for mathematical concepts. *Inquiry: A Journal Of Medical Care Organization, Provision And Financing*, *61*, 881–913.

  [Article](https://doi.org/10.1080%2F0020174X.2017.1385526) 
  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Conceptual%20engineering%20for%20mathematical%20concepts&journal=Inquiry%3A%20A%20Journal%20Of%20Medical%20Care%20Organization%2C%20Provision%20And%20Financing&doi=10.1080%2F0020174X.2017.1385526&volume=61&pages=881-913&publication_year=2018&author=Tanswell%2CFS)
* Thornley, E. (2024). The shutdown problem: an AI engineering puzzle for decision theorists. *Philosophical Studies*. Online First.
* Trapp, M., Schneider, D., & Weiss, G. (2018). Towards Safety-Awareness and Dynamic Safety Management. *14th European Dependable Computing Conference (EDCC)*, 107–11.
* Trowler, P. (2012). Disciplines and academic practices. In Trowler, P., Saunders, M., and Bamber, V. (2012), pp. 30–38.
* Trowler, P., Saunders, M., & Bamber, V. (Eds.). (2012). *Tribes and Territories in the 21st Century: Rethinking the Significance of Disciplines in Higher Education*. Routledge.
* Tubert, A., & Tiehen, J. (2024). Existentialist risk and value misalignment. *Philosophical Studies*. Online First.
* Verma, S., Boonsanong, V., Hoang, M., Hines, K. E., Dickerson, J. P., & Shah, C. (2022). Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review. ArXiv Preprint. <https://arxiv.org/abs/2010.10596>
* Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P. S., & Gabriel, I. (2021). Ethical and social risks of harm from language models. *ArXiv preprint*. <https://arxiv.org/abs/2112.04359>
* Weidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang, P. S., Mellor, J., & Gabriel, I. (2022). Taxonomy of risks posed by language models. *Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency*: 214–229.
* Weidinger, L., Rauh, M., Marchal, N., Manzini, A., Hendricks, L. A., Mateos-Garcia, Bergman, S.… and, & Isaac, W. (2023). Sociotechnical Safety Evaluation of Generative AI Systems. ArXiv Preprint. <https://arxiv.org/abs/2310.11986>
* Wijk, H., Lin, T., Becker, J., Jawhar, S., Parikh, N., Broadley, T., & Barnes, E. (2024). RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts. ArXiv preprint. <https://arxiv.org/abs/2411.15114>
* Yampolskiy, R., & Fox, J. (2013). Safety Engineering for Artificial General Intelligence. *Topoi*, *32*, 217–226.

  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Safety%20Engineering%20for%20Artificial%20General%20Intelligence&journal=Topoi&volume=32&pages=217-226&publication_year=2013&author=Yampolskiy%2CR&author=Fox%2CJ)
* Yang, J., Zhou, K., Li, Y., & Liu, Z. (2021). Generalized out-of-distribution detection: A survey. *ArXiv Preprint*. <https://arxiv.org/abs/2110.11334>

[Download references](https://citation-needed.springer.com/v2/references/10.1007/s11098-025-02367-z?format=refman&flavour=references)

## Acknowledgments

We are grateful to Seth Lazar and two anonymous referees for *Philosophical Studies* for their helpful comments on earlier drafts of this paper. We began work on this project in 2023 when we were fellows at the Center for AI Safety in San Francisco.

## Author information

### Authors and Affiliations

1. Department of Philosophy, Stanford University, Stanford, USA

   Jacqueline Harding
2. Department of Philosophy, Rutgers University–Newark, Newark, USA

   Cameron Domenico Kirk-Giannini

Authors

1. Jacqueline Harding

   [View author publications](/search?sortBy=newestFirst&dc.creator=Jacqueline%20Harding)

   Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Jacqueline%20Harding) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Jacqueline%20Harding%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)
2. Cameron Domenico Kirk-Giannini

   [View author publications](/search?sortBy=newestFirst&dc.creator=Cameron%20Domenico%20Kirk-Giannini)

   Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Cameron%20Domenico%20Kirk-Giannini) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Cameron%20Domenico%20Kirk-Giannini%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

### Corresponding author

Correspondence to
[Cameron Domenico Kirk-Giannini](mailto:camerondomenico.kirkgiannini@gmail.com).

## Additional information

### Publisher’s note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

## Rights and permissions

**Open Access** This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <http://creativecommons.org/licenses/by/4.0/>.

[Reprints and permissions](https://s100.copyright.com/AppDispatchServlet?title=What%20is%20AI%20safety%3F%20What%20do%20we%20want%20it%20to%20be%3F&author=Jacqueline%20Harding%20et%20al&contentID=10.1007%2Fs11098-025-02367-z&copyright=The%20Author%28s%29&publication=0031-8116&publicationDate=2025-06-24&publisherName=SpringerNature&orderBeanReset=true&oa=CC%20BY)

## About this article

[![Check for updates. Verify currency and authenticity via CrossMark](data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>)](ai-leaders-insights/技术路径与方法论/深度研究流派/安全优先路径/images/65c9d9e4e8be0d643d37181d77267ce4.jpg)

### Cite this article

Harding, J., Kirk-Giannini, C.D. What is AI safety? What do we want it to be?.
*Philos Stud* **182**, 1495–1518 (2025). https://doi.org/10.1007/s11098-025-02367-z

[Download citation](https://citation-needed.springer.com/v2/references/10.1007/s11098-025-02367-z?format=refman&flavour=citation)

* Accepted: 05 June 2025
* Published: 24 June 2025
* Version of record: 24 June 2025
* Issue date: July 2025
* DOI: https://doi.org/10.1007/s11098-025-02367-z

### Share this article

Anyone you share the following link with will be able to read this content:

Get shareable link

Sorry, a shareable link is not currently available for this article.

Copy shareable link to clipboard

Provided by the Springer Nature SharedIt content-sharing initiative

[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=11098)

Avoid common mistakes on your manuscript.

## Associated Content

Part of a collection:

### [**AI Safety**](https://link.springer.com/collections/cadgidecih)

Advertisement

---

*爬取时间: 2025-11-28 21:49:25*
